{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b0bd620",
   "metadata": {},
   "source": [
    "# 去除重复的预测（annotation）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f464dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Literal, Optional, Tuple\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "# === 强制使用美国中部时间 CST/CDT ===\n",
    "CENTRAL_TZ = ZoneInfo(\"US/Central\")\n",
    "\n",
    "\n",
    "def central_time(*args):\n",
    "    return datetime.now(CENTRAL_TZ).timetuple()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 日志\n",
    "# ============================================================\n",
    "\n",
    "LOGGER: logging.Logger = logging.getLogger(__name__)\n",
    "\n",
    "# ============================================================\n",
    "# 类型别名\n",
    "# ============================================================\n",
    "\n",
    "BoundingBox = Tuple[float, float, float, float]\n",
    "DedupMethod = Literal[\"NMS\", \"NMM\", \"GREEDYNMM\", \"LSNMS\"]\n",
    "OverlapMetric = Literal[\"IOU\", \"IOS\"]\n",
    "KeepStrategy = Literal[\"highest_score\"]\n",
    "\n",
    "# ============================================================\n",
    "# 配置 dataclass\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ProcessingConfig:\n",
    "    \"\"\"去重相关配置.\"\"\"\n",
    "    method: DedupMethod = \"NMS\"\n",
    "    overlap_metric: OverlapMetric = \"IOS\"\n",
    "    overlap_threshold: float = 0.5\n",
    "    class_agnostic: bool = False\n",
    "    keep_strategy: KeepStrategy = \"highest_score\"  # 目前仅支持 highest_score\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    \"\"\"输入输出 JSON 路径配置（支持多文件批处理）.\"\"\"\n",
    "    input_jsons: List[Path]\n",
    "    output_jsons: List[Path]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LoggingConfig:\n",
    "    \"\"\"日志配置.\"\"\"\n",
    "    level: str = \"INFO\"\n",
    "    log_to_file: bool = True\n",
    "    log_file_name: str = \"dedup.log\"\n",
    "    overwrite_log_file: bool = True\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AppConfig:\n",
    "    \"\"\"顶层应用配置，负责从 YAML 加载.\"\"\"\n",
    "    processing: ProcessingConfig\n",
    "    pipeline: PipelineConfig\n",
    "    logging: LoggingConfig\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_path(value: Any) -> Path:\n",
    "        if isinstance(value, Path):\n",
    "            return value\n",
    "        return Path(str(value))\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_yaml(cls, config_path: Path) -> \"AppConfig\":\n",
    "        import yaml  # 延迟导入\n",
    "\n",
    "        with config_path.open(\"r\", encoding=\"utf-8\") as file:\n",
    "            raw: Dict[str, Any] = yaml.safe_load(file)\n",
    "\n",
    "        processing_raw: Dict[str, Any] = raw.get(\"processing\", {})\n",
    "        pipeline_raw: Dict[str, Any] = raw.get(\"pipeline\", {})\n",
    "        logging_raw: Dict[str, Any] = raw.get(\"logging\", {})\n",
    "\n",
    "        processing_config = ProcessingConfig(\n",
    "            method=processing_raw.get(\"method\", \"NMS\"),\n",
    "            overlap_metric=processing_raw.get(\"overlap_metric\", \"IOS\"),\n",
    "            overlap_threshold=float(processing_raw.get(\"overlap_threshold\", 0.5)),\n",
    "            class_agnostic=bool(processing_raw.get(\"class_agnostic\", False)),\n",
    "            keep_strategy=processing_raw.get(\"keep_strategy\", \"highest_score\"),\n",
    "        )\n",
    "\n",
    "        def _ensure_path_list(value: Any, key_name: str) -> List[Path]:\n",
    "            if isinstance(value, list):\n",
    "                return [cls._to_path(v) for v in value]\n",
    "            if value is None:\n",
    "                raise ValueError(f\"`pipeline.{key_name}` is required in config.\")\n",
    "            # 单个字符串 / Path 时，包装成单元素列表\n",
    "            return [cls._to_path(value)]\n",
    "\n",
    "        root_dir_path_list = pipeline_raw.get(\"root_dir_path_list\")\n",
    "\n",
    "        root_dir_path_list = _ensure_path_list(root_dir_path_list, \"root_dir_path_list\")\n",
    "\n",
    "        input_json_subpath = pipeline_raw.get(\"input_json_subpath\", \"\")\n",
    "        output_json_subpath = pipeline_raw.get(\"output_json_subpath\", \"\")\n",
    "\n",
    "        input_jsons: List[Path] = []\n",
    "        output_jsons: List[Path] = []\n",
    "\n",
    "        if input_json_subpath:\n",
    "            input_jsons = [p / input_json_subpath for p in root_dir_path_list]\n",
    "        if output_json_subpath:\n",
    "            output_jsons = [p / output_json_subpath for p in root_dir_path_list]\n",
    "\n",
    "        pipeline_config = PipelineConfig(\n",
    "            input_jsons=input_jsons,\n",
    "            output_jsons=output_jsons,\n",
    "        )\n",
    "\n",
    "        logging_config = LoggingConfig(\n",
    "            level=logging_raw.get(\"level\", \"INFO\"),\n",
    "            log_to_file=bool(logging_raw.get(\"log_to_file\", True)),\n",
    "            log_file_name=logging_raw.get(\"log_file_name\", \"dedup.log\"),\n",
    "            overwrite_log_file=bool(logging_raw.get(\"overwrite_log_file\", True)),\n",
    "        )\n",
    "\n",
    "        return cls(\n",
    "            processing=processing_config,\n",
    "            pipeline=pipeline_config,\n",
    "            logging=logging_config,\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 领域对象 dataclass\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CocoImage:\n",
    "    id: int\n",
    "    file_name: str\n",
    "    width: int\n",
    "    height: int\n",
    "    license: Optional[int] = None\n",
    "    date_captured: Optional[str] = None\n",
    "    focal_length_parameter: Optional[float] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CocoCategory:\n",
    "    id: int\n",
    "    name: str\n",
    "    supercategory: Optional[str] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CocoAnnotation:\n",
    "    id: int\n",
    "    image_id: int\n",
    "    category_id: int\n",
    "    bbox: BoundingBox\n",
    "    area: float\n",
    "    iscrowd: int = 0\n",
    "    score: Optional[float] = None\n",
    "    object_id: Optional[str] = None\n",
    "    segmentation: Optional[Any] = None\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        data: Dict[str, Any] = {\n",
    "            \"id\": self.id,\n",
    "            \"image_id\": self.image_id,\n",
    "            \"category_id\": self.category_id,\n",
    "            \"bbox\": list(self.bbox),\n",
    "            \"area\": self.area,\n",
    "            \"iscrowd\": self.iscrowd,\n",
    "        }\n",
    "        if self.segmentation is not None:\n",
    "            data[\"segmentation\"] = self.segmentation\n",
    "        if self.score is not None:\n",
    "            data[\"score\"] = self.score\n",
    "        if self.object_id is not None:\n",
    "            data[\"object_id\"] = self.object_id\n",
    "        return data\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CocoDataset:\n",
    "    images: List[CocoImage]\n",
    "    annotations: List[CocoAnnotation]\n",
    "    categories: List[CocoCategory]\n",
    "    info: Optional[Dict[str, Any]] = None\n",
    "    licenses: Optional[List[Dict[str, Any]]] = None\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 日志配置\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def configure_logging_for_directory(base_dir: Path, logging_config: LoggingConfig) -> None:\n",
    "    \"\"\"\n",
    "    配置 LOGGER，让日志输出到：\n",
    "    - 控制台\n",
    "    - base_dir / log_file_name\n",
    "\n",
    "    每调用一次会清空之前的 handlers，\n",
    "    所以可以在循环里为“每个目录”单独配置一次。\n",
    "    \"\"\"\n",
    "    base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    level = getattr(logging, logging_config.level.upper(), logging.INFO)\n",
    "    LOGGER.setLevel(level)\n",
    "\n",
    "    formatter = logging.Formatter(\n",
    "        fmt=\"%(asctime)s [%(levelname)s] %(name)s - %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    )\n",
    "\n",
    "    # 清空旧的 handler（关键：保证不会重复叠加）\n",
    "    if LOGGER.handlers:\n",
    "        LOGGER.handlers.clear()\n",
    "\n",
    "    # 控制台输出\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(level)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    LOGGER.addHandler(console_handler)\n",
    "\n",
    "    log_path: Optional[Path] = None\n",
    "    if logging_config.log_to_file:\n",
    "        log_path = base_dir / logging_config.log_file_name\n",
    "        file_mode = \"w\" if logging_config.overwrite_log_file else \"a\"\n",
    "        file_handler = logging.FileHandler(log_path, mode=file_mode, encoding=\"utf-8\")\n",
    "        file_handler.setLevel(level)\n",
    "        file_handler.setFormatter(formatter)\n",
    "        LOGGER.addHandler(file_handler)\n",
    "\n",
    "    # 使用美国中部时间\n",
    "    logging.Formatter.converter = central_time\n",
    "\n",
    "    LOGGER.info(\"Logging configured. Base directory: %s\", base_dir)\n",
    "    LOGGER.info(\"Log path: %s\", log_path if log_path is not None else \"N/A\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# COCO JSON 读取 / 写入\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def load_coco_dataset(json_path: Path) -> CocoDataset:\n",
    "    LOGGER.info(\"Loading COCO dataset from %s\", json_path)\n",
    "    with json_path.open(\"r\", encoding=\"utf-8\") as file:\n",
    "        raw: Dict[str, Any] = json.load(file)\n",
    "\n",
    "    # 保存 info 和 licenses\n",
    "    info = raw.get(\"info\")\n",
    "    licenses = raw.get(\"licenses\")\n",
    "\n",
    "    images: List[CocoImage] = []\n",
    "    for img in raw.get(\"images\", []):\n",
    "        images.append(\n",
    "            CocoImage(\n",
    "                id=int(img[\"id\"]),\n",
    "                file_name=str(img[\"file_name\"]),\n",
    "                width=int(img[\"width\"]),\n",
    "                height=int(img[\"height\"]),\n",
    "                license=img.get(\"license\"),\n",
    "                date_captured=img.get(\"date_captured\"),\n",
    "                focal_length_parameter=img.get(\"focal_length_parameter\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    categories: List[CocoCategory] = []\n",
    "    for cat in raw.get(\"categories\", []):\n",
    "        categories.append(\n",
    "            CocoCategory(\n",
    "                id=int(cat[\"id\"]),\n",
    "                name=str(cat[\"name\"]),\n",
    "                supercategory=cat.get(\"supercategory\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    annotations: List[CocoAnnotation] = []\n",
    "    for ann in raw.get(\"annotations\", []):\n",
    "        bbox_raw = ann.get(\"bbox\", [0.0, 0.0, 0.0, 0.0])\n",
    "        bbox: BoundingBox = (\n",
    "            float(bbox_raw[0]),\n",
    "            float(bbox_raw[1]),\n",
    "            float(bbox_raw[2]),\n",
    "            float(bbox_raw[3]),\n",
    "        )\n",
    "        area_value = float(ann.get(\"area\", bbox[2] * bbox[3]))\n",
    "        score_value = float(ann[\"score\"]) if \"score\" in ann else None\n",
    "        object_id_value = ann.get(\"object_id\")\n",
    "\n",
    "        annotations.append(\n",
    "            CocoAnnotation(\n",
    "                id=int(ann[\"id\"]),\n",
    "                image_id=int(ann[\"image_id\"]),\n",
    "                category_id=int(ann[\"category_id\"]),\n",
    "                bbox=bbox,\n",
    "                area=area_value,\n",
    "                iscrowd=int(ann.get(\"iscrowd\", 0)),\n",
    "                score=score_value,\n",
    "                object_id=object_id_value,\n",
    "                segmentation=ann.get(\"segmentation\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    LOGGER.info(\n",
    "        \"Loaded dataset: %d images, %d annotations, %d categories\",\n",
    "        len(images),\n",
    "        len(annotations),\n",
    "        len(categories),\n",
    "    )\n",
    "    return CocoDataset(\n",
    "        images=images,\n",
    "        annotations=annotations,\n",
    "        categories=categories,\n",
    "        info=info,\n",
    "        licenses=licenses,\n",
    "    )\n",
    "\n",
    "\n",
    "def save_coco_dataset(dataset: CocoDataset, output_path: Path) -> None:\n",
    "    LOGGER.info(\"Saving deduplicated dataset to %s\", output_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    images_out: List[Dict[str, Any]] = []\n",
    "    for img in dataset.images:\n",
    "        data: Dict[str, Any] = {\n",
    "            \"id\": img.id,\n",
    "            \"file_name\": img.file_name,\n",
    "            \"width\": img.width,\n",
    "            \"height\": img.height,\n",
    "        }\n",
    "        # 添加可选字段\n",
    "        if img.license is not None:\n",
    "            data[\"license\"] = img.license\n",
    "        if img.date_captured is not None:\n",
    "            data[\"date_captured\"] = img.date_captured\n",
    "        if img.focal_length_parameter is not None:\n",
    "            data[\"focal_length_parameter\"] = img.focal_length_parameter\n",
    "        images_out.append(data)\n",
    "\n",
    "    categories_out: List[Dict[str, Any]] = []\n",
    "    for cat in dataset.categories:\n",
    "        data: Dict[str, Any] = {\n",
    "            \"id\": cat.id,\n",
    "            \"name\": cat.name,\n",
    "        }\n",
    "        if cat.supercategory is not None:\n",
    "            data[\"supercategory\"] = cat.supercategory\n",
    "        categories_out.append(data)\n",
    "\n",
    "    annotations_out: List[Dict[str, Any]] = [ann.to_dict() for ann in dataset.annotations]\n",
    "\n",
    "    # info.description 改为 SWD COCO format deduplicated instance segmentation\n",
    "    if dataset.info is not None:\n",
    "        dataset.info[\"description\"] = \"SWD COCO format deduplicated instance segmentation\"\n",
    "\n",
    "    # 构建完整的输出结构，包含 info 和 licenses\n",
    "    raw_out: Dict[str, Any] = {\n",
    "        \"info\": dataset.info if dataset.info is not None else {},\n",
    "        \"licenses\": dataset.licenses if dataset.licenses is not None else [],\n",
    "        \"images\": images_out,\n",
    "        \"annotations\": annotations_out,\n",
    "        \"categories\": categories_out,\n",
    "    }\n",
    "\n",
    "    with output_path.open(\"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(raw_out, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    LOGGER.info(\n",
    "        \"Saved dataset: %d images, %d annotations, %d categories\",\n",
    "        len(dataset.images),\n",
    "        len(dataset.annotations),\n",
    "        len(dataset.categories),\n",
    "    )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 几何工具（只用 bbox，不用 segmentation）\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def convert_bbox_to_xyxy(bbox: BoundingBox) -> BoundingBox:\n",
    "    x_min, y_min, width, height = bbox\n",
    "    return x_min, y_min, x_min + width, y_min + height\n",
    "\n",
    "\n",
    "def compute_intersection_area(bbox_a: BoundingBox, bbox_b: BoundingBox) -> float:\n",
    "    ax1, ay1, ax2, ay2 = convert_bbox_to_xyxy(bbox_a)\n",
    "    bx1, by1, bx2, by2 = convert_bbox_to_xyxy(bbox_b)\n",
    "\n",
    "    inter_x1 = max(ax1, bx1)\n",
    "    inter_y1 = max(ay1, by1)\n",
    "    inter_x2 = min(ax2, bx2)\n",
    "    inter_y2 = min(ay2, by2)\n",
    "\n",
    "    inter_width = max(0.0, inter_x2 - inter_x1)\n",
    "    inter_height = max(0.0, inter_y2 - inter_y1)\n",
    "    return inter_width * inter_height\n",
    "\n",
    "\n",
    "def compute_bbox_area(bbox: BoundingBox) -> float:\n",
    "    return max(0.0, bbox[2]) * max(0.0, bbox[3])\n",
    "\n",
    "\n",
    "def compute_overlap(\n",
    "    bbox_a: BoundingBox,\n",
    "    bbox_b: BoundingBox,\n",
    "    metric: OverlapMetric,\n",
    ") -> float:\n",
    "    inter_area = compute_intersection_area(bbox_a, bbox_b)\n",
    "    if inter_area <= 0.0:\n",
    "        return 0.0\n",
    "\n",
    "    area_a = compute_bbox_area(bbox_a)\n",
    "    area_b = compute_bbox_area(bbox_b)\n",
    "\n",
    "    if metric == \"IOU\":\n",
    "        union_area = area_a + area_b - inter_area\n",
    "        if union_area <= 0.0:\n",
    "            return 0.0\n",
    "        return inter_area / union_area\n",
    "\n",
    "    # IOS: intersection over smaller box\n",
    "    smaller_area = min(area_a, area_b)\n",
    "    if smaller_area <= 0.0:\n",
    "        return 0.0\n",
    "    return inter_area / smaller_area\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 去重核心逻辑\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def group_annotations_for_dedup(\n",
    "    annotations: Iterable[CocoAnnotation],\n",
    "    class_agnostic: bool,\n",
    ") -> Dict[Tuple[int, Optional[int]], List[CocoAnnotation]]:\n",
    "    grouped: Dict[Tuple[int, Optional[int]], List[CocoAnnotation]] = {}\n",
    "    for ann in annotations:\n",
    "        category_key: Optional[int] = None if class_agnostic else ann.category_id\n",
    "        key = (ann.image_id, category_key)\n",
    "        if key not in grouped:\n",
    "            grouped[key] = []\n",
    "        grouped[key].append(ann)\n",
    "    return grouped\n",
    "\n",
    "\n",
    "def sort_annotations_by_score_desc(annotations: List[CocoAnnotation]) -> List[CocoAnnotation]:\n",
    "    return sorted(\n",
    "        annotations,\n",
    "        key=lambda ann: float(ann.score) if ann.score is not None else -1.0,\n",
    "        reverse=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def cluster_annotations_greedily(\n",
    "    annotations: List[CocoAnnotation],\n",
    "    processing_config: ProcessingConfig,\n",
    ") -> List[List[CocoAnnotation]]:\n",
    "    \"\"\"\n",
    "    通用\"贪心聚类\"：\n",
    "    - 按 score 降序遍历（highest_score 代表）\n",
    "    - 如果与已有 cluster 的代表框重叠 >= threshold，则归入该 cluster\n",
    "    - 否则新建一个 cluster\n",
    "    所有 method（NMS / NMM / GREEDYNMM / LSNMS）都基于这个聚类结果，再决定如何输出。\n",
    "    \"\"\"\n",
    "    if not annotations:\n",
    "        return []\n",
    "\n",
    "    sorted_anns = sort_annotations_by_score_desc(annotations)\n",
    "    clusters: List[List[CocoAnnotation]] = []\n",
    "\n",
    "    for ann in sorted_anns:\n",
    "        assigned = False\n",
    "        for cluster in clusters:\n",
    "            representative = cluster[0]  # 因为按 score 降序，cluster[0] 始终是 highest_score\n",
    "            overlap_value = compute_overlap(\n",
    "                ann.bbox,\n",
    "                representative.bbox,\n",
    "                metric=processing_config.overlap_metric,\n",
    "            )\n",
    "            if overlap_value >= processing_config.overlap_threshold:\n",
    "                cluster.append(ann)\n",
    "                assigned = True\n",
    "                break\n",
    "        if not assigned:\n",
    "            clusters.append([ann])\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def merge_cluster_to_single_annotation(cluster: List[CocoAnnotation]) -> CocoAnnotation:\n",
    "    \"\"\"\n",
    "    NMM / GREEDYNMM 使用：\n",
    "    - bbox 使用 score 加权平均的中心和宽高；\n",
    "    - score 使用 cluster 中的最大 score；\n",
    "    - segmentation 和 object_id 直接继承代表框（score 最大的那个）的。\n",
    "    \"\"\"\n",
    "    if not cluster:\n",
    "        raise ValueError(\"Cluster must contain at least one annotation.\")\n",
    "\n",
    "    representative = sort_annotations_by_score_desc(cluster)[0]\n",
    "\n",
    "    # 计算权重：用 score，如果没有 score 就当 1.0\n",
    "    weights: List[float] = [\n",
    "        float(ann.score) if ann.score is not None else 1.0 for ann in cluster\n",
    "    ]\n",
    "    weight_sum = sum(weights)\n",
    "    if weight_sum <= 0.0:\n",
    "        weights = [1.0 for _ in cluster]\n",
    "        weight_sum = float(len(cluster))\n",
    "\n",
    "    # 将 bbox 转为中心点 + 宽高\n",
    "    centers_x: List[float] = []\n",
    "    centers_y: List[float] = []\n",
    "    widths: List[float] = []\n",
    "    heights: List[float] = []\n",
    "\n",
    "    for ann in cluster:\n",
    "        x, y, w, h = ann.bbox\n",
    "        centers_x.append(x + w / 2.0)\n",
    "        centers_y.append(y + h / 2.0)\n",
    "        widths.append(w)\n",
    "        heights.append(h)\n",
    "\n",
    "    # score 加权平均\n",
    "    merged_cx = sum(c * w for c, w in zip(centers_x, weights)) / weight_sum\n",
    "    merged_cy = sum(c * w for c, w in zip(centers_y, weights)) / weight_sum\n",
    "    merged_w = sum(w * wt for w, wt in zip(widths, weights)) / weight_sum\n",
    "    merged_h = sum(h * wt for h, wt in zip(heights, weights)) / weight_sum\n",
    "\n",
    "    merged_x = merged_cx - merged_w / 2.0\n",
    "    merged_y = merged_cy - merged_h / 2.0\n",
    "    merged_bbox: BoundingBox = (merged_x, merged_y, merged_w, merged_h)\n",
    "    merged_area = compute_bbox_area(merged_bbox)\n",
    "\n",
    "    # 新 annotation：沿用代表框的大部分属性，只更新 bbox / area\n",
    "    return CocoAnnotation(\n",
    "        id=representative.id,\n",
    "        image_id=representative.image_id,\n",
    "        category_id=representative.category_id,\n",
    "        bbox=merged_bbox,\n",
    "        area=merged_area,\n",
    "        iscrowd=representative.iscrowd,\n",
    "        score=representative.score,\n",
    "        object_id=representative.object_id,\n",
    "        segmentation=representative.segmentation,\n",
    "    )\n",
    "\n",
    "\n",
    "def deduplicate_group_annotations(\n",
    "    annotations: List[CocoAnnotation],\n",
    "    processing_config: ProcessingConfig,\n",
    ") -> List[CocoAnnotation]:\n",
    "    \"\"\"\n",
    "    针对同一组（同一 image_id，且 class_agnostic 控制是否按类别分组）的 annotations 做去重。\n",
    "    \"\"\"\n",
    "    if not annotations:\n",
    "        return []\n",
    "\n",
    "    clusters = cluster_annotations_greedily(\n",
    "        annotations=annotations,\n",
    "        processing_config=processing_config,\n",
    "    )\n",
    "\n",
    "    method = processing_config.method.upper()\n",
    "\n",
    "    if method == \"NMS\":\n",
    "        # 标准 NMS：每个 cluster 只保留代表框（highest_score）\n",
    "        return [sort_annotations_by_score_desc(cluster)[0] for cluster in clusters]\n",
    "\n",
    "    if method == \"NMM\":\n",
    "        # Non-Maximum Merging：合并 cluster 为单一 bbox\n",
    "        return [merge_cluster_to_single_annotation(cluster) for cluster in clusters]\n",
    "\n",
    "    if method == \"GREEDYNMM\":\n",
    "        # 这里实现为与 NMM 等价的\"贪心合并\"，后续如有需要可改为更复杂策略\n",
    "        return [merge_cluster_to_single_annotation(cluster) for cluster in clusters]\n",
    "\n",
    "    if method == \"LSNMS\":\n",
    "        # 目前实现为 NMS 的别名；后续可扩展为更复杂的 Local Soft-NMS 策略\n",
    "        return [sort_annotations_by_score_desc(cluster)[0] for cluster in clusters]\n",
    "\n",
    "    # 理论上不会走到这里（被 Literal 类型保护），但为了安全加一个 fallback\n",
    "    LOGGER.warning(\"Unknown method '%s', fallback to NMS.\", method)\n",
    "    return [sort_annotations_by_score_desc(cluster)[0] for cluster in clusters]\n",
    "\n",
    "\n",
    "def deduplicate_annotations(\n",
    "    annotations: List[CocoAnnotation],\n",
    "    processing_config: ProcessingConfig,\n",
    ") -> List[CocoAnnotation]:\n",
    "    LOGGER.info(\n",
    "        \"Deduplicating %d annotations (method=%s, metric=%s, threshold=%.3f, class_agnostic=%s, keep_strategy=%s)\",\n",
    "        len(annotations),\n",
    "        processing_config.method,\n",
    "        processing_config.overlap_metric,\n",
    "        processing_config.overlap_threshold,\n",
    "        processing_config.class_agnostic,\n",
    "        processing_config.keep_strategy,\n",
    "    )\n",
    "\n",
    "    if processing_config.keep_strategy != \"highest_score\":\n",
    "        LOGGER.warning(\n",
    "            \"Keep strategy '%s' is not supported yet. Falling back to 'highest_score'.\",\n",
    "            processing_config.keep_strategy,\n",
    "        )\n",
    "\n",
    "    grouped = group_annotations_for_dedup(\n",
    "        annotations=annotations,\n",
    "        class_agnostic=processing_config.class_agnostic,\n",
    "    )\n",
    "\n",
    "    deduplicated: List[CocoAnnotation] = []\n",
    "    removed_count = 0\n",
    "\n",
    "    for group_key, group_annotations in grouped.items():\n",
    "        image_id, category_key = group_key\n",
    "        kept = deduplicate_group_annotations(\n",
    "            annotations=group_annotations,\n",
    "            processing_config=processing_config,\n",
    "        )\n",
    "        deduplicated.extend(kept)\n",
    "        removed_in_group = len(group_annotations) - len(kept)\n",
    "        removed_count += removed_in_group\n",
    "\n",
    "        if removed_in_group > 0:\n",
    "            LOGGER.debug(\n",
    "                \"Image %s, category %s: %d -> %d (removed %d)\",\n",
    "                image_id,\n",
    "                category_key,\n",
    "                len(group_annotations),\n",
    "                len(kept),\n",
    "                removed_in_group,\n",
    "            )\n",
    "\n",
    "    LOGGER.info(\n",
    "        \"Deduplication done: %d -> %d annotations (removed %d)\",\n",
    "        len(annotations),\n",
    "        len(deduplicated),\n",
    "        removed_count,\n",
    "    )\n",
    "    return deduplicated\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 管道入口\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def run_pipeline(config_path: Path) -> None:\n",
    "    app_config = AppConfig.load_from_yaml(config_path=config_path)\n",
    "\n",
    "    num_pairs = len(app_config.pipeline.input_jsons)\n",
    "\n",
    "    for index, (input_json, output_json) in enumerate(\n",
    "        zip(app_config.pipeline.input_jsons, app_config.pipeline.output_jsons),\n",
    "        start=1,\n",
    "    ):\n",
    "        # === 关键修改点：每个 output_json 的父目录单独配置日志 ===\n",
    "        base_dir = output_json.parent\n",
    "        configure_logging_for_directory(\n",
    "            base_dir=base_dir,\n",
    "            logging_config=app_config.logging,\n",
    "        )\n",
    "\n",
    "        LOGGER.info(\"Using configuration file: %s\", config_path)\n",
    "        LOGGER.info(\"Number of input JSON files: %d\", num_pairs)\n",
    "\n",
    "        LOGGER.info(\"Processing pair %d:\", index)\n",
    "        LOGGER.info(\"  Input JSON: %s\", input_json)\n",
    "        LOGGER.info(\"  Output JSON: %s\", output_json)\n",
    "\n",
    "        dataset = load_coco_dataset(json_path=input_json)\n",
    "\n",
    "        deduplicated_annotations = deduplicate_annotations(\n",
    "            annotations=dataset.annotations,\n",
    "            processing_config=app_config.processing,\n",
    "        )\n",
    "\n",
    "        deduplicated_dataset = CocoDataset(\n",
    "            images=dataset.images,\n",
    "            annotations=deduplicated_annotations,\n",
    "            categories=dataset.categories,\n",
    "            info=dataset.info,\n",
    "            licenses=dataset.licenses,\n",
    "        )\n",
    "\n",
    "        save_coco_dataset(\n",
    "            dataset=deduplicated_dataset,\n",
    "            output_path=output_json,\n",
    "        )\n",
    "\n",
    "    # 最后一条整体提示可以保留，也会记录在最后一个目录的 log 里\n",
    "    LOGGER.info(\"Pipeline finished for %d file pairs.\", num_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6de7e1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 19:34:50 [INFO] __main__ - Logging configured. Base directory: /workspace/_ty/01_data/00_test/00_try/output\n",
      "2025-11-24 19:34:50 [INFO] __main__ - Log path: /workspace/_ty/01_data/00_test/00_try/output/02_dedup.log\n",
      "2025-11-24 19:34:50 [INFO] __main__ - Using configuration file: 02_config.yaml\n",
      "2025-11-24 19:34:50 [INFO] __main__ - Number of input JSON files: 1\n",
      "2025-11-24 19:34:50 [INFO] __main__ - Processing pair 1:\n",
      "2025-11-24 19:34:50 [INFO] __main__ -   Input JSON: /workspace/_ty/01_data/00_test/00_try/output/01_swd_seg_results_coco.json\n",
      "2025-11-24 19:34:50 [INFO] __main__ -   Output JSON: /workspace/_ty/01_data/00_test/00_try/output/02_combined_annotations_dedup.json\n",
      "2025-11-24 19:34:50 [INFO] __main__ - Loading COCO dataset from /workspace/_ty/01_data/00_test/00_try/output/01_swd_seg_results_coco.json\n",
      "2025-11-24 19:34:50 [INFO] __main__ - Loaded dataset: 4 images, 333 annotations, 1 categories\n",
      "2025-11-24 19:34:50 [INFO] __main__ - Deduplicating 333 annotations (method=NMS, metric=IOS, threshold=0.500, class_agnostic=False, keep_strategy=highest_score)\n",
      "2025-11-24 19:34:50 [INFO] __main__ - Deduplication done: 333 -> 228 annotations (removed 105)\n",
      "2025-11-24 19:34:50 [INFO] __main__ - Saving deduplicated dataset to /workspace/_ty/01_data/00_test/00_try/output/02_combined_annotations_dedup.json\n",
      "2025-11-24 19:34:50 [INFO] __main__ - Saved dataset: 4 images, 228 annotations, 1 categories\n",
      "2025-11-24 19:34:50 [INFO] __main__ - Pipeline finished for 1 file pairs.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 命令行入口\n",
    "# ============================================================\n",
    "\n",
    "def parse_args() -> argparse.Namespace:\n",
    "    parser = argparse.ArgumentParser(description=\"COCO detection deduplication pipeline.\")\n",
    "    parser.add_argument(\n",
    "        \"--config\",\n",
    "        type=Path,\n",
    "        default=Path(\"02_config.yaml\"),\n",
    "        help=\"Path to YAML configuration file.\",\n",
    "    )\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    run_pipeline(config_path=args.config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
