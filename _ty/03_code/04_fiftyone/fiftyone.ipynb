{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d9b89ea",
   "metadata": {},
   "source": [
    "# FiftyOne 脚本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d05b9174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?notebook=True&subscription=cf391bde-7690-47d9-97e8-5cc4aaed8bf4\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x76331874ada0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "session = fo.launch_app()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "036d0a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00_try',\n",
       " 'jeff_0613-0624_04_ok',\n",
       " 'lloyd_0715-0729_04_ok',\n",
       " 'ms1_0605-0621_40_ok',\n",
       " 'ms1_0710-0726_36_ok',\n",
       " 'ms1_0710-0726_36_ok_v2',\n",
       " 'ms1_0726-0809_11_ok',\n",
       " 'ms1_0809-0823_34_ok',\n",
       " 'ms1_0809-0823_34_ok_v2',\n",
       " 'ms2_0726-0809_13_ok',\n",
       " 'ms2_0726-0809_13_ok_v2',\n",
       " 'ms2_0809-0823_10_ok',\n",
       " 'sw1_0605-0613_07_ok',\n",
       " 'sw1_0605-0613_07_ok_v2',\n",
       " 'sw1_0711-0725_03_ok',\n",
       " 'sw2_0808-0823_04_ok']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "# 获取所有dataset\n",
    "datasets = fo.list_datasets()\n",
    "display(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b67a1ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # delete all datasets\n",
    "# for ds_name in datasets:\n",
    "#     fo.delete_dataset(ds_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b28e8e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/tianqi/D/01_Projects/01_swd/02_code/pipeline/ultralytics_ty/_ty/01_data/01_16mp_2024_pipeline_data/sw1_0605-0613_07_ok_v2'),\n",
       " PosixPath('/home/tianqi/D/01_Projects/01_swd/02_code/pipeline/ultralytics_ty/_ty/01_data/01_16mp_2024_pipeline_data/ms2_0726-0809_13_ok_v2'),\n",
       " PosixPath('/home/tianqi/D/01_Projects/01_swd/02_code/pipeline/ultralytics_ty/_ty/01_data/01_16mp_2024_pipeline_data/ms1_0809-0823_34_ok_v2'),\n",
       " PosixPath('/home/tianqi/D/01_Projects/01_swd/02_code/pipeline/ultralytics_ty/_ty/01_data/01_16mp_2024_pipeline_data/ms1_0710-0726_36_ok_v2')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['sw1_0605-0613_07_ok_v2',\n",
       " 'ms2_0726-0809_13_ok_v2',\n",
       " 'ms1_0809-0823_34_ok_v2',\n",
       " 'ms1_0710-0726_36_ok_v2']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def fetch_subsequent_dir(data_root: Path, target_subdir_name: Path):\n",
    "    data_paths = list(data_root.glob(f\"*/{target_subdir_name}\"))\n",
    "    # display(data_paths)\n",
    "    # get sub dir - no target_subdir_name\n",
    "    subdir_path_list = [data_path.parent for data_path in data_paths]\n",
    "    # display(subdir_path_list)\n",
    "    subdir_name_list = [subdir.name for subdir in subdir_path_list]\n",
    "    # display(subdir_name_list)\n",
    "    return subdir_path_list, subdir_name_list\n",
    "\n",
    "\n",
    "# data_root = Path(\"/home/tianqi/D/01_Projects/01_swd/02_code/pipeline/ultralytics_ty/_ty/01_data/00_test\")\n",
    "data_root = Path(\"/home/tianqi/D/01_Projects/01_swd/02_code/pipeline/ultralytics_ty/_ty/01_data/01_16mp_2024_pipeline_data\")\n",
    "\n",
    "target_subdir_name = Path(\"raw_data\")\n",
    "subdir_path_list, subdir_name_list = fetch_subsequent_dir(data_root, target_subdir_name)\n",
    "display(subdir_path_list)\n",
    "display(subdir_name_list)\n",
    "len(subdir_path_list), len(subdir_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2171721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.core.labels as fol\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import fiftyone as fo\n",
    "\n",
    "def attach_dot_to_fiftyone(\n",
    "    dataset: fo.Dataset,\n",
    "    coco_json_path: Path,\n",
    "    dot_field: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    根据 COCO json 里的标准字段：\n",
    "      - annotation[\"attributes\"][\"dot_detections\"]\n",
    "    给 FiftyOne dataset 加一个新的可视化字段：\n",
    "      - sample[dot_field]: fo.Detections（所有 dot box）\n",
    "    \"\"\"\n",
    "    # 1. 读 COCO JSON\n",
    "    with coco_json_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        coco = json.load(f)\n",
    "\n",
    "    images = coco[\"images\"]\n",
    "    annotations = coco[\"annotations\"]\n",
    "\n",
    "    # image_id -> file_name\n",
    "    image_id_to_fname = {img[\"id\"]: img[\"file_name\"] for img in images}\n",
    "\n",
    "    # file_name -> [annotations...] （保持原始顺序）\n",
    "    anns_by_fname = defaultdict(list)\n",
    "    for ann in annotations:\n",
    "        fname = image_id_to_fname[ann[\"image_id\"]]\n",
    "        anns_by_fname[fname].append(ann)\n",
    "\n",
    "    # skeleton（跟 categories 里的 keypoints / skeleton 一致）\n",
    "    dataset.default_skeleton = fo.KeypointSkeleton(\n",
    "        labels=[\"h\", \"lp\", \"rp\"],\n",
    "        edges=[[0, 1], [0, 2]],\n",
    "    )\n",
    "\n",
    "    # 2. 按 sample 遍历，把 pose / dot 加进去\n",
    "    for sample in dataset:\n",
    "        fname = Path(sample.filepath).name\n",
    "        anns = anns_by_fname.get(fname, [])\n",
    "        if not anns:\n",
    "            continue\n",
    "\n",
    "        W = sample.metadata.width\n",
    "        H = sample.metadata.height\n",
    "\n",
    "        dot_dets_list = []\n",
    "\n",
    "        for ann in anns:\n",
    "            # -------- attributes.dot_detections -> fo.Detections --------\n",
    "            attrs = ann.get(\"attributes\") or {}\n",
    "            dot_list = attrs.get(\"dot_detections\") or []\n",
    "            for dot in dot_list:\n",
    "                # bbox: [x, y, w, h] 像素坐标\n",
    "                x, y, w, h = dot[\"bbox\"]\n",
    "                rel_box = [\n",
    "                    x / W,\n",
    "                    y / H,\n",
    "                    w / W,\n",
    "                    h / H,\n",
    "                ]\n",
    "                det = fol.Detection(\n",
    "                    bounding_box=rel_box,\n",
    "                    confidence=dot.get(\"score\", None),\n",
    "                    label=str(dot.get(\"category_id\", \"\")),  # 这里用 category_id，或者直接写 \"dot\"\n",
    "                )\n",
    "                dot_dets_list.append(det)\n",
    "\n",
    "        # 3. 挂到 sample 上\n",
    "        if dot_dets_list:\n",
    "            sample[dot_field] = fol.Detections(detections=dot_dets_list)\n",
    "\n",
    "        sample.save()\n",
    "\n",
    "    print(f\"✅ 已将 dot 检测框写入字段: {dot_field}\")\n",
    "\n",
    "\n",
    "def extract_time_info(file_name: str) -> datetime:\n",
    "    time_info = \"_\".join(file_name.split(\"_\")[:-1])\n",
    "    dt = datetime.strptime(time_info, \"%m%d_%H%M\")\n",
    "    return dt.replace(year=2024)  # 假设年份为2024年\n",
    "\n",
    "def extract_focus_info(file_name: str) -> str:\n",
    "    return file_name.split(\"_\")[-1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fee0ed09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "def add_tags_to_all_labels(\n",
    "    dataset: fo.Dataset,\n",
    "    bool_attr: str,\n",
    "    tag_name: str | None = None,\n",
    "    mode: str = \"both\",\n",
    "):\n",
    "    \"\"\"\n",
    "    给 label 添加 tag，当 label.<bool_attr> 为 True 时打上 tag。\n",
    "\n",
    "    参数:\n",
    "        bool_attr:   label 上的布尔属性名，例如 \"is_final_swd\"\n",
    "        tag_name:    标签名，默认用 bool_attr\n",
    "        mode:        \"keypoints\", \"detections\", 或 \"both\"\n",
    "    \"\"\"\n",
    "    if tag_name is None:\n",
    "        tag_name = bool_attr\n",
    "\n",
    "    mode = mode.lower()\n",
    "    assert mode in (\"keypoints\", \"detections\", \"both\"), \"mode 必须是 keypoints/detections/both\"\n",
    "\n",
    "    label_fields = dataset._get_label_fields()\n",
    "    print(\"所有 label 字段:\", label_fields)\n",
    "\n",
    "    for field in label_fields:\n",
    "        first_label = dataset.first()[field]\n",
    "\n",
    "        # 判断这个字段包含 keypoints 或 detections\n",
    "        is_kp = hasattr(first_label, \"keypoints\")\n",
    "        is_det = hasattr(first_label, \"detections\")\n",
    "\n",
    "        # 根据 mode 决定是否处理该字段\n",
    "        if mode == \"keypoints\" and not is_kp:\n",
    "            continue\n",
    "        if mode == \"detections\" and not is_det:\n",
    "            continue\n",
    "        if mode == \"both\" and not (is_kp or is_det):\n",
    "            continue\n",
    "\n",
    "        for sample in dataset:\n",
    "            labels_layer = sample[field]\n",
    "            if not labels_layer:\n",
    "                continue\n",
    "\n",
    "            # 选择对应的 label 列表\n",
    "            if is_kp:\n",
    "                labels = labels_layer.keypoints\n",
    "            else:\n",
    "                labels = labels_layer.detections\n",
    "\n",
    "            changed = False\n",
    "\n",
    "            for label in labels:\n",
    "                if not hasattr(label, bool_attr):\n",
    "                    continue\n",
    "                if not getattr(label, bool_attr):\n",
    "                    continue\n",
    "\n",
    "                if label.tags is None:\n",
    "                    label.tags = []\n",
    "                if tag_name not in label.tags:\n",
    "                    label.tags.append(tag_name)\n",
    "                    changed = True\n",
    "\n",
    "            if changed:\n",
    "                sample[field] = labels_layer\n",
    "                sample.save()\n",
    "\n",
    "    print(f\"✅ 已根据 `{bool_attr}` 添加 tag: `{tag_name}` (mode={mode})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8a6cf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subdir_path: /home/tianqi/D/01_Projects/01_swd/02_code/pipeline/ultralytics_ty/_ty/01_data/01_16mp_2024_pipeline_data/sw1_0605-0613_07_ok_v2, subdir_name: sw1_0605-0613_07_ok_v2\n",
      " 100% |█████████████████| 165/165 [641.1ms elapsed, 0s remaining, 257.8 samples/s]      \n",
      " 100% |█████████████████| 207/207 [24.6ms elapsed, 0s remaining, 8.4K samples/s]      \n",
      "Converting existing index 'filepath' to unique on dataset '2025.12.03.22.40.41.613960'\n",
      "Converting existing index 'filepath' to unique on dataset 'sw1_0605-0613_07_ok_v2'\n",
      "✅ 已完成添加时间和focus信息\n",
      "✅ 已完成时间和focus信息索引创建\n",
      "subdir_path: /home/tianqi/D/01_Projects/01_swd/02_code/pipeline/ultralytics_ty/_ty/01_data/01_16mp_2024_pipeline_data/ms2_0726-0809_13_ok_v2, subdir_name: ms2_0726-0809_13_ok_v2\n",
      " 100% |███████████████| 1633/1633 [3.9s elapsed, 0s remaining, 321.2 samples/s]      \n",
      " 100% |███████████████| 1713/1713 [152.9ms elapsed, 0s remaining, 11.4K samples/s] \n",
      "Converting existing index 'filepath' to unique on dataset '2025.12.03.22.40.46.354187'\n",
      "Converting existing index 'filepath' to unique on dataset 'ms2_0726-0809_13_ok_v2'\n",
      "✅ 已完成添加时间和focus信息\n",
      "✅ 已完成时间和focus信息索引创建\n",
      "subdir_path: /home/tianqi/D/01_Projects/01_swd/02_code/pipeline/ultralytics_ty/_ty/01_data/01_16mp_2024_pipeline_data/ms1_0809-0823_34_ok_v2, subdir_name: ms1_0809-0823_34_ok_v2\n",
      " 100% |███████████████| 1537/1537 [5.8s elapsed, 0s remaining, 278.5 samples/s]      \n",
      " 100% |███████████████| 1743/1743 [148.6ms elapsed, 0s remaining, 11.8K samples/s] \n",
      "Converting existing index 'filepath' to unique on dataset '2025.12.03.22.40.55.615480'\n",
      "Converting existing index 'filepath' to unique on dataset 'ms1_0809-0823_34_ok_v2'\n",
      "✅ 已完成添加时间和focus信息\n",
      "✅ 已完成时间和focus信息索引创建\n",
      "subdir_path: /home/tianqi/D/01_Projects/01_swd/02_code/pipeline/ultralytics_ty/_ty/01_data/01_16mp_2024_pipeline_data/ms1_0710-0726_36_ok_v2, subdir_name: ms1_0710-0726_36_ok_v2\n",
      " 100% |███████████████| 1552/1552 [10.2s elapsed, 0s remaining, 130.7 samples/s]      \n",
      " 100% |███████████████| 1664/1664 [144.9ms elapsed, 0s remaining, 11.7K samples/s] \n",
      "Converting existing index 'filepath' to unique on dataset '2025.12.03.22.41.10.359142'\n",
      "Converting existing index 'filepath' to unique on dataset 'ms1_0710-0726_36_ok_v2'\n",
      "✅ 已完成添加时间和focus信息\n",
      "✅ 已完成时间和focus信息索引创建\n"
     ]
    }
   ],
   "source": [
    "import fiftyone.utils.coco as fouc  \n",
    "\n",
    "RESET_DATASET = True\n",
    "\n",
    "for subdir_path, subdir_name in zip(subdir_path_list, subdir_name_list):\n",
    "    if subdir_name in fo.list_datasets() and RESET_DATASET:\n",
    "        fo.delete_dataset(subdir_name) \n",
    "    print(f\"subdir_path: {subdir_path}, subdir_name: {subdir_name}\")\n",
    "    dataset = fo.Dataset.from_dir(\n",
    "        dataset_type=fo.types.COCODetectionDataset,\n",
    "        name=f\"{subdir_name}\",\n",
    "        data_path=subdir_path / \"raw_data\",\n",
    "        # labels_path=subdir_path / \"output\" / \"swd_seg_results_coco.json\",   \n",
    "        labels_path=subdir_path / \"output\" / \"01_swd_seg_results_coco.json\",\n",
    "        label_field=\"01_swd_seg_results_coco\",\n",
    "        label_types=\"detections\",\n",
    "    )\n",
    "\n",
    "    # putin rest no annotation image data\n",
    "    dataset.merge_dir(  \n",
    "        dataset_dir=subdir_path / \"raw_data\",  \n",
    "        dataset_type=fo.types.ImageDirectory,  \n",
    "        skip_existing=True,  # 跳过已存在的样本  \n",
    "        insert_new=True,     # 插入新样本  \n",
    "    )\n",
    "\n",
    "    # fouc.add_coco_labels(\n",
    "    #     dataset,\n",
    "    #     label_field=\"02_combined_annotations_dedup\",\n",
    "    #     labels_or_path=str(subdir_path / \"output\" / \"02_combined_annotations_dedup.json\"),\n",
    "    #     categories={1: \"swd\"},\n",
    "    #     label_type=\"detections\",\n",
    "    # )\n",
    "    # fouc.add_coco_labels(\n",
    "    #     dataset,\n",
    "    #     label_field=\"03_coco_with_pose_dot_keypoints\",\n",
    "    #     labels_or_path=str(subdir_path / \"output\" / \"03_coco_with_pose_dot.json\"),\n",
    "    #     categories={1: \"swd\"},\n",
    "    #     label_type=\"keypoints\",\n",
    "    # )\n",
    "\n",
    "    # fouc.add_coco_labels(\n",
    "    #     dataset,\n",
    "    #     label_field=\"03_coco_with_pose_dot_detections\",\n",
    "    #     labels_or_path=str(subdir_path / \"output\" / \"03_coco_with_pose_dot.json\"),\n",
    "    #     categories={1: \"swd\"},\n",
    "    #     label_type=\"detections\",\n",
    "    # )\n",
    "\n",
    "    # attach_dot_to_fiftyone(\n",
    "    #     dataset=dataset,\n",
    "    #     coco_json_path=subdir_path / \"output\" / \"03_coco_with_pose_dot.json\",\n",
    "    #     dot_field=\"04_dot_boxes\",\n",
    "    # )\n",
    "\n",
    "    # add_tags_to_all_labels(dataset, \"is_final_swd\", \"fswd\",\"detections\")\n",
    "\n",
    "    # 添加时间和focus信息,通过file_name获取， 0606_0617_760.jpg 0606_0617表示时间信息 760表示焦点距离\n",
    "    for sample in dataset:\n",
    "        file_name = sample.filepath.split(\"/\")[-1]\n",
    "        sample[\"Date\"] = extract_time_info(file_name)\n",
    "        sample[\"focus\"] = extract_focus_info(file_name)\n",
    "        sample.save()\n",
    "    print(f\"✅ 已完成添加时间和focus信息\")\n",
    "    # 6. 建立索引\n",
    "    dataset.create_index(\"Date\")  \n",
    "    dataset.create_index(\"focus\")\n",
    "    print(f\"✅ 已完成时间和focus信息索引创建\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
