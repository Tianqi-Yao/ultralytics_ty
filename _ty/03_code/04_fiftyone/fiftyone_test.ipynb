{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d9b89ea",
   "metadata": {},
   "source": [
    "# FiftyOne 脚本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47c95175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianqi/miniconda3/envs/fiftyone/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numba: 0.59.1\n",
      "llvmlite: 0.42.0\n",
      "umap: 0.5.6\n"
     ]
    }
   ],
   "source": [
    "import numba, llvmlite, umap\n",
    "\n",
    "print(\"numba:\", numba.__version__)\n",
    "print(\"llvmlite:\", llvmlite.__version__)\n",
    "print(\"umap:\", umap.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a97da6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESET_DATASET = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d05b9174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to FiftyOne on port 5151 at localhost.\n",
      "If you are not connecting to a remote session, you may need to start a new session and specify a port\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?notebook=True&subscription=2be49f3d-2150-4f5b-b678-5d5354de65e7\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7b30292a7a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "dataset_name = fo.list_datasets()[0] if not RESET_DATASET else None\n",
    "if dataset_name is None:\n",
    "    session = fo.launch_app(port=5151)\n",
    "else:\n",
    "    dataset = fo.load_dataset(dataset_name)\n",
    "    print(f\"Default dataset name: {dataset_name}\")\n",
    "    session = fo.launch_app(dataset,port=5151)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "036d0a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jeff_0613-0624_04_ok',\n",
       " 'lloyd_0715-0729_04_ok',\n",
       " 'ms1_0605-0621_40_ok',\n",
       " 'ms1_0710-0726_36_ok',\n",
       " 'ms1_0726-0809_11_ok',\n",
       " 'ms1_0809-0823_34_ok',\n",
       " 'ms2_0726-0809_13_ok',\n",
       " 'ms2_0809-0823_10_ok',\n",
       " 'sw1_0605-0613_07_ok',\n",
       " 'sw1_0711-0725_03_ok',\n",
       " 'sw2_0808-0823_04_ok']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "# 获取所有dataset\n",
    "datasets = fo.list_datasets()\n",
    "display(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18218eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.core.labels as fol\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import fiftyone as fo\n",
    "\n",
    "\n",
    "def attach_dot_to_fiftyone(\n",
    "    dataset: fo.Dataset,\n",
    "    coco_json_path: Path,\n",
    "    dot_field: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    根据 COCO json 里的标准字段：\n",
    "      - annotation[\"attributes\"][\"dot_detections\"]\n",
    "    给 FiftyOne dataset 加一个新的可视化字段：\n",
    "      - sample[dot_field]: fo.Detections（所有 dot box）\n",
    "    \"\"\"\n",
    "    # 1. 读 COCO JSON\n",
    "    with coco_json_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        coco = json.load(f)\n",
    "\n",
    "    images = coco[\"images\"]\n",
    "    annotations = coco[\"annotations\"]\n",
    "\n",
    "    # image_id -> file_name\n",
    "    image_id_to_fname = {img[\"id\"]: img[\"file_name\"] for img in images}\n",
    "\n",
    "    # file_name -> [annotations...] （保持原始顺序）\n",
    "    anns_by_fname = defaultdict(list)\n",
    "    for ann in annotations:\n",
    "        fname = image_id_to_fname[ann[\"image_id\"]]\n",
    "        anns_by_fname[fname].append(ann)\n",
    "\n",
    "    # skeleton（跟 categories 里的 keypoints / skeleton 一致）\n",
    "    dataset.default_skeleton = fo.KeypointSkeleton(\n",
    "        labels=[\"h\", \"lp\", \"rp\"],\n",
    "        edges=[[0, 1], [0, 2]],\n",
    "    )\n",
    "\n",
    "    # 2. 按 sample 遍历，把 pose / dot 加进去\n",
    "    for sample in dataset:\n",
    "        fname = Path(sample.filepath).name\n",
    "        anns = anns_by_fname.get(fname, [])\n",
    "        if not anns:\n",
    "            continue\n",
    "\n",
    "        W = sample.metadata.width\n",
    "        H = sample.metadata.height\n",
    "\n",
    "        dot_dets_list = []\n",
    "\n",
    "        for ann in anns:\n",
    "            # -------- attributes.dot_detections -> fo.Detections --------\n",
    "            attrs = ann.get(\"attributes\") or {}\n",
    "            dot_list = attrs.get(\"dot_detections\") or []\n",
    "            for dot in dot_list:\n",
    "                # bbox: [x, y, w, h] 像素坐标\n",
    "                x, y, w, h = dot[\"bbox\"]\n",
    "                rel_box = [\n",
    "                    x / W,\n",
    "                    y / H,\n",
    "                    w / W,\n",
    "                    h / H,\n",
    "                ]\n",
    "                det = fol.Detection(\n",
    "                    bounding_box=rel_box,\n",
    "                    confidence=dot.get(\"score\", None),\n",
    "                    label=str(\n",
    "                        dot.get(\"category_id\", \"\")\n",
    "                    ),  # 这里用 category_id，或者直接写 \"dot\"\n",
    "                )\n",
    "                dot_dets_list.append(det)\n",
    "\n",
    "        # 3. 挂到 sample 上\n",
    "        if dot_dets_list:\n",
    "            sample[dot_field] = fol.Detections(detections=dot_dets_list)\n",
    "\n",
    "        sample.save()\n",
    "\n",
    "    print(f\"✅ 已将 dot 检测框写入字段: {dot_field}\")\n",
    "\n",
    "\n",
    "def extract_time_info(file_name: str) -> datetime:\n",
    "    time_info = \"_\".join(file_name.split(\"_\")[:-1])\n",
    "    dt = datetime.strptime(time_info, \"%m%d_%H%M\")\n",
    "    return dt.replace(year=2024)  # 假设年份为2024年\n",
    "\n",
    "\n",
    "def extract_focus_info(file_name: str) -> str:\n",
    "    return file_name.split(\"_\")[-1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "087407c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "def add_tags_to_all_labels(\n",
    "    dataset: fo.Dataset,\n",
    "    bool_attr: str,\n",
    "    tag_name: str | None = None,\n",
    "    mode: str = \"both\",\n",
    "):\n",
    "    \"\"\"\n",
    "    给 label 添加 tag，当 label.<bool_attr> 为 True 时打上 tag。\n",
    "\n",
    "    参数:\n",
    "        bool_attr:   label 上的布尔属性名，例如 \"is_final_swd\"\n",
    "        tag_name:    标签名，默认用 bool_attr\n",
    "        mode:        \"keypoints\", \"detections\", 或 \"both\"\n",
    "    \"\"\"\n",
    "    if tag_name is None:\n",
    "        tag_name = bool_attr\n",
    "\n",
    "    mode = mode.lower()\n",
    "    assert mode in (\"keypoints\", \"detections\", \"both\"), \"mode 必须是 keypoints/detections/both\"\n",
    "\n",
    "    label_fields = dataset._get_label_fields()\n",
    "    print(\"所有 label 字段:\", label_fields)\n",
    "\n",
    "    for field in label_fields:\n",
    "        first_label = dataset.first()[field]\n",
    "\n",
    "        # 判断这个字段包含 keypoints 或 detections\n",
    "        is_kp = hasattr(first_label, \"keypoints\")\n",
    "        is_det = hasattr(first_label, \"detections\")\n",
    "\n",
    "        # 根据 mode 决定是否处理该字段\n",
    "        if mode == \"keypoints\" and not is_kp:\n",
    "            continue\n",
    "        if mode == \"detections\" and not is_det:\n",
    "            continue\n",
    "        if mode == \"both\" and not (is_kp or is_det):\n",
    "            continue\n",
    "\n",
    "        for sample in dataset:\n",
    "            labels_layer = sample[field]\n",
    "            if not labels_layer:\n",
    "                continue\n",
    "\n",
    "            # 选择对应的 label 列表\n",
    "            if is_kp:\n",
    "                labels = labels_layer.keypoints\n",
    "            else:\n",
    "                labels = labels_layer.detections\n",
    "\n",
    "            changed = False\n",
    "\n",
    "            for label in labels:\n",
    "                if not hasattr(label, bool_attr):\n",
    "                    continue\n",
    "                if not getattr(label, bool_attr):\n",
    "                    continue\n",
    "\n",
    "                if label.tags is None:\n",
    "                    label.tags = []\n",
    "                if tag_name not in label.tags:\n",
    "                    label.tags.append(tag_name)\n",
    "                    changed = True\n",
    "\n",
    "            if changed:\n",
    "                sample[field] = labels_layer\n",
    "                sample.save()\n",
    "\n",
    "    print(f\"✅ 已根据 `{bool_attr}` 添加 tag: `{tag_name}` (mode={mode})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8a6cf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subdir_path: /home/tianqi/D/01_Projects/01_swd/02_code/pipeline/ultralytics_ty/_ty/01_data/00_test/00_try, subdir_name: 00_try\n",
      " 100% |█████████████████████| 4/4 [99.8ms elapsed, 0s remaining, 40.1 samples/s]     \n",
      " 100% |█████████████████████| 4/4 [12.0ms elapsed, 0s remaining, 334.6 samples/s]    \n",
      "Converting existing index 'filepath' to unique on dataset '2025.12.03.12.31.02.249377'\n",
      "Converting existing index 'filepath' to unique on dataset '00_try'\n",
      "✅ 已将 dot 检测框写入字段: 04_dot_boxes\n",
      "所有 label 字段: ['01_swd_seg_results_coco', '02_combined_annotations_dedup', '03_coco_with_pose_dot_keypoints', '03_coco_with_pose_dot_detections', '04_dot_boxes']\n",
      "✅ 已根据 `is_final_swd` 添加 tag: `fswd` (mode=detections)\n",
      "✅ 已添加时间和焦点信息\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'focus'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fiftyone.utils.coco as fouc  \n",
    "from pathlib import Path\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "\n",
    "subdir_path = Path(\"/home/tianqi/D/01_Projects/01_swd/02_code/pipeline/ultralytics_ty/_ty/01_data/00_test/00_try\")\n",
    "subdir_name = \"00_try\"\n",
    "# subdir_path = Path(\"/home/tianqi/D/01_Projects/01_swd/02_code/pipeline/ultralytics_ty/_ty/01_data/01_16mp_2024_pipeline_data/ms1_0726-0809_11_ok\")\n",
    "# subdir_name = \"ms1_0726-0809_11_ok\"\n",
    "\n",
    "if subdir_name in fo.list_datasets() and RESET_DATASET:\n",
    "    fo.delete_dataset(subdir_name) \n",
    "print(f\"subdir_path: {subdir_path}, subdir_name: {subdir_name}\")\n",
    "\n",
    "\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    dataset_type=fo.types.COCODetectionDataset,\n",
    "    name=f\"{subdir_name}\",\n",
    "    data_path=subdir_path / \"raw_data\",\n",
    "    labels_path=subdir_path / \"output\" / \"01_swd_seg_results_coco.json\",\n",
    "    label_field=\"01_swd_seg_results_coco\",\n",
    "    label_types=\"detections\",\n",
    ")\n",
    "\n",
    "# putin rest no annotation image data\n",
    "dataset.merge_dir(  \n",
    "    dataset_dir=subdir_path / \"raw_data\",  \n",
    "    dataset_type=fo.types.ImageDirectory,  \n",
    "    skip_existing=True,  # 跳过已存在的样本  \n",
    "    insert_new=True,     # 插入新样本  \n",
    ")\n",
    "\n",
    "fouc.add_coco_labels(\n",
    "    dataset,\n",
    "    label_field=\"02_combined_annotations_dedup\",\n",
    "    labels_or_path=str(subdir_path / \"output\" / \"02_combined_annotations_dedup.json\"),\n",
    "    categories={1: \"swd\"},\n",
    "    label_type=\"detections\",\n",
    ")\n",
    "\n",
    "\n",
    "fouc.add_coco_labels(\n",
    "    dataset,\n",
    "    label_field=\"03_coco_with_pose_dot_keypoints\",\n",
    "    labels_or_path=str(subdir_path / \"output\" / \"03_coco_with_pose_dot.json\"),\n",
    "    categories={1: \"swd\"},\n",
    "    label_type=\"keypoints\",\n",
    ")\n",
    "\n",
    "fouc.add_coco_labels(\n",
    "    dataset,\n",
    "    label_field=\"03_coco_with_pose_dot_detections\",\n",
    "    labels_or_path=str(subdir_path / \"output\" / \"03_coco_with_pose_dot.json\"),\n",
    "    categories={1: \"swd\"},\n",
    "    label_type=\"detections\",\n",
    ")\n",
    "\n",
    "\n",
    "attach_dot_to_fiftyone(\n",
    "    dataset=dataset,\n",
    "    coco_json_path=subdir_path / \"output\" / \"03_coco_with_pose_dot.json\",\n",
    "    dot_field=\"04_dot_boxes\",\n",
    ")\n",
    "\n",
    "add_tags_to_all_labels(dataset, \"is_final_swd\", \"fswd\",\"detections\")\n",
    "\n",
    "# 添加时间和focus信息,通过file_name获取， 0606_0617_760.jpg 0606_0617表示时间信息 760表示焦点距离\n",
    "for sample in dataset:\n",
    "    file_name = sample.filepath.split(\"/\")[-1]\n",
    "    sample[\"Date\"] = extract_time_info(file_name)\n",
    "    sample[\"focus\"] = extract_focus_info(file_name)\n",
    "    sample.save()\n",
    "\n",
    "print(\"✅ 已添加时间和焦点信息\")\n",
    "\n",
    "# 6. 建立索引  \n",
    "dataset.create_index(\"Date\")  \n",
    "dataset.create_index(\"focus\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78a0e6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset.list_brain_runs():\n",
    "    print(f\"Deleting existing brain run: {key}\")\n",
    "    # dataset.delete_brain_run(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f68f3a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 00_try\n",
      " 100% |█████████████████████| 4/4 [2.6s elapsed, 0s remaining, 1.6 samples/s]      \n",
      "Generating visualization...\n",
      "UMAP(n_jobs=1, random_state=51, verbose=True)\n",
      "Wed Dec  3 12:31:09 2025 Construct fuzzy simplicial set\n",
      "Wed Dec  3 12:31:09 2025 Finding Nearest Neighbors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianqi/miniconda3/envs/fiftyone/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/tianqi/miniconda3/envs/fiftyone/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  3 12:31:10 2025 Finished Nearest Neighbor Search\n",
      "Wed Dec  3 12:31:11 2025 Construct embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed: 100%| ██████████ 500/500 [00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  500 epochs\n",
      "\tcompleted  50  /  500 epochs\n",
      "\tcompleted  100  /  500 epochs\n",
      "\tcompleted  150  /  500 epochs\n",
      "\tcompleted  200  /  500 epochs\n",
      "\tcompleted  250  /  500 epochs\n",
      "\tcompleted  300  /  500 epochs\n",
      "\tcompleted  350  /  500 epochs\n",
      "\tcompleted  400  /  500 epochs\n",
      "\tcompleted  450  /  500 epochs\n",
      "Wed Dec  3 12:31:11 2025 Finished embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating visualization...\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 333 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 333 samples in 0.038s...\n",
      "[t-SNE] Computed conditional probabilities for sample 333 / 333\n",
      "[t-SNE] Mean sigma: 1.659828\n",
      "[t-SNE] Computed conditional probabilities in 0.004s\n",
      "[t-SNE] Iteration 50: error = 62.4039268, gradient norm = 0.2600984 (50 iterations in 0.031s)\n",
      "[t-SNE] Iteration 100: error = 62.7513695, gradient norm = 0.2857517 (50 iterations in 0.017s)\n",
      "[t-SNE] Iteration 150: error = 62.1042671, gradient norm = 0.2627090 (50 iterations in 0.018s)\n",
      "[t-SNE] Iteration 200: error = 61.2013969, gradient norm = 0.2815717 (50 iterations in 0.018s)\n",
      "[t-SNE] Iteration 250: error = 61.8424644, gradient norm = 0.2846527 (50 iterations in 0.018s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 61.842464\n",
      "[t-SNE] Iteration 300: error = 0.6519693, gradient norm = 0.0046239 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 350: error = 0.5977588, gradient norm = 0.0024639 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 400: error = 0.5761135, gradient norm = 0.0036144 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 450: error = 0.5700947, gradient norm = 0.0006518 (50 iterations in 0.017s)\n",
      "[t-SNE] Iteration 500: error = 0.5683075, gradient norm = 0.0003723 (50 iterations in 0.017s)\n",
      "[t-SNE] Iteration 550: error = 0.5672473, gradient norm = 0.0003838 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 600: error = 0.5635664, gradient norm = 0.0004422 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 650: error = 0.5617539, gradient norm = 0.0003310 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 700: error = 0.5616311, gradient norm = 0.0002421 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 750: error = 0.5613174, gradient norm = 0.0003138 (50 iterations in 0.015s)\n",
      "[t-SNE] Iteration 800: error = 0.5608935, gradient norm = 0.0006288 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 850: error = 0.5602995, gradient norm = 0.0003389 (50 iterations in 0.017s)\n",
      "[t-SNE] Iteration 900: error = 0.5599157, gradient norm = 0.0006140 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 950: error = 0.5598723, gradient norm = 0.0002307 (50 iterations in 0.015s)\n",
      "[t-SNE] Iteration 1000: error = 0.5594956, gradient norm = 0.0003131 (50 iterations in 0.015s)\n",
      "[t-SNE] KL divergence after 1000 iterations: 0.559496\n",
      "Generating visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/tianqi/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/tianqi/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/tianqi/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/tianqi/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_pretrain.pth\" to /home/tianqi/.cache/torch/hub/checkpoints/dinov2_vits14_pretrain.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84.2M/84.2M [00:03<00:00, 27.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 00_try\n",
      " 100% |█████████████████████| 4/4 [2.0s elapsed, 0s remaining, 2.1 samples/s]         \n",
      "Generating visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianqi/miniconda3/envs/fiftyone/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/tianqi/miniconda3/envs/fiftyone/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP(n_jobs=1, random_state=51, verbose=True)\n",
      "Wed Dec  3 12:31:18 2025 Construct fuzzy simplicial set\n",
      "Wed Dec  3 12:31:18 2025 Finding Nearest Neighbors\n",
      "Wed Dec  3 12:31:18 2025 Finished Nearest Neighbor Search\n",
      "Wed Dec  3 12:31:18 2025 Construct embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed:  99%| █████████▉ 494/500 [00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  500 epochs\n",
      "\tcompleted  50  /  500 epochs\n",
      "\tcompleted  100  /  500 epochs\n",
      "\tcompleted  150  /  500 epochs\n",
      "\tcompleted  200  /  500 epochs\n",
      "\tcompleted  250  /  500 epochs\n",
      "\tcompleted  300  /  500 epochs\n",
      "\tcompleted  350  /  500 epochs\n",
      "\tcompleted  400  /  500 epochs\n",
      "\tcompleted  450  /  500 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed: 100%| ██████████ 500/500 [00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  3 12:31:18 2025 Finished embedding\n",
      "Generating visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 333 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 333 samples in 0.028s...\n",
      "[t-SNE] Computed conditional probabilities for sample 333 / 333\n",
      "[t-SNE] Mean sigma: 6.690645\n",
      "[t-SNE] Computed conditional probabilities in 0.006s\n",
      "[t-SNE] Iteration 50: error = 64.1275558, gradient norm = 0.2901095 (50 iterations in 0.038s)\n",
      "[t-SNE] Iteration 100: error = 63.5008202, gradient norm = 0.2938944 (50 iterations in 0.018s)\n",
      "[t-SNE] Iteration 150: error = 65.5059433, gradient norm = 0.2695365 (50 iterations in 0.018s)\n",
      "[t-SNE] Iteration 200: error = 64.6506958, gradient norm = 0.2787073 (50 iterations in 0.018s)\n",
      "[t-SNE] Iteration 250: error = 63.7138062, gradient norm = 0.2685856 (50 iterations in 0.018s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 63.713806\n",
      "[t-SNE] Iteration 300: error = 0.7615527, gradient norm = 0.0027444 (50 iterations in 0.017s)\n",
      "[t-SNE] Iteration 350: error = 0.7017592, gradient norm = 0.0021553 (50 iterations in 0.018s)\n",
      "[t-SNE] Iteration 400: error = 0.6753202, gradient norm = 0.0020725 (50 iterations in 0.018s)\n",
      "[t-SNE] Iteration 450: error = 0.6465154, gradient norm = 0.0065104 (50 iterations in 0.018s)\n",
      "[t-SNE] Iteration 500: error = 0.6328986, gradient norm = 0.0011826 (50 iterations in 0.018s)\n",
      "[t-SNE] Iteration 550: error = 0.6244843, gradient norm = 0.0012262 (50 iterations in 0.017s)\n",
      "[t-SNE] Iteration 600: error = 0.6172994, gradient norm = 0.0012954 (50 iterations in 0.017s)\n",
      "[t-SNE] Iteration 650: error = 0.6111642, gradient norm = 0.0013303 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 700: error = 0.6052725, gradient norm = 0.0012568 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 750: error = 0.6000053, gradient norm = 0.0013189 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 800: error = 0.5943695, gradient norm = 0.0014491 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 850: error = 0.5896532, gradient norm = 0.0012642 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 900: error = 0.5843588, gradient norm = 0.0014502 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 950: error = 0.5788724, gradient norm = 0.0016254 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 1000: error = 0.5736642, gradient norm = 0.0013989 (50 iterations in 0.016s)\n",
      "[t-SNE] KL divergence after 1000 iterations: 0.573664\n",
      "Generating visualization...\n",
      "Downloading model from 'https://download.pytorch.org/models/resnet50-19c8e357.pth'...\n",
      " 100% |████|  782.0Mb/782.0Mb [3.8s elapsed, 0s remaining, 174.3Mb/s]      \n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /home/tianqi/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97.8M/97.8M [00:03<00:00, 29.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 00_try\n",
      " 100% |█████████████████████| 4/4 [2.9s elapsed, 0s remaining, 1.4 samples/s]      \n",
      "Generating visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianqi/miniconda3/envs/fiftyone/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/tianqi/miniconda3/envs/fiftyone/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP(n_jobs=1, random_state=51, verbose=True)\n",
      "Wed Dec  3 12:31:30 2025 Construct fuzzy simplicial set\n",
      "Wed Dec  3 12:31:30 2025 Finding Nearest Neighbors\n",
      "Wed Dec  3 12:31:30 2025 Finished Nearest Neighbor Search\n",
      "Wed Dec  3 12:31:30 2025 Construct embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed: 100%| ██████████ 500/500 [00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  500 epochs\n",
      "\tcompleted  50  /  500 epochs\n",
      "\tcompleted  100  /  500 epochs\n",
      "\tcompleted  150  /  500 epochs\n",
      "\tcompleted  200  /  500 epochs\n",
      "\tcompleted  250  /  500 epochs\n",
      "\tcompleted  300  /  500 epochs\n",
      "\tcompleted  350  /  500 epochs\n",
      "\tcompleted  400  /  500 epochs\n",
      "\tcompleted  450  /  500 epochs\n",
      "Wed Dec  3 12:31:31 2025 Finished embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating visualization...\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 333 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 333 samples in 0.025s...\n",
      "[t-SNE] Computed conditional probabilities for sample 333 / 333\n",
      "[t-SNE] Mean sigma: 3.001317\n",
      "[t-SNE] Computed conditional probabilities in 0.006s\n",
      "[t-SNE] Iteration 50: error = 63.3365326, gradient norm = 0.2999157 (50 iterations in 0.044s)\n",
      "[t-SNE] Iteration 100: error = 64.2667694, gradient norm = 0.2764226 (50 iterations in 0.017s)\n",
      "[t-SNE] Iteration 150: error = 64.8122787, gradient norm = 0.2716970 (50 iterations in 0.018s)\n",
      "[t-SNE] Iteration 200: error = 64.8288116, gradient norm = 0.2669099 (50 iterations in 0.019s)\n",
      "[t-SNE] Iteration 250: error = 63.6659813, gradient norm = 0.2631999 (50 iterations in 0.018s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 63.665981\n",
      "[t-SNE] Iteration 300: error = 0.7103608, gradient norm = 0.0029808 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 350: error = 0.6533123, gradient norm = 0.0020608 (50 iterations in 0.015s)\n",
      "[t-SNE] Iteration 400: error = 0.6275380, gradient norm = 0.0013482 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 450: error = 0.6146750, gradient norm = 0.0014676 (50 iterations in 0.015s)\n",
      "[t-SNE] Iteration 500: error = 0.6077080, gradient norm = 0.0022467 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 550: error = 0.6008945, gradient norm = 0.0006118 (50 iterations in 0.017s)\n",
      "[t-SNE] Iteration 600: error = 0.5992067, gradient norm = 0.0006469 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 650: error = 0.5946336, gradient norm = 0.0011619 (50 iterations in 0.014s)\n",
      "[t-SNE] Iteration 700: error = 0.5908501, gradient norm = 0.0008474 (50 iterations in 0.014s)\n",
      "[t-SNE] Iteration 750: error = 0.5850776, gradient norm = 0.0009056 (50 iterations in 0.015s)\n",
      "[t-SNE] Iteration 800: error = 0.5801488, gradient norm = 0.0010584 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 850: error = 0.5747225, gradient norm = 0.0005915 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 900: error = 0.5739039, gradient norm = 0.0001984 (50 iterations in 0.015s)\n",
      "[t-SNE] Iteration 950: error = 0.5736663, gradient norm = 0.0002354 (50 iterations in 0.014s)\n",
      "[t-SNE] Iteration 1000: error = 0.5734721, gradient norm = 0.0002862 (50 iterations in 0.014s)\n",
      "[t-SNE] KL divergence after 1000 iterations: 0.573472\n",
      "Generating visualization...\n",
      "Dataset: 00_try\n",
      " 100% |█████████████████████| 4/4 [2.8s elapsed, 0s remaining, 1.4 samples/s]      \n",
      "Generating visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianqi/miniconda3/envs/fiftyone/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/tianqi/miniconda3/envs/fiftyone/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP(n_jobs=1, random_state=51, verbose=True)\n",
      "Wed Dec  3 12:31:35 2025 Construct fuzzy simplicial set\n",
      "Wed Dec  3 12:31:35 2025 Finding Nearest Neighbors\n",
      "Wed Dec  3 12:31:35 2025 Finished Nearest Neighbor Search\n",
      "Wed Dec  3 12:31:35 2025 Construct embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed:  98%| █████████▊ 490/500 [00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  500 epochs\n",
      "\tcompleted  50  /  500 epochs\n",
      "\tcompleted  100  /  500 epochs\n",
      "\tcompleted  150  /  500 epochs\n",
      "\tcompleted  200  /  500 epochs\n",
      "\tcompleted  250  /  500 epochs\n",
      "\tcompleted  300  /  500 epochs\n",
      "\tcompleted  350  /  500 epochs\n",
      "\tcompleted  400  /  500 epochs\n",
      "\tcompleted  450  /  500 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed: 100%| ██████████ 500/500 [00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  3 12:31:35 2025 Finished embedding\n",
      "Generating visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 333 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 333 samples in 0.027s...\n",
      "[t-SNE] Computed conditional probabilities for sample 333 / 333\n",
      "[t-SNE] Mean sigma: 2.082960\n",
      "[t-SNE] Computed conditional probabilities in 0.007s\n",
      "[t-SNE] Iteration 50: error = 64.2189102, gradient norm = 0.2931502 (50 iterations in 0.066s)\n",
      "[t-SNE] Iteration 100: error = 63.3232384, gradient norm = 0.2744624 (50 iterations in 0.020s)\n",
      "[t-SNE] Iteration 150: error = 63.1111145, gradient norm = 0.3054615 (50 iterations in 0.020s)\n",
      "[t-SNE] Iteration 200: error = 64.3853226, gradient norm = 0.2803025 (50 iterations in 0.017s)\n",
      "[t-SNE] Iteration 250: error = 64.6651459, gradient norm = 0.2768654 (50 iterations in 0.017s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 64.665146\n",
      "[t-SNE] Iteration 300: error = 0.7335223, gradient norm = 0.0033968 (50 iterations in 0.015s)\n",
      "[t-SNE] Iteration 350: error = 0.6853495, gradient norm = 0.0031633 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 400: error = 0.6717398, gradient norm = 0.0012335 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 450: error = 0.6646994, gradient norm = 0.0008507 (50 iterations in 0.015s)\n",
      "[t-SNE] Iteration 500: error = 0.6502737, gradient norm = 0.0005582 (50 iterations in 0.015s)\n",
      "[t-SNE] Iteration 550: error = 0.6498571, gradient norm = 0.0002577 (50 iterations in 0.015s)\n",
      "[t-SNE] Iteration 600: error = 0.6494475, gradient norm = 0.0002073 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 650: error = 0.6491106, gradient norm = 0.0001972 (50 iterations in 0.015s)\n",
      "[t-SNE] Iteration 700: error = 0.6489336, gradient norm = 0.0003591 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 750: error = 0.6485646, gradient norm = 0.0001834 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 800: error = 0.6485736, gradient norm = 0.0001770 (50 iterations in 0.015s)\n",
      "[t-SNE] Iteration 850: error = 0.6483127, gradient norm = 0.0004125 (50 iterations in 0.015s)\n",
      "[t-SNE] Iteration 900: error = 0.6479511, gradient norm = 0.0003661 (50 iterations in 0.015s)\n",
      "[t-SNE] Iteration 950: error = 0.6481186, gradient norm = 0.0001886 (50 iterations in 0.015s)\n",
      "[t-SNE] Iteration 1000: error = 0.6464052, gradient norm = 0.0001410 (50 iterations in 0.016s)\n",
      "[t-SNE] KL divergence after 1000 iterations: 0.646405\n",
      "Generating visualization...\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.brain as fob\n",
    "\n",
    "label_field = \"01_swd_seg_results_coco\"\n",
    "\n",
    "# embedding模型\n",
    "models = [\"clip-vit-base32-torch\", \"dinov2-vits14-torch\", \"resnet50-imagenet-torch\", \"mobilenet-v2-imagenet-torch\"]\n",
    "# 降维方法\n",
    "dim_reduction_methods = [\"umap\", \"tsne\", \"pca\"]\n",
    "\n",
    "embeddings_fields = [\"emb_clip\", \"emb_dinov2\", \"emb_resnet50\", \"emb_mobilenet\"]\n",
    "\n",
    "datasets = [\"00_try\"]\n",
    "\n",
    "# 清理已有的 brain runs，避免冲突\n",
    "for dataset_name in datasets:\n",
    "    dataset = fo.load_dataset(dataset_name)\n",
    "    for key in dataset.list_brain_runs():\n",
    "        dataset.delete_brain_run(key)\n",
    "\n",
    "\n",
    "for model_name, emb_field in zip(models, embeddings_fields):\n",
    "    model = foz.load_zoo_model(model_name)\n",
    "\n",
    "    for dataset_name in datasets:\n",
    "        print(f\"Dataset: {dataset_name}\")\n",
    "        dataset = fo.load_dataset(dataset_name)\n",
    "\n",
    "        # 1) 对每个 ann 直接算 patch embedding（按 bbox/mask 裁剪，不导出图片）\n",
    "        dataset.compute_patch_embeddings(\n",
    "            model,\n",
    "            patches_field=label_field,   # 关键：按这个字段里的 bbox/mask 作为 patch\n",
    "            embeddings_field=emb_field,      # embedding 存在每个 ann 的 .emb 里\n",
    "        )\n",
    "\n",
    "        for method in dim_reduction_methods:\n",
    "            brain_key = f\"patches_{model_name.split('-')[0]}_{method}_v1\"\n",
    "\n",
    "            # 2) 对所有 patch 做 降维 可视化\n",
    "            fob.compute_visualization(\n",
    "                dataset,\n",
    "                patches_field=label_field,   # 告诉 brain 这是 patch 字段\n",
    "                embeddings=emb_field,            # 用上一步算好的 embedding 字段\n",
    "                method=method,                # 先用 pca，规避 umap/numba 问题\n",
    "                seed=51,\n",
    "                brain_key=brain_key,  # 每个 dataset 自己有一份同名 brain_key 就行\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
