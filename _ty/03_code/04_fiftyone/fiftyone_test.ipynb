{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d9b89ea",
   "metadata": {},
   "source": [
    "# FiftyOne 脚本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c95175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba, llvmlite, umap\n",
    "\n",
    "print(\"numba:\", numba.__version__)\n",
    "print(\"llvmlite:\", llvmlite.__version__)\n",
    "print(\"umap:\", umap.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a97da6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESET_DATASET = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05b9174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "dataset_name = fo.list_datasets()[0] if not RESET_DATASET else None\n",
    "if dataset_name is None:\n",
    "    session = fo.launch_app(port=5151)\n",
    "else:\n",
    "    dataset = fo.load_dataset(dataset_name)\n",
    "    print(f\"Default dataset name: {dataset_name}\")\n",
    "    session = fo.launch_app(dataset,port=5151)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036d0a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "# 获取所有dataset\n",
    "datasets = fo.list_datasets()\n",
    "display(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18218eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.core.labels as fol\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import fiftyone as fo\n",
    "\n",
    "\n",
    "def attach_dot_to_fiftyone(\n",
    "    dataset: fo.Dataset,\n",
    "    coco_json_path: Path,\n",
    "    dot_field: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    根据 COCO json 里的标准字段：\n",
    "      - annotation[\"attributes\"][\"dot_detections\"]\n",
    "    给 FiftyOne dataset 加一个新的可视化字段：\n",
    "      - sample[dot_field]: fo.Detections（所有 dot box）\n",
    "    \"\"\"\n",
    "    # 1. 读 COCO JSON\n",
    "    with coco_json_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        coco = json.load(f)\n",
    "\n",
    "    images = coco[\"images\"]\n",
    "    annotations = coco[\"annotations\"]\n",
    "\n",
    "    # image_id -> file_name\n",
    "    image_id_to_fname = {img[\"id\"]: img[\"file_name\"] for img in images}\n",
    "\n",
    "    # file_name -> [annotations...] （保持原始顺序）\n",
    "    anns_by_fname = defaultdict(list)\n",
    "    for ann in annotations:\n",
    "        fname = image_id_to_fname[ann[\"image_id\"]]\n",
    "        anns_by_fname[fname].append(ann)\n",
    "\n",
    "    # skeleton（跟 categories 里的 keypoints / skeleton 一致）\n",
    "    dataset.default_skeleton = fo.KeypointSkeleton(\n",
    "        labels=[\"h\", \"lp\", \"rp\"],\n",
    "        edges=[[0, 1], [0, 2]],\n",
    "    )\n",
    "\n",
    "    # 2. 按 sample 遍历，把 pose / dot 加进去\n",
    "    for sample in dataset:\n",
    "        fname = Path(sample.filepath).name\n",
    "        anns = anns_by_fname.get(fname, [])\n",
    "        if not anns:\n",
    "            continue\n",
    "\n",
    "        W = sample.metadata.width\n",
    "        H = sample.metadata.height\n",
    "\n",
    "        dot_dets_list = []\n",
    "\n",
    "        for ann in anns:\n",
    "            # -------- attributes.dot_detections -> fo.Detections --------\n",
    "            attrs = ann.get(\"attributes\") or {}\n",
    "            dot_list = attrs.get(\"dot_detections\") or []\n",
    "            for dot in dot_list:\n",
    "                # bbox: [x, y, w, h] 像素坐标\n",
    "                x, y, w, h = dot[\"bbox\"]\n",
    "                rel_box = [\n",
    "                    x / W,\n",
    "                    y / H,\n",
    "                    w / W,\n",
    "                    h / H,\n",
    "                ]\n",
    "                det = fol.Detection(\n",
    "                    bounding_box=rel_box,\n",
    "                    confidence=dot.get(\"score\", None),\n",
    "                    label=str(\n",
    "                        dot.get(\"category_id\", \"\")\n",
    "                    ),  # 这里用 category_id，或者直接写 \"dot\"\n",
    "                )\n",
    "                dot_dets_list.append(det)\n",
    "\n",
    "        # 3. 挂到 sample 上\n",
    "        if dot_dets_list:\n",
    "            sample[dot_field] = fol.Detections(detections=dot_dets_list)\n",
    "\n",
    "        sample.save()\n",
    "\n",
    "    print(f\"✅ 已将 dot 检测框写入字段: {dot_field}\")\n",
    "\n",
    "\n",
    "def extract_time_info(file_name: str) -> datetime:\n",
    "    time_info = \"_\".join(file_name.split(\"_\")[:-1])\n",
    "    dt = datetime.strptime(time_info, \"%m%d_%H%M\")\n",
    "    return dt.replace(year=2024)  # 假设年份为2024年\n",
    "\n",
    "\n",
    "def extract_focus_info(file_name: str) -> str:\n",
    "    return file_name.split(\"_\")[-1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087407c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "def add_tags_to_all_labels(\n",
    "    dataset: fo.Dataset,\n",
    "    bool_attr: str,\n",
    "    tag_name: str | None = None,\n",
    "    mode: str = \"both\",\n",
    "):\n",
    "    \"\"\"\n",
    "    给 label 添加 tag，当 label.<bool_attr> 为 True 时打上 tag。\n",
    "\n",
    "    参数:\n",
    "        bool_attr:   label 上的布尔属性名，例如 \"is_final_swd\"\n",
    "        tag_name:    标签名，默认用 bool_attr\n",
    "        mode:        \"keypoints\", \"detections\", 或 \"both\"\n",
    "    \"\"\"\n",
    "    if tag_name is None:\n",
    "        tag_name = bool_attr\n",
    "\n",
    "    mode = mode.lower()\n",
    "    assert mode in (\"keypoints\", \"detections\", \"both\"), \"mode 必须是 keypoints/detections/both\"\n",
    "\n",
    "    label_fields = dataset._get_label_fields()\n",
    "    print(\"所有 label 字段:\", label_fields)\n",
    "\n",
    "    for field in label_fields:\n",
    "        first_label = dataset.first()[field]\n",
    "\n",
    "        # 判断这个字段包含 keypoints 或 detections\n",
    "        is_kp = hasattr(first_label, \"keypoints\")\n",
    "        is_det = hasattr(first_label, \"detections\")\n",
    "\n",
    "        # 根据 mode 决定是否处理该字段\n",
    "        if mode == \"keypoints\" and not is_kp:\n",
    "            continue\n",
    "        if mode == \"detections\" and not is_det:\n",
    "            continue\n",
    "        if mode == \"both\" and not (is_kp or is_det):\n",
    "            continue\n",
    "\n",
    "        for sample in dataset:\n",
    "            labels_layer = sample[field]\n",
    "            if not labels_layer:\n",
    "                continue\n",
    "\n",
    "            # 选择对应的 label 列表\n",
    "            if is_kp:\n",
    "                labels = labels_layer.keypoints\n",
    "            else:\n",
    "                labels = labels_layer.detections\n",
    "\n",
    "            changed = False\n",
    "\n",
    "            for label in labels:\n",
    "                if not hasattr(label, bool_attr):\n",
    "                    continue\n",
    "                if not getattr(label, bool_attr):\n",
    "                    continue\n",
    "\n",
    "                if label.tags is None:\n",
    "                    label.tags = []\n",
    "                if tag_name not in label.tags:\n",
    "                    label.tags.append(tag_name)\n",
    "                    changed = True\n",
    "\n",
    "            if changed:\n",
    "                sample[field] = labels_layer\n",
    "                sample.save()\n",
    "\n",
    "    print(f\"✅ 已根据 `{bool_attr}` 添加 tag: `{tag_name}` (mode={mode})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a6cf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.utils.coco as fouc  \n",
    "from pathlib import Path\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "\n",
    "subdir_path = Path(\"/home/tianqi/D/01_Projects/01_swd/02_code/pipeline/ultralytics_ty/_ty/01_data/00_test/00_try\")\n",
    "subdir_name = \"00_try\"\n",
    "# subdir_path = Path(\"/home/tianqi/D/01_Projects/01_swd/02_code/pipeline/ultralytics_ty/_ty/01_data/01_16mp_2024_pipeline_data/ms1_0726-0809_11_ok\")\n",
    "# subdir_name = \"ms1_0726-0809_11_ok\"\n",
    "\n",
    "if subdir_name in fo.list_datasets() and RESET_DATASET:\n",
    "    fo.delete_dataset(subdir_name) \n",
    "print(f\"subdir_path: {subdir_path}, subdir_name: {subdir_name}\")\n",
    "\n",
    "\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    dataset_type=fo.types.COCODetectionDataset,\n",
    "    name=f\"{subdir_name}\",\n",
    "    data_path=subdir_path / \"raw_data\",\n",
    "    labels_path=subdir_path / \"output\" / \"01_swd_seg_results_coco.json\",\n",
    "    label_field=\"01_swd_seg_results_coco\",\n",
    "    label_types=\"detections\",\n",
    ")\n",
    "\n",
    "# putin rest no annotation image data\n",
    "dataset.merge_dir(  \n",
    "    dataset_dir=subdir_path / \"raw_data\",  \n",
    "    dataset_type=fo.types.ImageDirectory,  \n",
    "    skip_existing=True,  # 跳过已存在的样本  \n",
    "    insert_new=True,     # 插入新样本  \n",
    ")\n",
    "\n",
    "fouc.add_coco_labels(\n",
    "    dataset,\n",
    "    label_field=\"02_combined_annotations_dedup\",\n",
    "    labels_or_path=str(subdir_path / \"output\" / \"02_combined_annotations_dedup.json\"),\n",
    "    categories={1: \"swd\"},\n",
    "    label_type=\"detections\",\n",
    ")\n",
    "\n",
    "\n",
    "fouc.add_coco_labels(\n",
    "    dataset,\n",
    "    label_field=\"03_coco_with_pose_dot_keypoints\",\n",
    "    labels_or_path=str(subdir_path / \"output\" / \"03_coco_with_pose_dot.json\"),\n",
    "    categories={1: \"swd\"},\n",
    "    label_type=\"keypoints\",\n",
    ")\n",
    "\n",
    "fouc.add_coco_labels(\n",
    "    dataset,\n",
    "    label_field=\"03_coco_with_pose_dot_detections\",\n",
    "    labels_or_path=str(subdir_path / \"output\" / \"03_coco_with_pose_dot.json\"),\n",
    "    categories={1: \"swd\"},\n",
    "    label_type=\"detections\",\n",
    ")\n",
    "\n",
    "\n",
    "attach_dot_to_fiftyone(\n",
    "    dataset=dataset,\n",
    "    coco_json_path=subdir_path / \"output\" / \"03_coco_with_pose_dot.json\",\n",
    "    dot_field=\"04_dot_boxes\",\n",
    ")\n",
    "\n",
    "add_tags_to_all_labels(dataset, \"is_final_swd\", \"fswd\",\"detections\")\n",
    "\n",
    "# 添加时间和focus信息,通过file_name获取， 0606_0617_760.jpg 0606_0617表示时间信息 760表示焦点距离\n",
    "for sample in dataset:\n",
    "    file_name = sample.filepath.split(\"/\")[-1]\n",
    "    sample[\"Date\"] = extract_time_info(file_name)\n",
    "    sample[\"focus\"] = extract_focus_info(file_name)\n",
    "    sample.save()\n",
    "\n",
    "print(\"✅ 已添加时间和焦点信息\")\n",
    "\n",
    "# 6. 建立索引  \n",
    "dataset.create_index(\"Date\")  \n",
    "dataset.create_index(\"focus\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a0e6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset.list_brain_runs():\n",
    "    print(f\"Deleting existing brain run: {key}\")\n",
    "    # dataset.delete_brain_run(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68f3a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.brain as fob\n",
    "\n",
    "label_field = \"01_swd_seg_results_coco\"\n",
    "\n",
    "# 模型只加载一次就行\n",
    "model = foz.load_zoo_model(\"clip-vit-base32-torch\")\n",
    "# model = foz.load_zoo_model(\"mobilenet-v2-imagenet-torch\")\n",
    "\n",
    "dataset_name = \"00_try\"\n",
    "print(f\"Dataset: {dataset_name}\")\n",
    "dataset = fo.load_dataset(dataset_name)\n",
    "\n",
    "# 1) 对每个 ann 直接算 patch embedding（按 bbox/mask 裁剪，不导出图片）\n",
    "dataset.compute_patch_embeddings(\n",
    "    model,\n",
    "    patches_field=label_field,   # 关键：按这个字段里的 bbox/mask 作为 patch\n",
    "    embeddings_field=\"emb\",      # embedding 存在每个 ann 的 .emb 里\n",
    "    # force_recompute=True,        # 如需覆盖之前的结果可以留着\n",
    ")\n",
    "\n",
    "# 2) 对所有 patch 做 PCA 可视化\n",
    "fob.compute_visualization(\n",
    "    dataset,\n",
    "    patches_field=label_field,   # 告诉 brain 这是 patch 字段\n",
    "    embeddings=\"emb\",            # 用上一步算好的 embedding 字段\n",
    "    method=\"pca\",                # 先用 pca，规避 umap/numba 问题\n",
    "    seed=51,\n",
    "    brain_key=\"patches_pca_v1\",  # 每个 dataset 自己有一份同名 brain_key 就行\n",
    ")\n",
    "\n",
    "# 3) 对所有 patch 做 tsne 可视化\n",
    "fob.compute_visualization(\n",
    "    dataset,\n",
    "    patches_field=label_field,   # 告诉 brain 这是 patch 字段\n",
    "    embeddings=\"emb\",            # 用上一步算好的 embedding 字段\n",
    "    method=\"tsne\",                # 先用 pca，规避 umap/numba 问题\n",
    "    seed=51,\n",
    "    brain_key=\"patches_tsne_v1\",  # 每个 dataset 自己有一份同名 brain_key 就行\n",
    ")\n",
    "\n",
    "# 4) 对所有 patch 做 umap 可视化\n",
    "fob.compute_visualization(\n",
    "    dataset,\n",
    "    patches_field=label_field,   # 告诉 brain 这是 patch 字段\n",
    "    embeddings=\"emb\",            # 用上一步算好的 embedding 字段\n",
    "    method=\"umap\",                # 先用 pca，规避 umap/numba 问题\n",
    "    seed=51,\n",
    "    brain_key=\"patches_umap_v1\",  # 每个 dataset 自己有一份同名 brain_key 就行\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
