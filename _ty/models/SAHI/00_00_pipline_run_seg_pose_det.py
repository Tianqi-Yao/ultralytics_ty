# %% [markdown]
# # é€šè¿‡segæ¨¡å‹å°†ç–‘ä¼¼SWDçš„objectæ ‡æ³¨å‡ºæ¥ã€‚

# %%
version = "v1"
run_type = "pose_and_det"  # "pose_and_det" or "cls"

# éœ€è¦è¿è¡Œçš„stepåˆ—è¡¨
steps_to_run = [
    "run_clean_and_slice_images_on_dirs",           # Step 1 æ¸…ç†åå›¾å¹¶åˆ‡ç‰‡å¤§å›¾åˆ°640*640å°å›¾
    "process_sliced_images_with_yolo_seg",          # Step 2 ä½¿ç”¨YOLO-segæ¨¡å‹å¤„ç†640*640åˆ‡ç‰‡å›¾åƒ
    "combine_sliced_predictions",                   # Step 3 åˆå¹¶segé¢„æµ‹ç»“æœï¼Œå›åˆ°åŸå›¾ã€‚ åŒæ—¶åˆ‡å‡ºobjectså°å›¾
]

# %% [markdown]
# # Step_0 æŸ¥çœ‹æ ¹ç›®å½•ä¸‹éœ€è¦è¿è¡Œçš„æ–‡ä»¶å¤¹ 
# é€‰æ‹©å«æœ‰ raw_data å›¾ç‰‡çš„ *_data ç›®å½•
# 
# ![image.png](attachment:image.png)
# 

# %%
# é€‰æ‹©æ•°æ®ç›®å½•çš„æ ¸å¿ƒä»£ç 
from pathlib import Path

def select_data_dirs(root_dir: Path, end_with: str = "_data"):
    # === 1) éå†æ‰€æœ‰å­ç›®å½• ===
    sub_dirs = list(root_dir.glob("*/*" + end_with))

    if not sub_dirs:
        print(f"æ²¡æœ‰æ‰¾åˆ° *{end_with} ç›®å½•")
        return []

    print(f"æ‰¾åˆ°ä»¥ä¸‹ {end_with} æ•°æ®é›†ï¼š")
    for i, d in enumerate(sub_dirs):
        print(f"[{i}] {d}")

    # === 2) è®©ç”¨æˆ·é€‰æ‹©è¦è·‘çš„ç›®å½• ===
    idx_str = input("è¯·è¾“å…¥è¦å¤„ç†çš„ç¼–å· (å¤šä¸ªç”¨é€—å·åˆ†éš”, å›è½¦é»˜è®¤å…¨é€‰): ").strip()
    if idx_str:
        indices = [int(x) for x in idx_str.split(",")]
        chosen_dirs = [sub_dirs[i] for i in indices]
    else:
        chosen_dirs = sub_dirs

    print(f"å°†å¤„ç†ä»¥ä¸‹ {end_with} ç›®å½•ï¼š")
    for i, d in enumerate(chosen_dirs):
        print(f"- {i+1}. {d}")

    # === 3) ç­›é€‰æ‰æ²¡æœ‰ raw_data å›¾ç‰‡çš„ç›®å½• ===
    chosen_dirs = [
        d for d in chosen_dirs
        if (d.parent / "raw_data").exists() and any((d.parent / "raw_data").glob("*.jpg"))
    ]

    if not chosen_dirs:
        print(f"æ²¡æœ‰æ‰¾åˆ°åŒ…å«å›¾ç‰‡çš„ *{end_with} ç›®å½•")
        return []

    return chosen_dirs

# %%
root_dir = Path("/workspace/models/SAHI/run_v8")
chosen_dirs = select_data_dirs(root_dir, end_with="_data")
print("æœ€ç»ˆç¡®è®¤çš„ç›®å½•ï¼š", chosen_dirs)
if not chosen_dirs:
    raise ValueError("æ²¡æœ‰é€‰æ‹©ä»»ä½•ç›®å½•ï¼Œç¨‹åºç»ˆæ­¢ã€‚")

# %% [markdown]
# # Step_1 å°†æ–‡ä»¶å¤¹ä¸­çš„RAWå›¾ç‰‡å…¨éƒ¨åˆ‡ç‰‡640x640å¹¶ä¿å­˜
# 
# ### è¾“å…¥
# ![image.png](attachment:image.png)
# 
# ### è¾“å‡º
# ![image-2.png](attachment:image-2.png)

# %%
# Step 1 æ¸…ç†åå›¾å¹¶åˆ‡ç‰‡

import os
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, Any, List, Optional, Tuple

import numpy as np
import cv2

# ============================================================
# åŸºæœ¬é…ç½®
# ============================================================
IMG_EXTS = (".jpg", ".jpeg", ".png", ".bmp", ".tif", ".tiff", ".webp")


# ============================================================
# 1) å•å¼ å›¾æ— æŸåˆ‡ç‰‡ï¼ˆè¾“å‡º PNGï¼‰
# ============================================================
def slice_image_cv2(
    image_path: Path,
    output_dir: Path,
    tile_h: int = 640,
    tile_w: int = 640,
    overlap: float = 0.2,
    out_ext: str = ".jpg",           # âœ… JPGï¼šæœ‰æŸ
    jpeg_quality: int = 95,          # å¯¹ PNG æ— æ•ˆï¼Œä¿ç•™å‚æ•°ä¾¿äºå…¼å®¹
    png_compression: int = 3,        # 0(æœ€å¿«,å¤§)~9(æœ€æ…¢,å°)ï¼Œ3~5 è¾ƒå‡è¡¡
    keep_small_edge: bool = True,    # æœ«ç«¯ä¸è¶³ä¸€ç‰‡æ—¶ä»ä¿å­˜å°ç‰‡
) -> int:
    """
    è¯»å–ä¸€å¼ å›¾å¹¶åˆ‡ç‰‡åˆ° output_dirï¼Œè¿”å›ä¿å­˜çš„åˆ‡ç‰‡æ•°ã€‚
    æ–‡ä»¶å‘½åï¼š<stem>_x0_y0_x1_y1.<ext>ï¼ˆä¸ SAHI åŸºæœ¬å…¼å®¹ï¼‰
    """
    # è¯»å›¾ï¼šnp.fromfile + imdecode æ›´ç¨³æ›´å¿«ï¼ˆå…¼å®¹ä¸­æ–‡è·¯å¾„ç­‰ï¼‰
    buf = np.fromfile(str(image_path), dtype=np.uint8)
    if buf.size == 0:
        return 0
    img = cv2.imdecode(buf, cv2.IMREAD_COLOR)
    if img is None:
        return 0

    H, W = img.shape[:2]
    out_ext = out_ext.lower()
    os.makedirs(output_dir, exist_ok=True)

    # æ­¥é•¿ï¼ˆå¸¦é‡å ï¼‰
    overlap = max(0.0, min(0.99, overlap))
    sh = max(1, int(round(tile_h * (1.0 - overlap))))
    sw = max(1, int(round(tile_w * (1.0 - overlap))))

    # ç”Ÿæˆèµ·ç‚¹ï¼Œç¡®ä¿å³/ä¸‹è¾¹ç¼˜è¦†ç›–
    ys = list(range(0, max(1, H - tile_h + 1), sh))
    xs = list(range(0, max(1, W - tile_w + 1), sw))
    if keep_small_edge:
        if ys[-1] != max(0, H - tile_h):
            ys.append(max(0, H - tile_h))
        if xs[-1] != max(0, W - tile_w):
            xs.append(max(0, W - tile_w))

    # å†™å›¾å‚æ•°
    if out_ext in (".jpg", ".jpeg"):
        imwrite_params = [cv2.IMWRITE_JPEG_QUALITY, int(jpeg_quality)]
    elif out_ext == ".png":
        imwrite_params = [cv2.IMWRITE_PNG_COMPRESSION, int(png_compression)]
    else:
        imwrite_params = []

    stem = image_path.stem
    saved = 0

    for y0 in ys:
        y1 = min(y0 + tile_h, H)
        for x0 in xs:
            x1 = min(x0 + tile_w, W)
            crop = img[y0:y1, x0:x1]
            out_name = f"{stem}_{x0}_{y0}_{x1}_{y1}{out_ext}"
            out_path = output_dir / out_name
            try:
                cv2.imwrite(str(out_path), crop, imwrite_params)
                saved += 1
            except Exception:
                pass
    return saved


# ============================================================
# 2) æ–‡ä»¶å¤¹æ‰¹é‡åˆ‡ç‰‡ï¼ˆå¹¶è¡ŒæŒ‰â€œå›¾â€ï¼‰
# ============================================================
def slice_folder_cv2(
    input_folder: Path,
    output_folder: Optional[Path] = None,
    tile_h: int = 640,
    tile_w: int = 640,
    overlap: float = 0.2,
    out_ext: str = ".jpg",           # âœ… é»˜è®¤ JPGï¼šæœ‰æŸ
    jpeg_quality: int = 95,
    png_compression: int = 3,
    recurse: bool = False,
    max_workers: Optional[int] = None,
) -> Dict[str, int]:
    """
    æ‰¹é‡åˆ‡å›¾ï¼›è¿”å› {'images':N, 'tiles':M, 'failed':K}
    """
    if output_folder is None:
        output_folder = input_folder.parent / f"{input_folder.name}_sliced"
    output_folder.mkdir(parents=True, exist_ok=True)

    it = input_folder.rglob("*") if recurse else input_folder.iterdir()
    images = [p for p in it if p.is_file() and p.suffix.lower() in IMG_EXTS]
    if not images:
        print(f"[slice] no images in {input_folder}")
        return {"images": 0, "tiles": 0, "failed": 0}

    if max_workers is None:
        cpu = os.cpu_count() or 8
        max_workers = max(2, min(16, cpu * 4))  # ç•™å‡ºä½™é‡ï¼Œå°é¡¶ 16

    def _one(p: Path) -> Tuple[Path, int]:
        try:
            return p, slice_image_cv2(
                p, output_folder,
                tile_h=tile_h, tile_w=tile_w, overlap=overlap,
                out_ext=out_ext, jpeg_quality=jpeg_quality, png_compression=png_compression
            )
        except Exception:
            return p, 0

    tiles = failed = 0

    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        futures = [ex.submit(_one, p) for p in images]
        for fut in as_completed(futures):
            _, saved = fut.result()
            tiles += saved
            if saved == 0:
                failed += 1

    print(f"[slice] images={len(images)} tiles={tiles} failed={failed} -> {output_folder}")
    return {"images": len(images), "tiles": tiles, "failed": failed}


# ============================================================
# 3) å¿«é€Ÿåˆ é™¤åå›¾ï¼ˆå¹¶è¡Œï¼‰
# ============================================================
def delete_corrupt_images_fast(
    root_dir: Path | str,
    recurse: bool = False,
    exts: Tuple[str, ...] = (".jpg", ".jpeg", ".png"),  # ä¸»è¦ JPG/PNG
    min_bytes: int = 32,                                # å°äºè¿™ä¸ªå¤§å°ç›´æ¥åˆ¤åå›¾
    max_workers: Optional[int] = None,                  # é»˜è®¤=CPU*4
    dry_run: bool = False,                              # ä»…ç»Ÿè®¡ä¸åˆ é™¤
) -> Dict[str, int]:
    root = Path(root_dir)
    if recurse:
        files = [p for p in root.rglob("*") if p.is_file() and p.suffix.lower() in exts]
    else:
        files = [p for p in root.iterdir() if p.is_file() and p.suffix.lower() in exts]

    if max_workers is None:
        cpu = os.cpu_count() or 8
        max_workers = max(2, min(16, cpu * 4))  # ç•™å‡ºä½™é‡ï¼Œå°é¡¶ 16

    def is_bad(p: Path) -> Tuple[Path, bool, str]:
        # 1) ç©º/è¶…å°æ–‡ä»¶ï¼šç›´æ¥å
        try:
            if p.stat().st_size < min_bytes:
                return p, True, "too_small"
        except Exception:
            return p, True, "stat_error"

        # 2) OpenCV é«˜é€Ÿè§£ç æ ¡éªŒ
        try:
            data = np.fromfile(str(p), dtype=np.uint8)
            if data.size == 0:
                return p, True, "empty"
            img = cv2.imdecode(data, cv2.IMREAD_UNCHANGED)
            if img is None:
                return p, True, "imdecode_none"
            h, w = img.shape[:2]
            if h == 0 or w == 0:
                return p, True, "zero_dim"
            return p, False, ""
        except Exception as e:
            return p, True, f"decode_error:{type(e).__name__}"

    scanned = 0
    deleted = 0

    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        for fut in as_completed([ex.submit(is_bad, p) for p in files]):
            p, bad, _reason = fut.result()
            scanned += 1
            if bad:
                if not dry_run:
                    try:
                        p.unlink(missing_ok=True)
                    except Exception:
                        pass
                deleted += 1

    kept = scanned - deleted
    print(f"[clean-fast] scanned={scanned} kept={kept} deleted={deleted}")
    return {"scanned": scanned, "kept": kept, "deleted": deleted}


# ============================================================
# 4) ä¸€é”®ï¼šæŒ‰ç›®å½•æ‰§è¡Œã€æ¸…ç†åå›¾ â†’ åˆ‡ç‰‡ã€‘
# ============================================================
def run_clean_and_slice_images_on_dirs(
    dirs: List[Path],
    *,
    # â€”â€” æ¸…ç†åå›¾å‚æ•° â€”â€”
    clean_recurse: bool = False,
    clean_exts: Tuple[str, ...] = (".jpg", ".jpeg", ".png"),
    clean_min_bytes: int = 32,
    clean_max_workers: Optional[int] = None,
    clean_dry_run: bool = False,

    # â€”â€” åˆ‡ç‰‡å‚æ•°â€”â€”
    tile_h: int = 640,
    tile_w: int = 640,
    overlap: float = 0.2,
    out_ext: str = ".jpg",          
    jpeg_quality: int = 95,        
    png_compression: int = 3,
    slice_recurse: bool = False,
    slice_max_workers: Optional[int] = None,
) -> Dict[str, Dict[str, Any]]:
    """
    å¯¹æ¯ä¸ªç›®å½•ä¾æ¬¡æ‰§è¡Œï¼š
      1) delete_corrupt_images_fast æ¸…ç†åå›¾
      2) slice_folder_cv2 è¿›è¡Œåˆ‡ç‰‡ï¼ˆè¾“å‡ºåˆ°åŒçº§ *_slicedï¼ŒPNG æ— æŸï¼‰

    è¿”å›æ±‡æ€»å­—å…¸ï¼š
      {
        "<dir>": {
          "clean": {"scanned":..., "kept":..., "deleted":...},
          "slice": {"images":..., "tiles":..., "failed":...},
          "out_dir": "<dir>_sliced"
        },
        ...
      }
    """
    summary: Dict[str, Dict[str, Any]] = {}

    total = len(dirs)
    for idx, folder in enumerate(dirs, 1):
        print(f"\n[{idx}/{total}] Processing: {folder}")

        # 1) æ¸…ç†åå›¾
        clean_stats = delete_corrupt_images_fast(
            root_dir=folder,
            recurse=clean_recurse,
            exts=clean_exts,
            min_bytes=clean_min_bytes,
            max_workers=clean_max_workers,
            dry_run=clean_dry_run,
        )

        print(f"--- åˆ‡ç‰‡ {folder} ---")
        # 2) åˆ‡ç‰‡ï¼ˆPNG æ— æŸï¼‰
        slice_stats = slice_folder_cv2(
            input_folder=folder,
            output_folder=None,       # None: è‡ªåŠ¨ <folder>_sliced
            tile_h=tile_h,
            tile_w=tile_w,
            overlap=overlap,
            out_ext=out_ext,          # âœ… PNG
            jpeg_quality=jpeg_quality,
            png_compression=png_compression,
            recurse=slice_recurse,
            max_workers=slice_max_workers,
        )

        out_dir = folder.parent / f"{folder.name}_sliced"
        summary[str(folder)] = {
            "clean": clean_stats,
            "slice": slice_stats,
            "out_dir": str(out_dir),
        }

    print("\nâœ… æ¸…ç†ä¸åˆ‡ç‰‡å®Œæˆ")
    return summary


# %%
if "run_clean_and_slice_images_on_dirs" in steps_to_run:
    summary = run_clean_and_slice_images_on_dirs(
        chosen_dirs,
        clean_recurse=False,      # True=å­ç›®å½•ä¹Ÿæ¸…ç†ï¼ŒFalse=ä»…å½“å‰ç›®å½•
        clean_exts=(".jpg", ".jpeg", ".png"),
        clean_min_bytes=32,
        clean_max_workers=None,
        clean_dry_run=False,      # True=ä»…ç»Ÿè®¡ä¸åˆ é™¤ï¼ŒFalse=å®é™…åˆ é™¤
        tile_h=640, tile_w=640, overlap=0.2,
        out_ext=".jpg",
        jpeg_quality=95,          # å¯¹ PNG æ— æ•ˆ
        png_compression=3,        # 3~5 è¾ƒå¹³è¡¡
        slice_recurse=False,      # True=å­ç›®å½•ä¹Ÿåˆ‡ç‰‡ï¼ŒFalse=ä»…å½“å‰ç›®å½•
        slice_max_workers=None,
    )
    print(summary)
else:
    print("è·³è¿‡ Step 1: æ¸…ç†åå›¾å¹¶åˆ‡ç‰‡")

# %% [markdown]
# # Step_2 0202 è¿è¡ŒYOLOåˆ†å‰²æ¨¡å‹ï¼Œç»™è¢«åˆ†å‰²çš„å­å›¾æ•°æ®æ ‡è®°æ©ç 
# ä½¿ç”¨YOLOæ¨¡å‹å¤„ç†åˆ‡ç‰‡å›¾åƒå¹¶ç”ŸæˆLabelMeæ ¼å¼çš„æ ‡æ³¨æ–‡ä»¶
# 
# ### è¾“å…¥
# 
# ![image-3.png](attachment:image-3.png)
# ### è¾“å‡º
# 
# ![image-4.png](attachment:image-4.png)
# 
# ### æ•ˆæœ
# 
# ![image-2.png](attachment:image-2.png)
# 

# %%
# Step 2 ä½¿ç”¨YOLOåˆ†å‰²æ¨¡å‹å¤„ç†åˆ‡ç‰‡å›¾åƒ
from ultralytics import YOLO
from pathlib import Path
import os, gc
import orjson as jsonlib
import torch

def process_sliced_images_with_yolo_seg(chosen_dirs, model_path, COMMON_KWARGS):
    """
    ä½¿ç”¨YOLOæ¨¡å‹å¤„ç†åˆ‡ç‰‡å›¾åƒå¹¶ç”ŸæˆLabelMeæ ¼å¼çš„æ ‡æ³¨æ–‡ä»¶
    """
    model = YOLO(model_path)

    _dumps = lambda obj: jsonlib.dumps(obj, option=jsonlib.OPT_INDENT_2 | jsonlib.OPT_NON_STR_KEYS)
    _loads = jsonlib.loads

    for directory in chosen_dirs:
        print(f"\n=== å¤„ç†ç›®å½•: {directory} ===")
        src_dir = Path(str(directory) + "_sliced")
        
        if not src_dir.exists() or not any(src_dir.iterdir()):
            print(f"è·³è¿‡ç©ºç›®å½•: {src_dir}")
            continue

        # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
        image_files = list(src_dir.glob("*.jpg")) + list(src_dir.glob("*.png"))
        if not image_files:
            print(f"æ— å›¾ç‰‡: {src_dir}")
            continue

        # åˆ†ç‰‡å¤„ç†é¿å…å†…å­˜æº¢å‡º
        CHUNK_SIZE = 100
        for chunk_index in range(0, len(image_files), CHUNK_SIZE):
            image_chunk = image_files[chunk_index:chunk_index + CHUNK_SIZE]
            print(f" -> å¤„ç†åˆ†ç‰‡ {chunk_index}-{chunk_index + len(image_chunk) - 1} / {len(image_files)}")

            # æ‰¹é‡é¢„æµ‹
            results_generator = model.predict(image_chunk, **COMMON_KWARGS)

            # é€å›¾åƒå¤„ç†ç»“æœ
            for result_index, result in enumerate(results_generator, 1):
                try:
                    detections_list = _loads(result.to_json())

                    height, width = map(int, result.orig_shape[:2])
                    image_name = os.path.basename(getattr(result, "path", "")) or f"image_{result_index}.png"

                    shapes = []
                    for detection in detections_list:
                        segmentation = detection.get("segments", {})
                        xs, ys = segmentation.get("x", []), segmentation.get("y", [])
                        if not xs or not ys:
                            continue
                        points = [[float(x), float(y)] for x, y in zip(xs, ys)]
                        shapes.append({
                            "label": detection.get("name", ""),
                            "score": float(detection.get("confidence", 0.0)),
                            "points": points,
                            "shape_type": "polygon",
                        })

                    labelme_annotation = {
                        "shapes": shapes,
                        "imagePath": image_name,
                        "imageHeight": height,
                        "imageWidth": width,
                    }

                    output_path = src_dir / f"{Path(image_name).stem}.json"
                    output_path.write_bytes(_dumps(labelme_annotation))

                finally:
                    # åŠæ—¶é‡Šæ”¾å†…å­˜
                    del result
                    if result_index % 64 == 0:
                        torch.cuda.empty_cache()
                        gc.collect()

            # åˆ†ç‰‡ç»“æŸåæ¸…ç†å†…å­˜
            torch.cuda.empty_cache()
            gc.collect()

        print(f"âœ… å®Œæˆã€‚ä¿å­˜è‡³: {src_dir}")

# %%
if "process_sliced_images_with_yolo_seg" in steps_to_run:
    seg_model_path = f"/workspace/models/best_model/yolo11n-seg-best.pt"
    COMMON_KWARGS = dict(
        imgsz=640,
        conf=0.25,
        iou=0.45,
        device=0,
        batch=3,
        retina_masks=False,
        workers=2,
        verbose=False,
        save=False,
    )
    process_sliced_images_with_yolo_seg(chosen_dirs, seg_model_path, COMMON_KWARGS  = COMMON_KWARGS)
else:
    print("è·³è¿‡ Step 1: ä½¿ç”¨YOLOåˆ†å‰²æ¨¡å‹æ ‡è®°æ©ç ")

# %% [markdown]
# # Step_3 0203  åˆå¹¶å­å›¾åˆ°å¤§å›¾ã€‚ä¹Ÿå°†segmentationä¿¡æ¯æ•´åˆ
# 
# ### è¾“å…¥è¾“å‡º
# ![image.png](attachment:image.png)
# 
# ### å±•å¼€è¾“å‡ºæ–‡ä»¶å¤¹
# 
# ![image-4.png](attachment:image-4.png)
# 
# 
# ### æ•ˆæœ
# 
# ![image-5.png](attachment:image-5.png)

# %%
# Step 3 åˆå¹¶å’Œå»é‡åˆ‡ç‰‡é¢„æµ‹ç»“æœ
import os
import math
import uuid
from collections import defaultdict
from pathlib import Path
from typing import Optional, Iterable, List, Dict, Any, Tuple

import orjson
from tqdm import tqdm
import numpy as np
import cv2
from concurrent.futures import ProcessPoolExecutor, as_completed

# ============== JSON å·¥å…·å‡½æ•° ==============
def json_load(path: str):
    with open(path, "rb") as f:
        return orjson.loads(f.read())

def json_dump(obj, path: str):
    with open(path, "wb") as f:
        f.write(orjson.dumps(obj, option=orjson.OPT_INDENT_2))

# ============== shapelyï¼ˆå¯é€‰ï¼‰ ==============
try:
    from shapely.geometry import Polygon
    from shapely.ops import unary_union
    _HAVE_SHAPELY = True
except Exception:
    _HAVE_SHAPELY = False

# ============== å·¥å…·å‡½æ•° ==============
def _build_image_index(original_image_dir: str) -> Dict[str, str]:
    """å»ºç«‹åŸå›¾æ–‡ä»¶ååˆ°è·¯å¾„çš„ç´¢å¼•"""
    idx: Dict[str, str] = {}
    for p in Path(original_image_dir).glob("*.jpg"):
        idx[p.stem] = str(p)
    return idx

def _draw_annotations_on_image(args: Tuple[str, List[Dict[str, Any]], str, str, bool, int]) -> bool:
    """åœ¨å•å¼ å›¾åƒä¸Šç»˜åˆ¶å¤šè¾¹å½¢æ ‡æ³¨"""
    image_name, annotations, image_path, out_dir, draw_text, jpeg_quality = args
    img = cv2.imread(image_path)
    if img is None:
        return False

    for ann in annotations:
        pts = np.asarray(ann["points"], dtype=np.int32).reshape(-1, 1, 2)
        cv2.polylines(img, [pts], isClosed=True, thickness=1, color=(0, 255, 255))

        if draw_text:
            label = ann.get("label", "")
            score = float(ann.get("score", 0.0))
            x0, y0 = int(ann["points"][0][0]) + 12, int(ann["points"][0][1]) + 12
            txt = f"{label} {score:.3f}"
            cv2.putText(img, txt, (x0, y0), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 3, cv2.LINE_AA)
            cv2.putText(img, txt, (x0, y0), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2, cv2.LINE_AA)

    out_path = os.path.join(out_dir, f"{image_name}_vis.jpg")
    cv2.imwrite(out_path, img, [int(cv2.IMWRITE_JPEG_QUALITY), int(jpeg_quality)])
    return True

def parse_slice_filename(filename: str):
    """è§£æåˆ‡ç‰‡æ–‡ä»¶åè·å–åŸå›¾åå’Œåç§»åæ ‡"""
    parts = Path(filename).stem.split("_")
    name = "_".join(parts[:-4])
    x1, y1, x2, y2 = map(int, parts[-4:])
    return name, x1, y1

# ============== ä¸»è¦å¤„ç†å‡½æ•° ==============
def merge_slice_annotations(sliced_label_dir: str, output_json_path: str) -> Dict[str, List[Dict[str, Any]]]:
    """åˆå¹¶æ‰€æœ‰åˆ‡ç‰‡æ ‡æ³¨åˆ°åŸå›¾åæ ‡ç³»"""
    merged_annotations: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
    json_files = list(Path(sliced_label_dir).rglob("*.json"))

    for json_file in tqdm(json_files, desc="åˆå¹¶åˆ‡ç‰‡æ ‡æ³¨", unit="file"):
        data = json_load(str(json_file))
        image_path = data["imagePath"]
        original_name, offset_x, offset_y = parse_slice_filename(image_path)
        for shape in data.get("shapes", []):
            points = shape["points"]
            label = shape.get("label", "")
            new_points = [[x + offset_x, y + offset_y] for x, y in points]
            merged_annotations[original_name].append({
                "uuid": str(uuid.uuid4()),
                "original_name": original_name,
                "label": label,
                "points": new_points,
                "offset_x": offset_x,
                "offset_y": offset_y,
                "score": float(shape.get("score", 0.0)),
            })

    json_dump(merged_annotations, output_json_path)
    print(f"âœ… åˆå¹¶å®Œæˆï¼Œå…±å¤„ç† {len(merged_annotations)} å¼ åŸå›¾")
    print(f"âœ”ï¸ åˆå¹¶æ ‡æ³¨å·²ä¿å­˜åˆ° {output_json_path}")
    return merged_annotations

def deduplicate_annotations(
    merged_annotations: Dict[str, List[Dict[str, Any]]],
    output_json_path: str,
    method: str = "GREEDYNMM",
    metric: str = "IOS",
    threshold: float = 0.5,
    class_agnostic: bool = False,
    center_threshold: Optional[float] = 20.0,
    keep_strategy: str = "REP"
) -> Dict[str, List[Dict[str, Any]]]:
    """å»é™¤é‡å¤æ ‡æ³¨ï¼ˆå¤šç§å»é‡ç®—æ³•ï¼‰"""
    
    # å†…éƒ¨å·¥å…·å‡½æ•°å®šä¹‰
    def polygon_to_bbox(points: Iterable[Iterable[float]]) -> List[float]:
        xs, ys = zip(*points)
        return [min(xs), min(ys), max(xs), max(ys)]

    def bbox_area(bbox):
        w = bbox[2] - bbox[0]; h = bbox[3] - bbox[1]
        return (w if w > 0 else 0) * (h if h > 0 else 0)

    def bbox_iou(bbox_a, bbox_b):
        xA = max(bbox_a[0], bbox_b[0]); yA = max(bbox_a[1], bbox_b[1])
        xB = min(bbox_a[2], bbox_b[2]); yB = min(bbox_a[3], bbox_b[3])
        inter = max(0, xB - xA) * max(0, yB - yA)
        if inter <= 0: return 0.0
        u = bbox_area(bbox_a) + bbox_area(bbox_b) - inter
        return inter / u if u > 0 else 0.0

    def bbox_ios(bbox_a, bbox_b):
        xA = max(bbox_a[0], bbox_b[0]); yA = max(bbox_a[1], bbox_b[1])
        xB = min(bbox_a[2], bbox_b[2]); yB = min(bbox_a[3], bbox_b[3])
        inter = max(0, xB - xA) * max(0, yB - yA)
        if inter <= 0: return 0.0
        smaller = min(bbox_area(bbox_a), bbox_area(bbox_b))
        return inter / smaller if smaller > 0 else 0.0

    def center_distance(bbox_a, bbox_b):
        cxA = (bbox_a[0] + bbox_a[2]) * 0.5; cyA = (bbox_a[1] + bbox_a[3]) * 0.5
        cxB = (bbox_b[0] + bbox_b[2]) * 0.5; cyB = (bbox_b[1] + bbox_b[3]) * 0.5
        return math.hypot(cxA - cxB, cyA - cyB)

    match_metric = bbox_iou if metric.upper() == "IOU" else bbox_ios

    def is_same_group(ann1: Dict, ann2: Dict) -> bool:
        if (not class_agnostic) and (ann1["label"] != ann2["label"]):
            return False
        bbox1 = polygon_to_bbox(ann1["points"])
        bbox2 = polygon_to_bbox(ann2["points"])
        if (center_threshold is not None) and center_distance(bbox1, bbox2) > center_threshold:
            return False
        return match_metric(bbox1, bbox2) >= threshold

    def get_annotation_score(ann: Dict) -> float:
        try:
            return float(ann.get("score", 0.0))
        except Exception:
            return 0.0

    def select_representative_polygon(group: List[Dict]) -> Dict:
        best_ann = None
        best_key = (-1e9, -1e9)
        for i, ann in enumerate(group):
            bbox = polygon_to_bbox(ann["points"])
            overlap_sum = 0.0
            for j, other_ann in enumerate(group):
                if i == j: continue
                overlap_sum += match_metric(bbox, polygon_to_bbox(other_ann["points"]))
            key = (get_annotation_score(ann), overlap_sum)
            if key > best_key:
                best_key = key; best_ann = ann
        representative = dict(best_ann)
        representative["uuid"] = str(uuid.uuid4())
        return representative

    def merge_polygon_group(group: List[Dict]) -> Dict:
        if not _HAVE_SHAPELY:
            return select_representative_polygon(group)
        polygons = []
        for ann in group:
            points = ann["points"]
            if len(points) >= 3:
                try:
                    polygons.append(Polygon(points))
                except Exception:
                    pass
        if not polygons:
            return select_representative_polygon(group)
        merged_polygon = unary_union(polygons)
        if merged_polygon.geom_type == "MultiPolygon":
            merged_polygon = max(list(merged_polygon.geoms), key=lambda p: p.area)
        coordinates = list(merged_polygon.exterior.coords)[:-1]
        base_ann = dict(group[0])
        base_ann["uuid"] = str(uuid.uuid4())
        base_ann["points"] = [[float(x), float(y)] for (x, y) in coordinates] or group[0]["points"]
        base_ann["score"] = max(get_annotation_score(ann) for ann in group)
        return base_ann

    def process_annotation_group(group: List[Dict]) -> Dict:
        return merge_polygon_group(group) if keep_strategy.upper() == "UNION_POLY" else select_representative_polygon(group)

    # å»é‡ç®—æ³•å®ç°
    def nms_algorithm(annotations: List[Dict]) -> List[Dict]:
        sorted_annotations = sorted(annotations, key=lambda a: get_annotation_score(a), reverse=True)
        kept_annotations: List[Dict] = []
        for ann in sorted_annotations:
            should_suppress = False
            ann_bbox = None
            for kept_ann in kept_annotations:
                if (not class_agnostic) and ann["label"] != kept_ann["label"]:
                    continue
                if ann_bbox is None:
                    ann_bbox = polygon_to_bbox(ann["points"])
                kept_bbox = polygon_to_bbox(kept_ann["points"])
                if (center_threshold is None or center_distance(ann_bbox, kept_bbox) <= center_threshold) and \
                   match_metric(ann_bbox, kept_bbox) >= threshold:
                    should_suppress = True; break
            if not should_suppress:
                kept_annotations.append(ann)
        result = []
        for ann in kept_annotations:
            new_ann = dict(ann); new_ann["uuid"] = str(uuid.uuid4())
            result.append(new_ann)
        return result

    def greedy_grouping_algorithm(annotations: List[Dict]) -> List[List[Dict]]:
        used = [False] * len(annotations)
        indices_sorted = sorted(range(len(annotations)), key=lambda i: get_annotation_score(annotations[i]), reverse=True)
        groups: List[List[Dict]] = []
        for idx in indices_sorted:
            if used[idx]: continue
            seed_ann = annotations[idx]
            group = [seed_ann]; used[idx] = True
            changed = True
            while changed:
                changed = False
                for j, other_ann in enumerate(annotations):
                    if used[j]: continue
                    if any(is_same_group(other_ann, group_ann) for group_ann in group):
                        group.append(other_ann); used[j] = True; changed = True
            groups.append(group)
        return groups

    # ä¸»å¤„ç†é€»è¾‘
    cleaned_annotations: Dict[str, List[Dict]] = {}
    total_before = sum(len(v) for v in merged_annotations.values())
    total_after = 0

    for image_name, annotations in tqdm(merged_annotations.items(), desc="å»é‡å¤„ç†", unit="image"):
        if not class_agnostic:
            label_buckets = defaultdict(list)
            for ann in annotations:
                label_buckets[ann["label"]].append(ann)
            result_annotations: List[Dict] = []
            for _, bucket in label_buckets.items():
                algorithm = method.upper()
                if algorithm == "NMS":
                    result_annotations.extend(nms_algorithm(bucket))
                elif algorithm == "NMM":
                    result_annotations.extend(process_annotation_group(g) for g in pairwise_grouping(bucket))
                elif algorithm == "LSNMS":
                    result_annotations.extend(lsnms_algorithm(bucket))
                else:  # GREEDYNMM
                    result_annotations.extend(process_annotation_group(g) for g in greedy_grouping_algorithm(bucket))
        else:
            algorithm = method.upper()
            if algorithm == "NMS":
                result_annotations = nms_algorithm(annotations)
            elif algorithm == "NMM":
                result_annotations = [process_annotation_group(g) for g in pairwise_grouping(annotations)]
            elif algorithm == "LSNMS":
                result_annotations = lsnms_algorithm(annotations)
            else:
                result_annotations = [process_annotation_group(g) for g in greedy_grouping_algorithm(annotations)]

        cleaned_annotations[image_name] = result_annotations
        total_after += len(result_annotations)

    json_dump(cleaned_annotations, output_json_path)

    print(f"ğŸ” å»é‡å®Œæˆï¼ˆ{method}, metric={metric}, threshold={threshold}, class_agnostic={class_agnostic}, strategy={keep_strategy}ï¼‰")
    print(f"    ç›®æ ‡æ•°ï¼š{total_before} â†’ {total_after}")
    if keep_strategy.upper() == "UNION_POLY" and not _HAVE_SHAPELY:
        print("âš ï¸ æœªå®‰è£…shapelyï¼Œå·²é€€å›REPæ¨¡å¼")
    print(f"âœ”ï¸ å·²ä¿å­˜åˆ° {output_json_path}")
    return cleaned_annotations

def visualize_annotations(
    merged_annotations: Dict[str, List[Dict[str, Any]]],
    original_image_dir: str,
    output_visual_dir: str,
    draw_text: bool = True,
    jpeg_quality: int = 95,
    parallel: bool = True,
    max_workers: Optional[int] = None
):
    """å¯è§†åŒ–æ ‡æ³¨ç»“æœ"""
    os.makedirs(output_visual_dir, exist_ok=True)
    image_index = _build_image_index(original_image_dir)

    tasks = []
    for image_name, annotations in merged_annotations.items():
        image_path = image_index.get(image_name)
        if image_path is None or not os.path.exists(image_path):
            found = None
            for p in Path(original_image_dir).glob(f"{image_name}*.jpg"):
                found = str(p); break
            image_path = found
        if image_path is None or not os.path.exists(image_path):
            continue
        tasks.append((image_name, annotations, image_path, output_visual_dir, draw_text, jpeg_quality))

    if not tasks:
        print("âš ï¸ æ²¡æœ‰å¯è§†åŒ–ä»»åŠ¡")
        return

    if not parallel:
        for task in tqdm(tasks, desc="å¯è§†åŒ–å¤„ç†ï¼ˆä¸²è¡Œï¼‰", unit="image"):
            _draw_annotations_on_image(task)
    else:
        if max_workers is None:
            max_workers = max(2, (os.cpu_count() or 8) // 2)
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(_draw_annotations_on_image, task) for task in tasks]
            for _ in tqdm(as_completed(futures), total=len(futures), desc="å¯è§†åŒ–å¤„ç†ï¼ˆå¹¶è¡Œï¼‰", unit="image"):
                pass

    print(f"ğŸ–¼ å¯è§†åŒ–å›¾ç‰‡å·²ä¿å­˜åˆ° {output_visual_dir}/")

def _crop_single_object(args: Tuple[str, List[Dict[str, Any]], str, str, int, int]) -> int:
    """è£å‰ªå•ä¸ªç›®æ ‡å¯¹è±¡"""
    image_name, annotations, image_path, out_dir, margin, jpeg_quality = args
    img = cv2.imread(image_path)
    if img is None:
        return 0
    height, width = img.shape[:2]
    saved_count = 0

    for idx, ann in enumerate(annotations):
        points = np.asarray(ann["points"], dtype=np.float32)
        xs = points[:, 0]; ys = points[:, 1]
        min_x, max_x = float(xs.min()), float(xs.max())
        min_y, max_y = float(ys.min()), float(ys.max())

        bbox_width = max_x - min_x
        bbox_height = max_y - min_y
        side_length = max(bbox_width, bbox_height)

        center_x = (min_x + max_x) * 0.5
        center_y = (min_y + max_y) * 0.5

        left = int(round(center_x - side_length * 0.5)) - margin
        top = int(round(center_y - side_length * 0.5)) - margin
        right = int(round(center_x + side_length * 0.5)) + margin
        bottom = int(round(center_y + side_length * 0.5)) + margin

        left = max(0, left)
        top = max(0, top)
        right = min(width, right)
        bottom = min(height, bottom)

        if right - left <= 1 or bottom - top <= 1:
            continue

        crop = img[top:bottom, left:right]
        save_name = f"{image_name}_obj{idx}_{ann.get('label','')}_uuid_{ann['uuid']}.jpg"
        out_path = os.path.join(out_dir, save_name)
        cv2.imwrite(out_path, crop, [int(cv2.IMWRITE_JPEG_QUALITY), int(jpeg_quality)])
        saved_count += 1

    return saved_count

def export_cropped_objects(
    merged_annotations: Dict[str, List[Dict[str, Any]]],
    original_image_dir: str,
    cropped_object_dir: str,
    margin: int = 0,
    jpeg_quality: int = 95,
    parallel: bool = True,
    max_workers: Optional[int] = None
):
    """å¯¼å‡ºè£å‰ªçš„ç›®æ ‡å¯¹è±¡"""
    os.makedirs(cropped_object_dir, exist_ok=True)
    image_index = _build_image_index(original_image_dir)

    tasks = []
    for image_name, annotations in merged_annotations.items():
        image_path = image_index.get(image_name)
        if image_path is None or not os.path.exists(image_path):
            found = None
            for p in Path(original_image_dir).glob(f"{image_name}*.jpg"):
                found = str(p); break
            image_path = found
        if image_path is None or not os.path.exists(image_path):
            continue
        tasks.append((image_name, annotations, image_path, cropped_object_dir, margin, jpeg_quality))

    total_saved = 0
    if not tasks:
        print("âš ï¸ æ²¡æœ‰å¯å¯¼å‡ºçš„è£å‰ªä»»åŠ¡")
        return

    if not parallel:
        for task in tqdm(tasks, desc="å¯¼å‡ºè£å‰ªï¼ˆä¸²è¡Œï¼‰", unit="image"):
            total_saved += _crop_single_object(task)
    else:
        if max_workers is None:
            max_workers = max(2, (os.cpu_count() or 8) // 2)
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(_crop_single_object, task) for task in tasks]
            for future in tqdm(as_completed(futures), total=len(futures), desc="å¯¼å‡ºè£å‰ªï¼ˆå¹¶è¡Œï¼‰", unit="image"):
                total_saved += future.result()

    print(f"ğŸ“¦ ä¸ªä½“è£å‰ªå›¾åƒå·²ä¿å­˜åˆ° {cropped_object_dir}/ ï¼ˆå…±å¯¼å‡º {total_saved} å¼ ï¼‰")



# %%
def combine_sliced_predictions(chosen_dirs):
    """å¤„ç†åˆ‡ç‰‡é¢„æµ‹ç»“æœçš„ä¸»å‡½æ•°"""
    for directory in chosen_dirs:
        print(f"\n=== å¤„ç†ç›®å½•: {directory} ===")
        original_image_dir = str(directory)
        sliced_label_dir = str(directory) + "_sliced"
        output_json_path = str(directory) + "_sliced_merge/01_merged_annotations.json"
        output_visual_dir = str(directory) + "_sliced_merge/01_visualizations"
        cropped_object_dir = str(directory) + "_sliced_merge/01_cropped_objects"

        print(f"åŸå›¾ç›®å½•: {original_image_dir}")
        print(f"åˆ‡ç‰‡æ ‡æ³¨ç›®å½•: {sliced_label_dir}")
        print(f"è¾“å‡ºåˆå¹¶æ ‡æ³¨: {output_json_path}")
        print(f"è¾“å‡ºå¯è§†åŒ–ç›®å½•: {output_visual_dir}")
        print(f"è¾“å‡ºè£å‰ªç›®å½•: {cropped_object_dir}")

        os.makedirs(output_visual_dir, exist_ok=True)
        os.makedirs(cropped_object_dir, exist_ok=True)

        # 1) åˆå¹¶åˆ‡ç‰‡æ ‡æ³¨
        merged_annotations = merge_slice_annotations(sliced_label_dir, output_json_path)

        # 2) å»é‡å¤„ç†
        merged_annotations = deduplicate_annotations(
            merged_annotations,
            output_json_path,
            method="NMS",               # 'NMM'/'GREEDYNMM'/'LSNMS'/'NMS' -- NMSæ¯ä¸ªç›®æ ‡æœ€å¤š 1 ä¸ªé‡å¤ï¼ˆå®Œæ•´ + å°ç¢ç‰‡ï¼‰ï¼Œç›´æ¥ä¿ç•™é«˜åˆ†çš„é‚£ä¸€ä¸ªå°±è¡Œã€‚
            metric="IOS",               # 'IOU'/'IOS'  å½“å¯èƒ½å‡ºç°"å°æ¡†è¢«å¤§æ¡†åŒ…å«"æ—¶ï¼Œæ¨èä½¿ç”¨ IOSï¼Œå½“ä¸¤ä¸ªæ¡†å¤§å°ç›¸è¿‘ï¼Œä¸”ä½ æƒ³çŸ¥é“"æ•´ä½“é‡å ç¨‹åº¦"æ—¶ï¼Œæ¨èä½¿ç”¨ IOU
            threshold=0.5,              # ä¸¤ä¸ªè¾¹ç•Œæ¡†è¢«è®¤ä¸ºæ˜¯é‡å¤çš„é˜ˆå€¼
            class_agnostic=False,
            center_threshold=20,
            keep_strategy="REP"         # 'REP'/'UNION_POLY' -- ä¸éœ€è¦ UNION_POLYï¼ˆå¹¶é›†ï¼‰å»â€œç²˜åˆç¢ç‰‡â€ï¼Œå› ä¸ºæˆ‘ä»¬åªä¿ç•™å®Œæ•´çš„é‚£ä»½å°±å¥½ã€‚
        )

        # 3) å¯è§†åŒ–
        visualize_annotations(
            merged_annotations,
            original_image_dir,
            output_visual_dir,
            draw_text=True,         # æ˜¯å¦ç»˜åˆ¶æ ‡ç­¾å’Œç½®ä¿¡åº¦ï¼Œå…³æ‰å¯ä»¥æå‡é€Ÿåº¦
            jpeg_quality=95,
            parallel=True,
            max_workers=None
        )

        # 4) å¯¼å‡ºè£å‰ª
        export_cropped_objects(
            merged_annotations,
            original_image_dir,
            cropped_object_dir,
            margin=15,
            jpeg_quality=95,
            parallel=True,
            max_workers=None
        )

# %%
if "combine_sliced_predictions" in steps_to_run:
    combine_sliced_predictions(chosen_dirs)
else:
    print("è·³è¿‡ Step 2: åˆå¹¶å’Œå»é‡åˆ‡ç‰‡é¢„æµ‹ç»“æœ")


# %% [markdown]
# # ç²¾ç‚¼æ£€æµ‹v3--ç”¨seg+detæ¨¡å‹å¯¹æ‰€æœ‰ç–‘ä¼¼SWDçš„objectè¿›ä¸€æ­¥ç­›é€‰

# %%
version = "v1"
run_type = "pose_and_det"  # "pose_and_det" or "cls"

# éœ€è¦è¿è¡Œçš„stepåˆ—è¡¨
steps_to_run = [
    # "run_clean_and_slice_images_on_dirs",           # Step 1 æ¸…ç†åå›¾å¹¶åˆ‡ç‰‡å¤§å›¾åˆ°640*640å°å›¾
    # "process_sliced_images_with_yolo_seg",          # Step 2 ä½¿ç”¨YOLO-segæ¨¡å‹å¤„ç†640*640åˆ‡ç‰‡å›¾åƒ
    # "combine_sliced_predictions",                   # Step 3 åˆå¹¶segé¢„æµ‹ç»“æœï¼Œå›åˆ°åŸå›¾ã€‚ åŒæ—¶åˆ‡å‡ºobjectså°å›¾
    "run_pose_on_chosen_dirs",                        # Step 4 è¿è¡Œ Pose Estimationï¼ˆåœ¨ cropped_objects ä¸Šï¼‰
    "run_batch_dot_det",                              # Step 5 è¿è¡Œ det æ¨¡å‹ æ£€æµ‹â€œå°é»‘ç‚¹â€ï¼ˆåœ¨ cropped_objects ä¸Šï¼‰
    "process_swd_matching",                          # Step 6 åˆ¤å®š SWD â‡’ åŒ¹é…è§„åˆ™ï¼šä¸¤ç¿¼å…³é”®ç‚¹åˆ†åˆ«è½å…¥ä¸¤ä¸ªä¸åŒçš„å°é»‘ç‚¹æ¡†
]

# %%
def free_gpu():
    import gc, torch
    gc.collect()                    # è§¦å‘ Python åƒåœ¾å›æ”¶
    torch.cuda.empty_cache()        # é‡Šæ”¾æœªä½¿ç”¨çš„ GPU ç¼“å­˜åˆ°é©±åŠ¨
    torch.cuda.ipc_collect()        # æ¸…ç†è·¨è¿›ç¨‹ç¼“å­˜ï¼ˆå¶å°”æœ‰ç”¨ï¼‰


# %% [markdown]
# # Step_0 æŸ¥çœ‹æ ¹ç›®å½•ä¸‹éœ€è¦è¿è¡Œçš„æ–‡ä»¶å¤¹ 
# é€‰æ‹©å«æœ‰ raw_data å›¾ç‰‡çš„ *_data ç›®å½•
# 
# ![image.png](attachment:image.png)
# 

# %%
# é€‰æ‹©æ•°æ®ç›®å½•çš„æ ¸å¿ƒä»£ç 
from pathlib import Path

def select_data_dirs(root_dir: Path, end_with: str = "_data"):
    # === 1) éå†æ‰€æœ‰å­ç›®å½• ===
    sub_dirs = list(root_dir.glob("*/*" + end_with))

    if not sub_dirs:
        print(f"æ²¡æœ‰æ‰¾åˆ° *{end_with} ç›®å½•")
        return []

    print(f"æ‰¾åˆ°ä»¥ä¸‹ {end_with} æ•°æ®é›†ï¼š")
    for i, d in enumerate(sub_dirs):
        print(f"[{i}] {d}")

    # === 2) è®©ç”¨æˆ·é€‰æ‹©è¦è·‘çš„ç›®å½• ===
    idx_str = input("è¯·è¾“å…¥è¦å¤„ç†çš„ç¼–å· (å¤šä¸ªç”¨é€—å·åˆ†éš”, å›è½¦é»˜è®¤å…¨é€‰): ").strip()
    if idx_str:
        indices = [int(x) for x in idx_str.split(",")]
        chosen_dirs = [sub_dirs[i] for i in indices]
    else:
        chosen_dirs = sub_dirs

    print(f"å°†å¤„ç†ä»¥ä¸‹ {end_with} ç›®å½•ï¼š")
    for i, d in enumerate(chosen_dirs):
        print(f"- {i+1}. {d}")

    # === 3) ç­›é€‰æ‰æ²¡æœ‰ raw_data å›¾ç‰‡çš„ç›®å½• ===
    chosen_dirs = [
        d for d in chosen_dirs
        if (d.parent / "raw_data").exists() and any((d.parent / "raw_data").glob("*.jpg"))
    ]

    if not chosen_dirs:
        print(f"æ²¡æœ‰æ‰¾åˆ°åŒ…å«å›¾ç‰‡çš„ *{end_with} ç›®å½•")
        return []

    return chosen_dirs

# %%
# root_dir = Path("/workspace/models/SAHI/run_v8")
# chosen_dirs = select_data_dirs(root_dir, end_with="_data")
# print("æœ€ç»ˆç¡®è®¤çš„ç›®å½•ï¼š", chosen_dirs)
# if not chosen_dirs:
#     raise ValueError("æ²¡æœ‰é€‰æ‹©ä»»ä½•ç›®å½•ï¼Œç¨‹åºç»ˆæ­¢ã€‚")

# %% [markdown]
# # Step_4 è¿è¡Œ Pose Estimationï¼ˆåœ¨ cropped_objects ä¸Šï¼‰
# è¯»å–æ¯å¼ å°å›¾ï¼ˆå¸¦ uuid_... å‘½åï¼‰ï¼Œè¾“å‡ºå¤´(h)ã€å·¦ç¿¼(lp)ã€å³ç¿¼(rp) ä¸‰å…³é”®ç‚¹
# 
# ### è¾“å…¥è¾“å‡º
# ![image.png](attachment:image.png)
# 
# ### æ•ˆæœ
# ![image-2.png](attachment:image-2.png)

# %%
# Step 4: è¿è¡Œ Pose Estimation
import re, os, json
from pathlib import Path
from typing import List, Dict, Any

from shapely import box
from ultralytics import YOLO

# æ–‡ä»¶åè§£æï¼š..._uuid_<uuid>.jpg
UUID_RE = re.compile(r"uuid_([a-f0-9\-]+)\.(jpg|jpeg|png)$", re.IGNORECASE)
ORIG_RE = re.compile(r"^(\d+_\d+_\d+)_obj", re.IGNORECASE)

def run_pose_on_dir(
    model_path: str,
    input_dir: Path,
    out_json: Path,
    kpt_names: List[str],
    predict_args: Dict[str, Any]
):
    """
    å¯¹å•ä¸ªç›®å½•è¿è¡Œå§¿æ€ä¼°è®¡
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        input_dir: è¾“å…¥å›¾ç‰‡ç›®å½•
        out_json: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
        kpt_names: å…³é”®ç‚¹åç§°åˆ—è¡¨
        predict_args: model.predictçš„å‚æ•°ï¼ˆå¿…é¡»ä¼ å…¥ï¼›å¦‚æœ imgsz=None åˆ™è¯»å–æ¨¡å‹é»˜è®¤å€¼ï¼‰
    """
    if not any(input_dir.glob("*.jpg")) and not any(input_dir.glob("*.png")):
        print(f"âš ï¸ è¾“å…¥ç›®å½•æ— å›¾ç‰‡ï¼š{input_dir}")
        return
    
    print(f"åŠ è½½å§¿æ€æ¨¡å‹ï¼š{model_path}")
    model = YOLO(model_path)

    # å¤„ç† imgsz=None -> ä½¿ç”¨æ¨¡å‹é»˜è®¤
    args = dict(predict_args)  # å¤åˆ¶ä¸€ä»½
    if "imgsz" in args and args["imgsz"] is None:
        args["imgsz"] = model.overrides.get("imgsz")
        print(f"â„¹ï¸ ä½¿ç”¨æ¨¡å‹é»˜è®¤ imgsz = {args['imgsz']}")

    results = model.predict(source=str(input_dir), **args)

    out: List[Dict[str, Any]] = []
    for res in results:
        fpath = getattr(res, "path", "")
        fname = os.path.basename(fpath)
        m_uuid = UUID_RE.search(fname)
        uuid_str = m_uuid.group(1) if m_uuid else None
        m_orig = ORIG_RE.match(fname)
        original_name = m_orig.group(1) if m_orig else None

        dets = []
        kpts = getattr(res, "keypoints", None)
        if kpts is not None and kpts.data is not None:
            arr = kpts.data.cpu().numpy()
            conf_arr = getattr(kpts, "conf", None)
            conf_arr = conf_arr.cpu().numpy() if conf_arr is not None else None

            for i in range(arr.shape[0]):
                pts = arr[i]
                item = []
                for ki in range(min(len(kpt_names), pts.shape[0])):
                    x, y = float(pts[ki][0]), float(pts[ki][1])
                    c = float(conf_arr[i][ki]) if (
                        conf_arr is not None and conf_arr.shape == (arr.shape[0], pts.shape[0])
                    ) else None
                    item.append({"name": kpt_names[ki], "x": x, "y": y, "conf": c})
                dets.append({"kpts": item})
        boxes = getattr(res, "boxes", None)
        if boxes is not None and boxes.data is not None:
            arr = boxes.data.cpu().numpy()
            for i in range(arr.shape[0]):
                x1, y1, x2, y2, conf, cls = arr[i]
                item = {
                    "box": {"x1": float(x1), "y1": float(y1), "x2": float(x2), "y2": float(y2)},
                    "conf": float(conf),
                    "cls": int(cls)
                }
                if i < len(dets):
                    dets[i]["box"] = item["box"]
                    dets[i]["box_conf"] = item["conf"]
                    dets[i]["box_cls"] = item["cls"]
                else:
                    dets.append(item)

        out.append({
            "path": fpath,
            "file": fname,
            "uuid": uuid_str,
            "original_name": original_name,
            "instances": dets
        })

    out_json.parent.mkdir(parents=True, exist_ok=True)
    with out_json.open("w", encoding="utf-8") as f:
        json.dump(out, f, indent=2, ensure_ascii=False)
    print(f"âœ… Pose ç»“æœä¿å­˜ï¼š{out_json}")

def run_pose_on_chosen_dirs(
    chosen_dirs: List[Path],
    model_path: str,
    kpt_names: List[str],
    predict_args: Dict[str, Any]
):
    """
    å¯¹é€‰å®šçš„ç›®å½•åˆ—è¡¨æ‰¹é‡è¿è¡Œå§¿æ€ä¼°è®¡
    
    Args:
        chosen_dirs: åŒ…å«å›¾ç‰‡çš„ç›®å½•åˆ—è¡¨
        model_path: æ¨¡å‹è·¯å¾„
        kpt_names: å…³é”®ç‚¹åç§°åˆ—è¡¨
        predict_args: model.predictçš„å‚æ•°ï¼ˆå¿…é¡»ä¼ å…¥ï¼›imgsz=None è¡¨ç¤ºè‡ªé€‚åº”æ¨¡å‹é»˜è®¤ï¼‰
    """
    for d in chosen_dirs:
        crops_dir = d.parent / (d.name + "_sliced_merge") / "01_cropped_objects"
        pose_json = d.parent / (d.name + "_sliced_merge") / f"pose_and_det_{version}" / "02_pose_predicted_results.json"
        print(f"\n=== Pose on: {crops_dir} ===")
        run_pose_on_dir(model_path, crops_dir, pose_json, kpt_names, predict_args)


# %%
if "run_pose_on_chosen_dirs" in steps_to_run:
    custom_kpt_names = ["h", "lp", "rp"]
    custom_predict_args = {
        "imgsz": None,  # None è¡¨ç¤ºä½¿ç”¨æ¨¡å‹é»˜è®¤å€¼
        "conf": 0.88,
        "iou": 0.6,
        "device": 0,
        "verbose": True,
        # "stream": True,
        "batch": 128,
    }

    run_pose_on_chosen_dirs(
        chosen_dirs, 
        model_path="/workspace/models/best_model/yolo11n-pose-best_v2.pt",
        kpt_names=custom_kpt_names,
        predict_args=custom_predict_args
    )
    free_gpu()
else:
    print("Step_4 è¿è¡Œ Pose Estimationï¼ˆåœ¨ cropped_objects ä¸Šï¼‰ è¢«è·³è¿‡")

# %% [markdown]
# # Step_5 è¿è¡Œ det æ¨¡å‹ æ£€æµ‹â€œå°é»‘ç‚¹â€ï¼ˆåœ¨ cropped_objects ä¸Šï¼‰
# è¯»å–æ¯å¼ å°å›¾ï¼ˆå¸¦ uuid_... å‘½åï¼‰ï¼Œè¾“å‡ºå¤´(h)ã€å·¦ç¿¼(lp)ã€å³ç¿¼(rp) ä¸‰å…³é”®ç‚¹
# 
# ### è¾“å…¥è¾“å‡º
# ![image-2.png](attachment:image-2.png)
# 
# ### æ•ˆæœ
# ![image-3.png](attachment:image-3.png)

# %%
# Step 5: è¿è¡Œ det æ¨¡å‹ æ£€æµ‹â€œå°é»‘ç‚¹â€
import os, re, json
from pathlib import Path
from typing import Dict, Any, List
from ultralytics import YOLO
import numpy as np

UUID_RE = re.compile(r"uuid_([a-f0-9\-]+)\.(jpg|jpeg|png)$", re.IGNORECASE)

def _to_float_list(x):
    # æŠŠ numpy/tensor æ ‡é‡å®‰å…¨è½¬æˆ python float
    return [float(v) for v in x]

def run_dot_det_on_dir(
    model_path: str,
    input_dir: Path,
    out_json: Path,
    custom_predict_args: Dict[str, Any]
):
    if not any(input_dir.glob("*.jpg")) and not any(input_dir.glob("*.png")):
        print(f"âš ï¸ è¾“å…¥ç›®å½•æ— å›¾ç‰‡ï¼š{input_dir}")
        return

    print(f"åŠ è½½å°é»‘ç‚¹æ£€æµ‹æ¨¡å‹ï¼š{model_path}")
    model = YOLO(model_path)

    if custom_predict_args['imgsz'] is None:
        custom_predict_args['imgsz'] = model.overrides.get("imgsz")
        print(f"â„¹ï¸ ä½¿ç”¨æ¨¡å‹é»˜è®¤ imgsz = {custom_predict_args['imgsz']}")

    # å¿…é¡»ä¼  custom_predict_args
    results = model.predict(**custom_predict_args, source=str(input_dir))

    out: List[Dict[str, Any]] = []
    for res in results:
        fpath = getattr(res, "path", "")
        fname = os.path.basename(fpath)
        m_uuid = UUID_RE.search(fname)
        uuid_str = m_uuid.group(1) if m_uuid else None

        det_list = []
        boxes = getattr(res, "boxes", None)
        if boxes is not None:
            xyxy = boxes.xyxy.cpu().numpy() if hasattr(boxes, "xyxy") else None
            confs = boxes.conf.cpu().numpy() if hasattr(boxes, "conf") and boxes.conf is not None else None
            clses = boxes.cls.cpu().numpy() if hasattr(boxes, "cls") and boxes.cls is not None else None

            if xyxy is not None:
                n = xyxy.shape[0]
                for i in range(n):
                    x1, y1, x2, y2 = _to_float_list(xyxy[i].tolist())
                    conf_score = float(confs[i]) if confs is not None and i < len(confs) else None
                    cls_id = int(clses[i]) if clses is not None and i < len(clses) else 0
                    det_list.append({
                        "bbox": [x1, y1, x2, y2],
                        "conf": conf_score,
                        "cls": cls_id
                    })

        out.append({
            "path": fpath,
            "file": fname,
            "uuid": uuid_str,
            "boxes": det_list
        })

    out_json.parent.mkdir(parents=True, exist_ok=True)
    with out_json.open("w", encoding="utf-8") as f:
        json.dump(out, f, indent=2, ensure_ascii=False)
    print(f"âœ… å°é»‘ç‚¹æ£€æµ‹ç»“æœä¿å­˜ï¼š{out_json}")

def run_batch_dot_det(
    chosen_dirs: List[Path],
    dot_model: str,
    custom_predict_args: Dict[str, Any]
):
    """æ‰¹å¤„ç†å…¥å£ï¼šéå† chosen_dirsï¼Œè¿è¡Œå°é»‘ç‚¹æ£€æµ‹ï¼›predict å‚æ•°å¿…é¡»é€šè¿‡ custom_predict_args æä¾›"""
    for d in chosen_dirs:
        crops_dir = d.parent / (d.name + "_sliced_merge") / "01_cropped_objects"
        dot_json  = d.parent / (d.name + "_sliced_merge") / f"pose_and_det_{version}" / "03_dot_predicted_results.json"
        print(f"\n=== Dot-Det on: {crops_dir} ===")
        run_dot_det_on_dir(dot_model, crops_dir, dot_json, custom_predict_args=custom_predict_args)


# %%
if "run_batch_dot_det" in steps_to_run:
    dot_det_model_path = "/workspace/models/best_model/yolo11n-det-best_v1.pt"
    custom_predict_args = {
        "imgsz": None,  # None åˆ™ä½¿ç”¨æ¨¡å‹é»˜è®¤å€¼
        "conf": 0.3,
        "iou": 0.5,
        "device": 0,
        "verbose": True,
        # "stream": True,
        "batch": 128
    }
    run_batch_dot_det(chosen_dirs, dot_model=dot_det_model_path, custom_predict_args=custom_predict_args)

    free_gpu()
else:
    print("Step_5 è¿è¡Œ det æ¨¡å‹ æ£€æµ‹â€œå°é»‘ç‚¹â€ï¼ˆåœ¨ cropped_objects ä¸Šï¼‰ è¢«è·³è¿‡")


# %% [markdown]
# # Step_6 åˆ¤å®š SWD â‡’ åŒ¹é…è§„åˆ™ï¼šä¸¤ç¿¼å…³é”®ç‚¹åˆ†åˆ«è½å…¥ä¸¤ä¸ªä¸åŒçš„å°é»‘ç‚¹æ¡†
# 
# ### è¾“å…¥
# ![image-3.png](attachment:image-3.png)
# ### è¾“å‡º
# ![image-4.png](attachment:image-4.png)
# 
# ### æ•ˆæœ
# ![image-2.png](attachment:image-2.png)
# ![image-8.png](attachment:image-8.png)
# ![image-10.png](attachment:image-10.png)

# %%
# Step 6: åˆ¤å®š SWD â‡’ åŒ¹é…è§„åˆ™ï¼šä¸¤ç¿¼å…³é”®ç‚¹åˆ†åˆ«è½å…¥ä¸¤ä¸ªä¸åŒçš„å°é»‘ç‚¹æ¡†

import os, re, shutil
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional
import cv2
import numpy as np

# ---------- å¯é€‰ï¼šorjson åŠ é€Ÿ ----------
try:
    import orjson as _fastjson
    def _loads(b: bytes): return _fastjson.loads(b)
    def _dumps(obj): return _fastjson.dumps(obj, option=_fastjson.OPT_INDENT_2)
except Exception:
    import json as _fastjson
    def _loads(b: bytes): return _fastjson.loads(b.decode("utf-8"))
    def _dumps(obj): return _fastjson.dumps(obj, indent=2, ensure_ascii=False).encode("utf-8")

# ====== å‚æ•°åŒºï¼ˆæŒ‰éœ€è°ƒæ•´ï¼‰======
REQUIRE_DIFFERENT_BOXES = True   # âœ… ä¸¤ç¿¼å¿…é¡»è½å…¥ä¸åŒæ¡†
USE_POINT_CONF = True            # è‹¥å…³é”®ç‚¹å¸¦ confï¼Œåˆ™åº”ç”¨é˜ˆå€¼è¿‡æ»¤
KPT_CONF_THR = 0.88
DO_VIS = False                    # è¾“å‡ºå¯è§†åŒ–å›¾
LIMIT_VIS = None                 # ä»…å¯è§†åŒ–å‰ N å¼ ï¼ˆNone ä¸ºå…¨éƒ¨ï¼‰
COPY_UNMATCHED = False            # ä¹Ÿæ‹·è´æœªåŒ¹é…æ ·æœ¬ï¼Œä¾¿äºäººå·¥å¤æ ¸

UUID_RE = re.compile(r"uuid_([a-f0-9\-]+)\.(jpg|jpeg|png)$", re.IGNORECASE)

# ---------- I/O ----------
def load_json(path: Path):
    with path.open("rb") as f:
        data = f.read()
    return _loads(data)

def save_json(obj: Any, path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("wb") as f:
        f.write(_dumps(obj))

# ---------- å‘é‡åŒ–ç‚¹è½æ¡† ----------
# è¾“å…¥ï¼špt=(x,y), boxes: np.ndarray (N,4) [x1,y1,x2,y2] float32
# è¾“å‡ºï¼šå‘½ä¸­çš„ç´¢å¼•æ•°ç»„ï¼ˆnp.int32ï¼‰
def hits_for_point(pt: Tuple[float, float], boxes: np.ndarray) -> np.ndarray:
    if boxes.size == 0:
        return np.empty((0,), dtype=np.int32)
    x, y = pt
    # (N,)
    cond = (x >= boxes[:, 0]) & (x <= boxes[:, 2]) & (y >= boxes[:, 1]) & (y <= boxes[:, 3])
    return np.flatnonzero(cond).astype(np.int32, copy=False)

def find_kpt(items: List[Dict[str, Any]], name: str) -> Optional[Dict[str, Any]]:
    # ä¿æŒ O(K) æ‰«æï¼Œä½†é¿å¼€å¤šæ¬¡ dict.get
    for it in items:
        if it.get("name") == name:
            return it
    return None

def kpt_ok(k: Optional[Dict[str, Any]], use_point_conf: bool, thr: float) -> bool:
    if not k:
        return False
    if use_point_conf:
        conf = k.get("conf")
        if conf is not None:
            # é¿å… try/exceptï¼Œç›´æ¥æŒ‰å¸¸è§æ•°å€¼/å­—ç¬¦ä¸²è½¬ float
            try:
                return float(conf) >= thr
            except Exception:
                return False
    return True

def match_one(uuid_: str,
              pose_items: List[Dict[str, Any]],
              dot_boxes_np: np.ndarray,
              require_different_boxes: bool = True,
              use_point_conf: bool = True,
              kpt_conf_thr: float = 0.15) -> Dict[str, Any]:
    """å¯¹ä¸€å¼ å°å›¾è¿›è¡ŒåŒ¹é…åˆ¤æ–­ï¼šlp ä¸ rp å¿…é¡»åˆ†åˆ«å‘½ä¸­ä¸¤ä¸ªä¸åŒçš„å°é»‘ç‚¹æ¡†ï¼ˆå‘é‡åŒ–ç‰ˆï¼‰"""
    lp = find_kpt(pose_items, "lp")
    rp = find_kpt(pose_items, "rp")

    lp_ok = kpt_ok(lp, use_point_conf, kpt_conf_thr)
    rp_ok = kpt_ok(rp, use_point_conf, kpt_conf_thr)

    matched = False
    lp_in_idx = None
    rp_in_idx = None

    if lp_ok and rp_ok and dot_boxes_np.size:
        lp_hits = hits_for_point((lp["x"], lp["y"]), dot_boxes_np)
        rp_hits = hits_for_point((rp["x"], rp["y"]), dot_boxes_np)

        if lp_hits.size and rp_hits.size:
            if require_different_boxes:
                # æ‰¾ä¸€å¯¹ä¸åŒç´¢å¼•ï¼šåˆ©ç”¨å¹¿æ’­å¿«é€Ÿæ‰¾åˆ°ç¬¬ä¸€å¯¹
                # ç­‰ä»·åŸé€»è¾‘çš„ "ç¬¬ä¸€å¯¹" â€”â€” å– lexicographically æœ€å°çš„ä¸€å¯¹
                # ç”Ÿæˆç¬›å¡å°”ç§¯æœ€çœäº‹ä½†å¯èƒ½å¤§ï¼›è¿™é‡Œç”¨é›†åˆä¼˜åŒ–ï¼š
                rp_set = set(int(i) for i in rp_hits.tolist())
                for i in lp_hits.tolist():
                    # å¯»æ‰¾ rp_set ä¸­ != i çš„ä»»æ„å…ƒç´ 
                    if i in rp_set:
                        # å¦‚æœ rp è¿˜æœ‰å…¶ä»–ä¸åŒäº i çš„å‘½ä¸­ï¼Œé€‰å…¶ä¸€
                        # è¿™é‡Œç»§ç»­å°è¯•æ‰¾ rp ä¸­ç¬¬ä¸€ä¸ª != i çš„
                        for j in rp_hits.tolist():
                            if j != i:
                                lp_in_idx, rp_in_idx = i, j
                                matched = True
                                break
                        if matched:
                            break
                    else:
                        # ç›´æ¥å– rp_hits[0]
                        lp_in_idx, rp_in_idx = i, int(rp_hits[0])
                        matched = True
                        break
                # è‹¥ä¸Šé¢æ²¡æ‰¾åˆ°ï¼Œå†å°è¯•åå‘
                if not matched and lp_hits.size > 1 and rp_hits.size > 1:
                    i = int(lp_hits[0]); j = int(rp_hits[1] if rp_hits[0] == i else rp_hits[0])
                    if i != j:
                        lp_in_idx, rp_in_idx = i, j
                        matched = True
            else:
                matched = True
                lp_in_idx = int(lp_hits[0])
                rp_in_idx = int(rp_hits[0])

    return {
        "uuid": uuid_,
        "matched": matched,
        "lp": lp,
        "rp": rp,
        "lp_box_idx": lp_in_idx,
        "rp_box_idx": rp_in_idx,
    }

def visualize_match(img_path: Path,
                    pose_items: List[Dict[str, Any]],
                    boxes_np: np.ndarray,
                    match_info: Dict[str, Any],
                    out_path: Path):
    """å¯è§†åŒ–ï¼šå°é»‘ç‚¹æ¡†ï¼ˆç»†çº¿ï¼‰ï¼Œå…³é”®ç‚¹å°åœ†ç‚¹"""
    img = cv2.imread(str(img_path), cv2.IMREAD_COLOR)
    if img is None:
        return
    vis = img.copy()

    lp_idx = match_info.get("lp_box_idx")
    rp_idx = match_info.get("rp_box_idx")

    # ç”»æ¡†ï¼šç»¿è‰²ï¼›lp å‘½ä¸­çº¢ï¼›rp å‘½ä¸­è“ï¼›ä¸¤è€…åŒæ¡†ç´«è‰²ï¼ˆè™½ç„¶é»˜è®¤ä¸å…è®¸ï¼‰
    if boxes_np.size:
        for i in range(boxes_np.shape[0]):
            x1, y1, x2, y2 = boxes_np[i]
            lp_hit = (lp_idx == i)
            rp_hit = (rp_idx == i)
            if lp_hit and rp_hit:
                color = (255, 0, 255)
            elif lp_hit:
                color = (0, 0, 255)
            elif rp_hit:
                color = (255, 0, 0)
            else:
                color = (0, 255, 0)
            cv2.rectangle(vis, (int(x1), int(y1)), (int(x2), int(y2)), color, 1)

    # ç”»å…³é”®ç‚¹ï¼šh=è“ï¼Œå…¶ä½™=çº¢ï¼ˆä¸ä½ ç°æœ‰é¢œè‰²ä¿æŒä¸€è‡´ï¼‰
    for k in pose_items:
        if not k:
            continue
        px, py = int(k["x"]), int(k["y"])
        name = k.get("name", "?")
        color = (255, 0, 0) if name == "h" else (0, 0, 255)
        cv2.circle(vis, (px, py), 2, color, -1)

    out_path.parent.mkdir(parents=True, exist_ok=True)
    cv2.imwrite(str(out_path), vis)

def process_swd_matching(chosen_dirs, 
                         require_different_boxes=REQUIRE_DIFFERENT_BOXES,
                         use_point_conf=USE_POINT_CONF,
                         kpt_conf_thr=KPT_CONF_THR,
                         do_vis=DO_VIS,
                         limit_vis=LIMIT_VIS,
                         copy_unmatched=COPY_UNMATCHED):
    """
    æ‰¹å¤„ç† chosen_dirs åˆ—è¡¨ä¸­çš„ç›®å½•è¿›è¡Œ SWD åŒ¹é…
    
    Args:
        chosen_dirs: ç›®å½•è·¯å¾„åˆ—è¡¨
        require_different_boxes: ä¸¤ç¿¼å¿…é¡»è½å…¥ä¸åŒæ¡†
        use_point_conf: è‹¥å…³é”®ç‚¹å¸¦ confï¼Œåˆ™åº”ç”¨é˜ˆå€¼è¿‡æ»¤
        kpt_conf_thr: å…³é”®ç‚¹ç½®ä¿¡åº¦é˜ˆå€¼
        do_vis: è¾“å‡ºå¯è§†åŒ–å›¾
        limit_vis: ä»…å¯è§†åŒ–å‰ N å¼ ï¼ˆNone ä¸ºå…¨éƒ¨ï¼‰
        copy_unmatched: ä¹Ÿæ‹·è´æœªåŒ¹é…æ ·æœ¬ï¼Œä¾¿äºäººå·¥å¤æ ¸
    """
    for d in chosen_dirs:
        base = d.parent / (d.name + "_sliced_merge")
        crops_dir = base / "01_cropped_objects"

        pose_json = base / f"pose_and_det_{version}" / "02_pose_predicted_results.json"
        dot_json  = base / f"pose_and_det_{version}" / "03_dot_predicted_results.json"
        out_json  = base / f"pose_and_det_{version}" / "04_pose_wing_matched_dot_results.json"
        vis_dir   = base / f"pose_and_det_{version}" / "04_detected_swd_pose_vis"

        if not pose_json.exists() or not dot_json.exists():
            print(f"âš ï¸ ç¼ºå°‘è¾“å…¥ï¼š{pose_json} æˆ– {dot_json}ï¼Œè·³è¿‡ {d}")
            continue

        pose_list: List[Dict[str, Any]] = load_json(pose_json)
        dot_list:  List[Dict[str, Any]] = load_json(dot_json)

        # ---------- é¢„å¤„ç† dot_listï¼šuuid -> np.ndarray(N,4) ----------
        dot_map_np: Dict[str, np.ndarray] = {}
        for item in dot_list:
            u = item.get("uuid")
            if not u:
                fname = item.get("file") or os.path.basename(item.get("path", ""))
                m = UUID_RE.search(str(fname))
                u = m.group(1) if m else None
            if not u:
                continue

            boxes = item.get("boxes", [])
            if not boxes:
                dot_map_np[u] = np.empty((0, 4), dtype=np.float32)
                continue

            # åªæ”¶ bboxï¼Œè¿‡æ»¤é list/tuple
            arr = [b["bbox"] for b in boxes if isinstance(b, dict) and isinstance(b.get("bbox"), (list, tuple)) and len(b["bbox"]) == 4]
            if arr:
                dot_map_np[u] = np.asarray(arr, dtype=np.float32)
            else:
                dot_map_np[u] = np.empty((0, 4), dtype=np.float32)

        out_rows: List[Dict[str, Any]] = []
        matched_cnt = 0
        total_cnt = 0
        vis_written = 0
        passcount = 0

        if do_vis:
            vis_dir.mkdir(parents=True, exist_ok=True)

        # ---------- ä¸»å¾ªç¯ï¼ˆçƒ­ç‚¹è·¯å¾„åªèµ° Python æœ€å°‘åˆ†æ”¯ï¼‰ ----------
        for item in pose_list:
            uuid_ = item.get("uuid")
            total_cnt += 1

            insts = item.get("instances", [])
            if not insts:
                out_rows.append({"uuid": uuid_, "matched": False, "reason": "no_pose", "path": item.get("path"), "file": item.get("file")})
                passcount += 1
                continue

            pose_items = insts[0].get("kpts") or []
            boxes_np = dot_map_np.get(uuid_, np.empty((0, 4), dtype=np.float32))

            info = match_one(
                uuid_=uuid_,
                pose_items=pose_items,
                dot_boxes_np=boxes_np,
                require_different_boxes=require_different_boxes,
                use_point_conf=use_point_conf,
                kpt_conf_thr=kpt_conf_thr,
            )
            info["boxes"] = boxes_np.tolist() if boxes_np.size else []
            info["path"]  = item.get("path")
            info["file"]  = item.get("file")
            info["pose_boxes"] = insts[0].get("box") if insts and isinstance(insts[0], dict) else None
            out_rows.append(info)

            if info["matched"]:
                matched_cnt += 1

            if do_vis and (limit_vis is None or vis_written < limit_vis):
                fname = item.get("file")
                if fname:
                    img_path = crops_dir / fname
                    if img_path.exists():
                        out_img = vis_dir / f"{Path(fname).stem}_vis.jpg"
                        visualize_match(img_path, pose_items, boxes_np, info, out_img)
                        vis_written += 1

        # ä¿å­˜ JSONï¼ˆorjson æ›´å¿«ï¼›æ— åˆ™å›é€€ï¼‰
        save_json(out_rows, out_json)

        # === æ‹·è´ matched ä¸ï¼ˆå¯é€‰ï¼‰unmatched åˆ°ç‹¬ç«‹æ–‡ä»¶å¤¹ï¼ˆå«åŸå›¾ä¸å¯è§†åŒ–ï¼‰ ===
        confirmed_raw = base / f"pose_and_det_{version}" / "04_confirmed_swd" / "raw"
        confirmed_vis = base / f"pose_and_det_{version}" / "04_confirmed_swd" / "vis"
        review_raw    = base / f"pose_and_det_{version}" / "04_review_unmatched" / "raw"
        review_vis    = base / f"pose_and_det_{version}" / "04_review_unmatched" / "vis"
        confirmed_raw.mkdir(parents=True, exist_ok=True)
        confirmed_vis.mkdir(parents=True, exist_ok=True)
        if copy_unmatched:
            review_raw.mkdir(parents=True, exist_ok=True)
            review_vis.mkdir(parents=True, exist_ok=True)

        copied_match = copied_unmatch = 0
        for info in out_rows:
            fname = info.get("file")
            if not fname:
                continue
            src_raw = crops_dir / fname
            if not src_raw.exists():
                continue
            src_vis = vis_dir / f"{Path(fname).stem}_vis.jpg"

            if info.get("matched", False):
                shutil.copy2(src_raw, confirmed_raw / fname)
                if src_vis.exists():
                    shutil.copy2(src_vis, confirmed_vis / src_vis.name)
                copied_match += 1
            elif copy_unmatched:
                shutil.copy2(src_raw, review_raw / fname)
                if src_vis.exists():
                    shutil.copy2(src_vis, review_vis / src_vis.name)
                copied_unmatch += 1

        # ç»Ÿè®¡æ‰“å°
        match_ratio = (matched_cnt / total_cnt * 100.0) if total_cnt else 0.0
        print(f"\n=== åŒ¹é…å®Œæˆ: {d.name} ===")
        print(f"æ€»å°å›¾: {total_cnt}")
        print(f"åŒ¹é…ä¸º SWD: {matched_cnt}")
        print(f"åŒ¹é…ç‡: {match_ratio:.2f}%")
        print(f"ç»“æœ JSON: {out_json}")
        print(f"å·²æ‹·è´ matched: {copied_match} å¼  -> {confirmed_raw} | {confirmed_vis}")
        if copy_unmatched:
            print(f"å·²æ‹·è´ unmatched: {copied_unmatch} å¼  -> {review_raw} | {review_vis}")
        if do_vis:
            print(f"å¯è§†åŒ–ç›®å½•: {vis_dir} ï¼ˆå·²å†™ {vis_written} å¼ ï¼‰")
            print(f"è·³è¿‡æ— å§¿æ€å›¾ç‰‡: {passcount} å¼ ")

# %%
if "process_swd_matching" in steps_to_run:
    stats = process_swd_matching(
        chosen_dirs=chosen_dirs,
        require_different_boxes=True,   # True = å¼ºåˆ¶ lp ä¸ rp å‘½ä¸­ä¸åŒå°é»‘ç‚¹æ¡†, False = å¯å‘½ä¸­åŒä¸€æ¡†
        use_point_conf=True,            # True = ä½¿ç”¨å…³é”®ç‚¹ç½®ä¿¡åº¦é˜ˆå€¼, False = ä¸ä½¿ç”¨   
        kpt_conf_thr=0.88,              # å…³é”®ç‚¹ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆä»…åœ¨ use_point_conf=True æ—¶ç”Ÿæ•ˆï¼‰
        do_vis=True,                    # False = æ‰€æœ‰æ–‡ä»¶å‡ä¸è¾“å‡ºå¯è§†åŒ–å›¾ï¼ŒTrue = è¾“å‡ºå¯è§†åŒ–å›¾
        limit_vis=None,                 # ä»…å¯è§†åŒ–å‰ N å¼ ï¼›None ä¸ºå…¨éƒ¨
        copy_unmatched=False,           # æ˜¯å¦æ‹·è´æœªåŒ¹é…æ ·æœ¬ï¼Œä¾¿äºäººå·¥å¤æ ¸
    )
else:
    print("Step_6 åˆ¤å®š SWD â‡’ åŒ¹é…è§„åˆ™ï¼šä¸¤ç¿¼å…³é”®ç‚¹åˆ†åˆ«è½å…¥ä¸¤ä¸ªä¸åŒçš„å°é»‘ç‚¹æ¡† è¢«è·³è¿‡")

# %% [markdown]
# # Step_7  æ ¹æ®åˆ¤å®šç»“æœï¼Œè¿‡æ»¤é”™è¯¯æ•°æ®
# 

# %%
import json
from pathlib import Path
from typing import Dict, Any


def filter_annotations_by_matched_uuid(
    matched_file: Path, annotations_file: Path, output_file: Path
) -> None:
    """
    æ ¹æ® matched_file ä¸­æ ‡è®° matched=True çš„ uuidï¼Œ
    ä» annotations_file ä¸­ç­›é€‰å¯¹åº”çš„æ ‡æ³¨ï¼Œå¹¶ä¿å­˜åˆ° output_fileã€‚

    Args:
        matched_file (str): åŒ¹é…ç»“æœ JSON æ–‡ä»¶è·¯å¾„ï¼Œä¾‹å¦‚ '04_pose_wing_matched_dot_results.json'
        annotations_file (str): åŸå§‹æ ‡æ³¨ JSON æ–‡ä»¶è·¯å¾„ï¼Œä¾‹å¦‚ '01_merged_annotations.json'
        output_file (str): è¾“å‡º JSON æ–‡ä»¶è·¯å¾„ï¼Œä¾‹å¦‚ '09_filtered_annotations.json'
    """

    # è¯»å–æ•°æ®
    with matched_file.open("r", encoding="utf-8") as f:
        matched_data = json.load(f)

    with annotations_file.open("r", encoding="utf-8") as f:
        annotations_data: Dict[str, Any] = json.load(f)

    # è·å– matched çš„ UUID
    matched_uuids = {item["uuid"] for item in matched_data if item.get("matched")}

    # è¿‡æ»¤æ ‡æ³¨
    filtered_annotations = {
        name: [ann for ann in anns if ann["uuid"] in matched_uuids]
        for name, anns in annotations_data.items()
    }
    # ç§»é™¤ç©ºçš„ç±»åˆ«
    filtered_annotations = {
        name: anns for name, anns in filtered_annotations.items() if anns
    }

    # ä¿å­˜ç»“æœ
    output_path = output_file
    with output_path.open("w", encoding="utf-8") as f:
        json.dump(filtered_annotations, f, indent=2, ensure_ascii=False)

    print(f"âœ… è¿‡æ»¤å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° {output_path}")

# %%
for chosen_dir in chosen_dirs:
    print(f"é€‰æ‹©çš„ç›®å½•: {chosen_dir}")
    filter_annotations_by_matched_uuid(
        matched_file= chosen_dir.parent / (chosen_dir.name + "_sliced_merge") / f"pose_and_det_{version}" / "04_pose_wing_matched_dot_results.json",
        annotations_file=chosen_dir.parent / (chosen_dir.name + "_sliced_merge") / "01_merged_annotations.json",
        output_file=chosen_dir.parent / (chosen_dir.name + "_sliced_merge") / f"pose_and_det_{version}" / "09_filtered_annotations.json"
    )


# %% [markdown]
# # æŸ¥çœ‹æ ¹ç›®å½•ä¸‹éœ€è¦è¿è¡Œçš„æ–‡ä»¶å¤¹

# %%
from pathlib import Path

version = "v1"
run_type = "pose_and_det"  # "pose_and_det" or "cls"

# path = Path("/workspace/models/SAHI/run_v8")
# chosen_dirs = [d / "raw_data" for d in path.iterdir() if d.is_dir()]
# print("å­æ–‡ä»¶å¤¹è·¯å¾„åˆ—è¡¨ï¼š", chosen_dirs)

# %% [markdown]
# # objectså»é‡ï¼Œç»“æœå¯è§†åŒ–
# ### è¾“å…¥è¾“å‡º
# ![image-4.png](attachment:image-4.png)
# 
# 
# ### æ•ˆæœ
# ![image.png](attachment:image.png)![image-2.png](attachment:image-2.png)
# 
# ![image-3.png](attachment:image-3.png)![image-5.png](attachment:image-5.png)

# %%
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ—¶é—´åºåˆ—ç›®æ ‡è·Ÿè¸ªä¸å¯è§†åŒ–ç³»ç»Ÿï¼ˆä»…å¤šè¾¹å½¢ + poly_iouï¼‰
- è¾“å…¥ï¼šç±»ä¼¼ 09_filtered_annotations.json çš„ dict[name] -> List[annotation]ï¼Œannotation å¿…é¡»åŒ…å«:
        label, points(å¤šè¾¹å½¢), original_name(å¯é€‰), uuid(å¯é€‰), score(å¯é€‰)
- åŸå›¾ç›®å½•ï¼šraw_data/ ä¸‹çš„ *.jpgï¼Œæ–‡ä»¶åä»¥é”®åä¸ºå‰ç¼€ï¼ˆå¦‚ 0801_1034_880*.jpgï¼‰
- å‘½åè§£æï¼šMMDD_HHMMï¼ˆç¤ºä¾‹ï¼š0801_1034_880ï¼‰

åŠŸèƒ½ï¼š
1) assign_persistent_idsï¼šè·¨æ—¶åˆ»åŒ¹é…åˆ†é…ç¨³å®š IDï¼ˆpoly IoUï¼‰
2) draw_overlaysï¼šåŸå›¾å åŠ å¯è§†åŒ–ï¼ˆæ–°=ç»¿ï¼Œé‡å¤=çº¢ã€å¹½çµè½¨è¿¹ã€å³ä¸Šè§’ NOW/SUM å¾½æ ‡ï¼‰
3) build_track_galleriesï¼šæŒ‰ ID è£å‰ªæ—¶é—´åºåˆ—å°å›¾
4) export_stats_Bï¼šå¯¼å‡ºç»Ÿè®¡ï¼ˆSlots.csv & IDs.csvï¼‰
5) ä¿å­˜å®Œæ•´æ—¶é—´çº¿ JSONï¼ˆtimeline.jsonï¼‰

ä¾èµ–ï¼šshapely, numpy, opencv-python, orjson
"""

import os
import re
from pathlib import Path
from typing import Any, Dict, List, Tuple, Optional, Iterable
from collections import defaultdict, Counter

import numpy as np
import cv2
import orjson
import json as pyjson

# ============== å¼ºåˆ¶ä¾èµ– shapelyï¼ˆä»…å¤šè¾¹å½¢ + poly_iouï¼‰ ==============
try:
    from shapely.geometry import Polygon
except Exception as e:
    raise ImportError(
        "æœ¬è„šæœ¬ä»…æ”¯æŒå¤šè¾¹å½¢ + poly_iouï¼Œè¯·å…ˆå®‰è£… shapelyï¼š\n"
        "  pip install shapely\n"
    ) from e


# ============== æ—¶é—´è§£æï¼šæ–‡ä»¶å MMDD_HHMM[...] ==============
_FN_RE = re.compile(r"(?P<mm>\d{2})(?P<dd>\d{2})_(?P<hh>\d{2})(?P<mi>\d{2})")

def parse_mmdd_hhmm(name: str) -> Optional[Tuple[str, str]]:
    """
    è¿”å› (date_str 'MM-DD', time_str 'HH:MM')ï¼Œå¤±è´¥è¿”å› None
    """
    m = _FN_RE.search(name)
    if not m:
        return None
    mm, dd, hh, mi = m.group("mm", "dd", "hh", "mi")
    return f"{mm}-{dd}", f"{hh}:{mi}"

def slot_sort_key(date_str: str, time_str: str) -> Tuple[int,int,int,int]:
    return (int(date_str[:2]), int(date_str[3:]), int(time_str[:2]), int(time_str[3:]))


# ============== IO ==============
def json_load(path: Path) -> Dict[str, Any]:
    with open(path, "rb") as f:
        return orjson.loads(f.read())

def json_dump(obj: Any, path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "wb") as f:
        f.write(orjson.dumps(obj, option=orjson.OPT_INDENT_2))


# ============== åŸå›¾ç´¢å¼• ==============
def build_image_index(image_dir: Path) -> Dict[str, str]:
    """
    è¿”å› {stem -> path}ï¼Œè‹¥æ‰¾ä¸åˆ°ç²¾ç¡® stemï¼Œåç»­ä¼šå°è¯•å‰ç¼€åŒ¹é…
    """
    idx = {}
    for p in image_dir.glob("*.jpg"):
        idx[p.stem] = str(p)
    return idx


# ============== å¤šè¾¹å½¢ IoUï¼ˆä»… shapelyï¼‰ ==============
def poly_iou(poly_a: List[List[float]], poly_b: List[List[float]]) -> float:
    try:
        A = Polygon(poly_a)
        B = Polygon(poly_b)
        if not (A.is_valid and B.is_valid):
            return 0.0
        inter = A.intersection(B).area
        if inter <= 0:
            return 0.0
        u = A.area + B.area - inter
        return float(inter / u) if u > 0 else 0.0
    except Exception:
        return 0.0


# ============== æ ‡ç­¾æ ‡å‡†åŒ– / è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰ ==============
def make_label_normalizer(label_map: Optional[Dict[str, str]] = None,
                          whitelist: Optional[Iterable[str]] = None):
    """
    è¿”å› normalize(label) -> æ ‡å‡†åŒ–åçš„ label
    - label_map: åˆ«ååˆ°ç»Ÿä¸€åï¼Œå¦‚ {'SWD':'swd','MAYSWD':'mayswd','may_swd':'mayswd'}
    - whitelist: åªä¿ç•™ç™½åå•ä¸­çš„æ ‡ç­¾ï¼›ä¸åœ¨ç™½åå•åˆ™è¿”å› 'other'ï¼ˆæˆ–è¿”å› '' è¡¨ç¤ºå¿½ç•¥ï¼‰
    """
    label_map = {k.lower(): v for k, v in (label_map or {}).items()}
    wl = set(x.lower() for x in whitelist) if whitelist else None

    def normalize(label: str) -> str:
        if label is None:
            return ""
        s = str(label).strip()
        if not s:
            return ""
        s_lo = s.lower()
        s_std = label_map.get(s_lo, s_lo)
        if wl is not None and s_std not in wl:
            return "other"  # å¦‚éœ€å¿½ç•¥å¯æ”¹ä¸ºè¿”å› ""
        return s_std
    return normalize


# ============== æ•°æ®è¯»å–ï¼š09_filtered_annotations.jsonï¼ˆæ”¯æŒ normalize_labelï¼‰ ==============
def load_annotations_json(json_path: Path, 
                            normalize_label=None) -> Dict[str, List[Dict[str, Any]]]:
    """
    æœŸæœ›ç»“æ„ï¼š{ img_key: [ {label, points, ...}, ... ], ... }
    ä»…ä½¿ç”¨ points å¤šè¾¹å½¢ï¼›è‹¥ç¼ºå¤±åˆ™è·³è¿‡è¯¥æ¡ annotation
    """
    if normalize_label is None:
        normalize_label = lambda x: ("" if x is None else str(x))

    data = json_load(json_path)
    cleaned: Dict[str, List[Dict[str, Any]]] = {}
    for img_key, anns in data.items():
        keep = []
        for a in anns or []:
            pts = a.get("points")
            if isinstance(pts, list) and len(pts) >= 3:
                lab = normalize_label(a.get("label"))
                keep.append({
                    "label": lab,
                    "points": [[float(x), float(y)] for x, y in pts],
                    "uuid": a.get("uuid"),
                    "original_name": a.get("original_name", img_key),
                    "score": a.get("score"),
                })
        if keep:
            cleaned[img_key] = keep
    return cleaned


# ============== ç¨³å®š ID åˆ†é…ï¼ˆä»… poly_iouï¼‰ ==============
def assign_persistent_ids(
    cleaned_annotations: Dict[str, List[Dict[str, Any]]],
    iou_threshold: float = 0.5,
    class_agnostic: bool = False
) -> Tuple[List[Dict[str, Any]], Dict[int, List[Dict[str, Any]]]]:
    """
    è¿”å›ï¼š
      - timeline: List[ {date,time,img,label,id,is_new,repeat_idx,points} ]
      - id_tracks: Dict[id] -> List[occurrence(dict)]
    åŒ¹é…ç­–ç•¥ï¼š
      - åŒæ—¶åˆ»å†…éƒ¨å»é‡ï¼šç›¸åŒ labelï¼ˆæˆ– class_agnostic=Trueï¼‰ä¹‹é—´ IoU >= é˜ˆå€¼è§†ä¸ºé‡å¤ï¼Œä»…ä¿ç•™ä¸€ä¸ª
      - è·¨æ—¶åˆ»åŒ¹é…ï¼šä¸â€œå·²è§åº“â€ä¸­ IoU æœ€é«˜ä¸” >= é˜ˆå€¼è€…åŒ¹é…ï¼Œå¦åˆ™åˆ†é…æ–° ID
    """
    by_slot: Dict[Tuple[str, str], List[Tuple[str, Dict[str, Any]]]] = defaultdict(list)
    for img_key, anns in cleaned_annotations.items():
        ts = parse_mmdd_hhmm(img_key)
        if not ts:
            continue
        d, t = ts
        for a in anns:
            by_slot[(d, t)].append((img_key, a))

    slots = sorted(by_slot.keys(), key=lambda k: slot_sort_key(k[0], k[1]))

    next_id = 1
    pool_by_label: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
    pool_all: List[Dict[str, Any]] = []
    id_tracks: Dict[int, List[Dict[str, Any]]] = defaultdict(list)
    timeline: List[Dict[str, Any]] = []

    for d, t in slots:
        obs = by_slot[(d, t)]

        # 1) åŒæ—¶åˆ»å†…éƒ¨å»é‡
        unique_obs: List[Tuple[str, Dict[str, Any]]] = []
        for img_key, det in obs:
            label, pts = det.get("label", ""), det.get("points")
            if not pts:
                continue
            dup = False
            for _, u in unique_obs:
                if (not class_agnostic) and (u.get("label", "") != label):
                    continue
                s = poly_iou(pts, u["points"])
                if s >= iou_threshold:
                    dup = True
                    break
            if not dup:
                unique_obs.append((img_key, det))

        # 2) è·¨æ—¶åˆ»åŒ¹é…
        for img_key, det in unique_obs:
            label, pts = det.get("label", ""), det.get("points")
            candidates = pool_all if class_agnostic else pool_by_label[label]
            best = None
            best_s = -1.0
            for c in candidates:
                s = poly_iou(pts, c["points"])
                if s >= iou_threshold and s > best_s:
                    best = c
                    best_s = s

            if best is None:
                cur_id = next_id
                next_id += 1
                entry = {"id": cur_id, "points": pts, "last_dt": (d, t)}
                if class_agnostic:
                    pool_all.append(entry)
                else:
                    pool_by_label[label].append(entry)
                repeat_idx = 1
                is_new = True
            else:
                cur_id = best["id"]
                best["points"] = pts or best["points"]
                best["last_dt"] = (d, t)
                repeat_idx = len(id_tracks[cur_id]) + 1
                is_new = False

            occ = {
                "date": d, "time": t, "img": img_key,
                "label": label, "id": cur_id, "is_new": is_new,
                "repeat_idx": repeat_idx, "points": pts
            }
            id_tracks[cur_id].append(occ)
            timeline.append(occ)

    return timeline, id_tracks


# ============== å¯è§†åŒ–ï¼šå³ä¸Šè§’è®¡æ•°å¾½æ ‡ ==============
def draw_top_right_counter(canvas: np.ndarray, now: int, cum: int):
    H, W = canvas.shape[:2]
    margin = max(8, W // 200)
    pad = max(8, W // 300)
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = max(0.8, min(2.2, W / 800.0))
    thickness = max(2, int(round(font_scale + 1)))

    line1 = f"NOW: {now}"
    line2 = f"SUM: {cum}"

    (w1, h1), _ = cv2.getTextSize(line1, font, font_scale, thickness)
    (w2, h2), _ = cv2.getTextSize(line2, font, font_scale, thickness)
    box_w = max(w1, w2) + 2 * pad
    line_gap = max(6, int(0.25 * h1))
    box_h = h1 + h2 + line_gap + 2 * pad

    x2 = W - margin
    y1 = margin
    x1 = x2 - box_w
    y2 = y1 + box_h

    overlay = canvas.copy()
    cv2.rectangle(overlay, (x1, y1), (x2, y2), (0, 0, 0), thickness=-1)
    cv2.addWeighted(overlay, 0.55, canvas, 0.45, 0, dst=canvas)

    tx = x1 + pad
    ty1 = y1 + pad + h1
    ty2 = ty1 + line_gap + h2
    cv2.putText(canvas, line1, (tx, ty1), font, font_scale, (255, 255, 255), thickness, cv2.LINE_AA)
    cv2.putText(canvas, line2, (tx, ty2), font, font_scale, (255, 255, 255), thickness, cv2.LINE_AA)


# ============== å¯è§†åŒ–ï¼šå åŠ ç»˜åˆ¶ï¼ˆæ–°=ç»¿ï¼Œé‡å¤=çº¢ï¼Œå¹½çµè½¨è¿¹ï¼‰ ==============
def draw_overlays(
    timeline: List[Dict[str, Any]],
    image_dir: Path,
    out_dir: Path,
    ghost_trail_steps: int = 0,
    ghost_alpha: float = 0.25
):
    os.makedirs(out_dir, exist_ok=True)
    index = build_image_index(image_dir)

    # åˆ†å›¾æ”¶é›†
    by_img = defaultdict(list)
    for r in timeline:
        by_img[r["img"]].append(r)

    # æŒ‰æ—¶é—´æ’åºçš„å…¨å±€æ—¶é—´çº¿ï¼Œç”¨äºå›æº¯è½¨è¿¹
    def dt_key(r): return slot_sort_key(r["date"], r["time"])
    timeline_sorted = sorted(timeline, key=dt_key)
    hist_by_id = defaultdict(list)
    for r in timeline_sorted:
        hist_by_id[r["id"]].append(r)

    # ä¸ºå¾½æ ‡å‡†å¤‡ per-slot now/cumï¼Œå¹¶æ˜ å°„åˆ°å›¾
    per_slot_counts = Counter()
    cum_total = 0
    slot_order = sorted({(r["date"], r["time"]) for r in timeline}, key=lambda x: slot_sort_key(*x))
    slot_to_cum = {}
    for d, t in slot_order:
        now = sum(1 for r in timeline if r["date"] == d and r["time"] == t and r["is_new"])
        cum_total += now
        per_slot_counts[(d, t)] = now
        slot_to_cum[(d, t)] = cum_total

    for img_key, rows in by_img.items():
        # æ‰¾åŸå›¾
        img_path = index.get(img_key)
        if img_path is None:
            for p in Path(image_dir).glob(f"{img_key}*.jpg"):
                img_path = str(p); break
        if not img_path or not os.path.exists(img_path):
            continue

        canvas = cv2.imread(img_path)
        if canvas is None:
            continue

        # å¹½çµè½¨è¿¹
        if ghost_trail_steps > 0:
            ghost = canvas.copy()
            for r in rows:
                hist = hist_by_id[r["id"]]
                idx = None
                for i, k in enumerate(hist):
                    if k["img"] == img_key:
                        idx = i; break
                if idx is None: 
                    continue
                start = max(0, idx - ghost_trail_steps)
                for j in range(start, idx):
                    pj = hist[j]
                    pts = np.asarray(pj["points"], dtype=np.int32).reshape(-1,1,2)
                    cv2.polylines(ghost, [pts], True, (200,200,200), 1, cv2.LINE_AA)
            canvas = cv2.addWeighted(ghost, ghost_alpha, canvas, 1-ghost_alpha, 0)

        # å½“å‰å¤šè¾¹å½¢
        for r in rows:
            pts = r["points"]
            if not pts:
                continue
            pts_i = np.asarray(pts, dtype=np.int32).reshape(-1,1,2)
            color = (0,255,0) if r["is_new"] else (0,0,255)  # æ–°=ç»¿ï¼Œé‡å¤=çº¢
            cv2.polylines(canvas, [pts_i], True, color, 2, cv2.LINE_AA)

            # æ ‡æ³¨æ–‡å­—ï¼ˆID/label/å‡ºç°æ¬¡æ•°ï¼‰
            if pts_i.size > 0:
                xs = [p[0] for p in pts]; ys = [p[1] for p in pts]
                x1, y1 = int(min(xs)) + 8, int(min(ys)) + 18
                txt = f"ID#{r['id']} {r.get('label','')} x{r['repeat_idx']}"
                cv2.putText(canvas, txt, (x1 + 64, y1), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2, cv2.LINE_AA)

        # å³ä¸Šè§’å¾½æ ‡
        dd, tt = parse_mmdd_hhmm(img_key) or (None, None)
        if dd and tt:
            now = per_slot_counts[(dd, tt)]
            cum = slot_to_cum[(dd, tt)]
            draw_top_right_counter(canvas, now=now, cum=cum)

        out_path = os.path.join(out_dir, f"{img_key}_track_vis.jpg")
        cv2.imwrite(out_path, canvas, [int(cv2.IMWRITE_JPEG_QUALITY), 90])


# ============== è½¨è¿¹ç›¸å†Œï¼ˆæŒ‰ ID è£å‰ªåºåˆ—ï¼‰ ==============
def _crop_square(img: np.ndarray, pts: List[List[float]], margin: int = 8) -> Optional[np.ndarray]:
    H, W = img.shape[:2]
    xs = [p[0] for p in pts]; ys = [p[1] for p in pts]
    x1, y1, x2, y2 = int(min(xs)), int(min(ys)), int(max(xs)), int(max(ys))
    side = max(x2 - x1, y2 - y1)
    cx, cy = (x1 + x2) // 2, (y1 + y2) // 2
    l = max(0, cx - side//2 - margin)
    t = max(0, cy - side//2 - margin)
    r = min(W, cx + side//2 + margin)
    b = min(H, cy + side//2 + margin)
    if r - l <= 1 or b - t <= 1:
        return None
    return img[t:b, l:r].copy()

def build_track_galleries(
    id_tracks: Dict[int, List[Dict[str, Any]]],
    image_dir: Path,
    out_dir: Path,
    margin: int = 8,
    workers: Optional[int] = None,
    jpeg_quality: int = 90
):
    os.makedirs(out_dir, exist_ok=True)
    index = build_image_index(image_dir)
    # æ¯ä¸ª id ä¸€ä¸ªæ–‡ä»¶å¤¹
    save_dirs = {}
    for tid in id_tracks.keys():
        d = os.path.join(out_dir, f"id_{tid:04d}")
        os.makedirs(d, exist_ok=True)
        save_dirs[tid] = d

    tasks_by_img: Dict[str, List[Tuple[List[List[float]], str]]] = defaultdict(list)
    _fallback: Dict[str, Optional[str]] = {}

    for tid, occs in id_tracks.items():
        occs_sorted = sorted(occs, key=lambda r: slot_sort_key(r["date"], r["time"]))
        for k, r in enumerate(occs_sorted, start=1):
            pts = r.get("points")
            if not pts:
                continue
            img_key = r["img"]
            img_path = index.get(img_key)
            if img_path is None:
                if img_key not in _fallback:
                    hit = None
                    for p in Path(image_dir).glob(f"{img_key}*.jpg"):
                        hit = str(p); break
                    _fallback[img_key] = hit
                img_path = _fallback[img_key]
            if not img_path or not os.path.exists(img_path):
                continue
            fn = f"{k:02d}_{r['date']}_{r['time']}_{img_key}.jpg"
            save_path = os.path.join(save_dirs[tid], fn)
            tasks_by_img[img_path].append((pts, save_path))

    if not tasks_by_img:
        return

    params = [int(cv2.IMWRITE_JPEG_QUALITY), int(jpeg_quality)]

    def _process(img_path: str, todo: List[Tuple[List[List[float]], str]]):
        img = cv2.imread(img_path)
        if img is None:
            return 0
        ok = 0
        for pts, save_path in todo:
            crop = _crop_square(img, pts, margin=margin)
            if crop is None:
                continue
            cv2.imwrite(save_path, crop, params)
            ok += 1
        return ok

    if workers is None:
        workers = min(32, (os.cpu_count() or 4) + 4)

    import concurrent.futures
    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as ex:
        futs = [ex.submit(_process, p, todo) for p, todo in tasks_by_img.items()]
        for _ in concurrent.futures.as_completed(futs):
            pass


# ============== ç»Ÿä¸€ç»Ÿè®¡ï¼ˆæŒ‰ä½ çš„å­—æ®µè¦æ±‚æ”¹å/ç²¾ç®€ï¼‰ ==============
def export_stats_B(
    timeline: list,
    slots_csv_path: str,
    ids_csv_path: str,
    final_label_by_id: dict | None = None,      # ä»ä¿ç•™æ¥å£ï¼Œä½†ä¸å†è¾“å‡ºåˆ° IDs.csv
    final_conf_by_id: dict | None = None
):
    """
    è¾“å‡ºï¼š
    - Slots.csvï¼šdatetime,new,repeat,total,cumulative_total,<label>_new,<label>_repeat,<label>_total,...,new_rate
      ï¼ˆç§»é™¤ date,timeï¼‰
    - IDs.csvï¼šid,appearances_times,first_time_slot,last_time_slot,main_label,main_ratio,
               labels_present(å¯¹è±¡å­—ç¬¦ä¸²),label_switch_times
      ï¼ˆç§»é™¤ span_slots/final_label/final_label_confidence/num_labels/purity/å„ <label>_count åŠ¨æ€åˆ—ï¼‰
    """
    import csv
    from collections import defaultdict, Counter

    # ç©ºè¾“å…¥æ—¶ï¼šå†™è¡¨å¤´
    if not timeline:
        with open(slots_csv_path, "w", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow(["datetime","new","repeat","total","cumulative_total","new_rate"])
        with open(ids_csv_path, "w", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow([
                "id","appearances_times","first_time_slot","last_time_slot",
                "main_label","main_ratio","labels_present","label_switch_times"
            ])
        return

    def slot_key(d, t):  # d='MM-DD', t='HH:MM'
        return (int(d[:2]), int(d[3:]), int(t[:2]), int(t[3:]))

    # â€”â€” æ ‡ç­¾å…¨é›†ï¼ˆæŒ‰å­—æ¯åºç¨³å®šï¼‰ â€”â€” #
    labels = sorted({str(r.get("label","")) for r in timeline if str(r.get("label","")) != ""})

    # =========================
    # 1) SLOTSï¼ˆæ¯æ—¶é—´æ§½èšåˆï¼‰
    # =========================
    grp = defaultdict(list)  # (date,time) -> [rows...]
    for r in timeline:
        grp[(r["date"], r["time"])] .append(r)

    ordered_slots = sorted(grp.keys(), key=lambda dt: slot_key(dt[0], dt[1]))

    cum_total = 0
    slot_rows = []
    for d, t in ordered_slots:
        sub = grp[(d, t)]
        new_cnt    = sum(1 for x in sub if x.get("is_new", False) is True)
        repeat_cnt = sum(1 for x in sub if x.get("is_new", False) is False)
        total_cnt  = len(sub)
        cum_total += new_cnt

        row = {
            "datetime": f"{d} {t}",
            "new": new_cnt,
            "repeat": repeat_cnt,
            "total": total_cnt,
            "cumulative_total": cum_total,
            "new_rate": (new_cnt / total_cnt) if total_cnt else 0.0,
        }

        for lab in labels:
            lab_sub = [x for x in sub if str(x.get("label","")) == lab]
            lab_new = sum(1 for x in lab_sub if x.get("is_new", False) is True)
            lab_rep = sum(1 for x in lab_sub if x.get("is_new", False) is False)
            row[f"{lab}_new"]    = lab_new
            row[f"{lab}_repeat"] = lab_rep
            row[f"{lab}_total"]  = lab_new + lab_rep

        slot_rows.append(row)

    slot_header = ["datetime","new","repeat","total","cumulative_total"]
    for lab in labels:
        slot_header += [f"{lab}_new", f"{lab}_repeat", f"{lab}_total"]
    slot_header += ["new_rate"]

    with open(slots_csv_path, "w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=slot_header)
        w.writeheader()
        for r in slot_rows:
            w.writerow(r)

    # =========================
    # 2) IDsï¼ˆæ¯è½¨è¿¹èšåˆï¼‰
    # =========================
    by_id = defaultdict(list)
    for r in timeline:
        by_id[int(r["id"])] .append(r)

    id_rows = []
    for tid, occs in by_id.items():
        occs_sorted = sorted(occs, key=lambda r: slot_key(r["date"], r["time"]))
        appearances = len(occs_sorted)
        first_slot  = f"{occs_sorted[0]['date']}_{occs_sorted[0]['time']}"
        last_slot   = f"{occs_sorted[-1]['date']}_{occs_sorted[-1]['time']}"

        # â€”â€” è®¡æ•°æ¯ä¸ª label â€”â€” #
        cnt = Counter(str(o.get("label","")) for o in occs_sorted if str(o.get("label","")) != "")
        main_lab, main_cnt = ("", 0)
        if cnt:
            main_lab, main_cnt = max(cnt.items(), key=lambda kv: kv[1])
        main_ratio = (main_cnt / appearances) if appearances else 0.0

        # â€”â€” ç±»åˆ«åˆ‡æ¢æ¬¡æ•°ï¼ˆæŒ‰æ—¶é—´åºï¼‰ â€”â€” #
        label_seq = [str(o.get("label","")) for o in occs_sorted]
        label_seq = [x for x in label_seq if x != ""]
        switch_times = sum(1 for i in range(1, len(label_seq)) if label_seq[i] != label_seq[i-1])

        # â€”â€” labels_present ä»¥å¯¹è±¡å­—ç¬¦ä¸²è¾“å‡º â€”â€” #
        labels_present_obj = {lab: cnt[lab] for lab in sorted(cnt.keys())}
        labels_present_str = pyjson.dumps(labels_present_obj, ensure_ascii=False, separators=(',',':'))

        row = {
            "id": tid,
            "appearances_times": appearances,
            "first_time_slot": first_slot,
            "last_time_slot": last_slot,
            "main_label": main_lab,
            "main_ratio": main_ratio,
            "labels_present": labels_present_str,
            "label_switch_times": switch_times,
        }

        id_rows.append(row)

    id_header = [
        "id","appearances_times","first_time_slot","last_time_slot",
        "main_label","main_ratio","labels_present","label_switch_times"
    ]

    with open(ids_csv_path, "w", newline="", encoding="utf-8") as f:
        import csv
        w = csv.DictWriter(f, fieldnames=id_header)
        w.writeheader()
        for r in sorted(id_rows, key=lambda x: x["id"]):
            w.writerow(r)



# ============== ä¸»æµç¨‹ï¼ˆæ”¯æŒæ ‡ç­¾å½’ä¸€/ç™½åå•ï¼‰ ==============
def run_pipeline(
    annotations_json: Path,
    image_dir: Path,
    out_root: Path,
    iou_threshold: float = 0.5,
    class_agnostic: bool = False,
    ghost_trail_steps: int = 0,
    ghost_alpha: float = 0.25,
    label_map: Optional[Dict[str,str]] = None,        # å¯é€‰ï¼šåˆ«ååˆ°ç»Ÿä¸€å
    label_whitelist: Optional[Iterable[str]] = None   # å¯é€‰ï¼šåªä¿ç•™è¿™äº›æ ‡ç­¾ï¼Œå…¶å®ƒå½’ 'other'
):
    os.makedirs(out_root, exist_ok=True)

    # 0) æ ‡ç­¾æ ‡å‡†åŒ–å™¨ï¼ˆå¯é€‰ï¼‰
    normalizer = make_label_normalizer(label_map, label_whitelist)

    # 1) è¯»æ³¨é‡Šï¼ˆåŠ å…¥æ ‡å‡†åŒ–ï¼‰
    cleaned = load_annotations_json(annotations_json, normalize_label=normalizer)

    # 2) åˆ†é…ç¨³å®š IDï¼ˆpoly IoUï¼‰
    timeline, id_tracks = assign_persistent_ids(
        cleaned, iou_threshold=iou_threshold, class_agnostic=class_agnostic
    )

    # 3) å¯è§†åŒ–å åŠ 
    vis_dir = out_root / "vis"
    draw_overlays(
        timeline, image_dir=image_dir, out_dir=vis_dir,
        ghost_trail_steps=ghost_trail_steps, ghost_alpha=ghost_alpha
    )

    # 4) è½¨è¿¹ç›¸å†Œ
    crops_dir = out_root / "galleries"
    build_track_galleries(id_tracks, image_dir=image_dir, out_dir=crops_dir)

    # 5) ç»Ÿä¸€ç»Ÿè®¡
    slots_csv = os.path.join(out_root, "Slots.csv")
    ids_csv   = os.path.join(out_root, "IDs.csv")

    export_stats_B(
        timeline,
        slots_csv_path=slots_csv,
        ids_csv_path=ids_csv,
        final_label_by_id=None,
        final_conf_by_id=None
    )

    # 6) ä¿å­˜æ—¶é—´çº¿
    json_dump(timeline, out_root / "timeline.json")

    # æ§åˆ¶å°æ‘˜è¦ï¼ˆå«æœ¬æ‰¹æ ‡ç­¾é›†åˆï¼‰
    labels_present = sorted({str(r.get("label","")) for r in timeline if str(r.get("label","")) != ""})
    num_ids = len(id_tracks)
    num_slots = len(set((r["date"], r["time"]) for r in timeline))
    num_obs = len(timeline)
    print("=== Summary ===")
    print(f"Observations (timeline rows): {num_obs}")
    print(f"Unique IDs: {num_ids}")
    print(f"Time slots: {num_slots}")
    print(f"Labels in this dataset: {labels_present}  (count={len(labels_present)})")
    print(f"Output:")
    print(f"  - Overlays:   {vis_dir}")
    print(f"  - Galleries:  {crops_dir}")
    print(f"  - Slots CSV:  {slots_csv}")
    print(f"  - IDs CSV:    {ids_csv}")
    print(f"  - Timeline:   {os.path.join(out_root, 'timeline.json')}")

# %%
for chosen_dir in chosen_dirs:
    print(f"Processing directory: {chosen_dir}")
    annotations_file = chosen_dir.parent / "raw_data_sliced_merge"  / f"{run_type}_{version}" / "09_filtered_annotations.json"
    if not annotations_file.exists():
        print(f"Annotations file not found: {annotations_file}")
        continue

    raw_image_dir = chosen_dir.parent / "raw_data"
    if not raw_image_dir.exists():
        print(f"Raw image directory not found: {raw_image_dir}")
        continue

    output_directory = chosen_dir.parent / "raw_data_sliced_merge" / f"{run_type}_{version}" / "10_visualization_tracking_results"
    run_pipeline(
        annotations_json=annotations_file,
        image_dir=raw_image_dir,
        out_root=output_directory,
        iou_threshold=0.5,
        class_agnostic=True,  # True=ç±»åˆ«å¦‚æœä¸åŒä¹Ÿå¯åŒ¹é…ä¸ºé‡å¤objectsï¼ŒFalse=å¿…é¡»åŒç±»æ‰è®°ä¸ºé‡å¤objects 
        ghost_trail_steps=1,
        ghost_alpha=0.25,
        label_map=None,
        label_whitelist=['swd', 'mayswd']  # åªä¿ç•™è¿™äº›æ ‡ç­¾ï¼Œå…¶å®ƒå½’ 'other'
    )

# %%



# %% [markdown]
# # æŸ¥çœ‹æ ¹ç›®å½•ä¸‹éœ€è¦è¿è¡Œçš„æ–‡ä»¶å¤¹

# %%
from pathlib import Path

version = "v1"
run_type = "pose_and_det"  # "pose_and_det" or "cls"

# path = Path("/workspace/models/SAHI/run_v8")
# chosen_dirs = [d / "raw_data" for d in path.iterdir() if d.is_dir()]
# print("å­æ–‡ä»¶å¤¹è·¯å¾„åˆ—è¡¨ï¼š", chosen_dirs)

# %% [markdown]
# # ç®€å•ç»Ÿè®¡ -- ç»Ÿè®¡å„ç±»åˆ«objectsæ•°é‡ï¼Œå¹¶å¯¼å‡ºä¸ºCSVæ–‡ä»¶
# ### è¾“å…¥è¾“å‡º
# ![image.png](attachment:image.png)
# 
# ### æ•ˆæœ
# ![image-2.png](attachment:image-2.png)

# %%
# ç»Ÿè®¡æ¯ä¸ªç±»åˆ«åœ¨æ¯ä¸ªæ—¶é—´ç‚¹çš„æ•°é‡å¹¶å¯¼å‡ºä¸º CSV

"""
ç»Ÿè®¡: å„ç±»åˆ«(label)åœ¨æ¯ä¸ªæ—¶åˆ»(original_nameè§£æ)çš„æ•°é‡ & æŒ‰æ—¶é—´æ’åºåçš„ç´¯ç§¯æ•°é‡
è¾“å…¥: 03_filtered_annotations.json
è¾“å‡º: 04_filtered_annotations_counts.csv  |  04_filtered_annotations_cumulative.csv
"""

import json, csv, re
from pathlib import Path
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Iterable, Optional

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) original_name â†’ è§£ææ—¶é—´é”®
#    å‡è®¾å‘½åç±»ä¼¼: "0801_1203_840" â†’ MMDD=0801, HHMM=1203
#    è‹¥è§£æå¤±è´¥: å›é€€åˆ°åŸå­—ç¬¦ä¸²å¹¶æŒ‰è‡ªç„¶æ’åº
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_TIME_RE = re.compile(r"^(\d{4})_(\d{4})(?:_.+)?$")  # e.g. 0801_1203_...

def parse_time_key(name: str) -> Tuple[int,int,int,int,str]:
    """
    è¿”å›ä¸€ä¸ªå¯æ’åºçš„é”® (MM, DD, HH, mm, display_str)
    è§£æå¤±è´¥æ—¶, è¿”å› (9999, 9999, 99, 99, name) ç¡®ä¿æ’åœ¨æœ€åå¹¶ç”¨åŸåå±•ç¤º
    """
    m = _TIME_RE.match(name)
    if not m:
        # å›é€€: æŠŠåå­—æ”¾æœ€åï¼Œdisplay å°±ç”¨åŸå
        return (9999, 9999, 99, 99, name)
    mmdd, hhmm = m.group(1), m.group(2)
    try:
        MM  = int(mmdd[:2])
        DD  = int(mmdd[2:])
        HH  = int(hhmm[:2])
        mm  = int(hhmm[2:])
        disp = f"{MM:02d}-{DD:02d} {HH:02d}:{mm:02d}"
        return (MM, DD, HH, mm, disp)
    except Exception:
        return (9999, 9999, 99, 99, name)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2) è¯»å–ä¸è®¡æ•°
#    JSON ç»“æ„: { original_name: [ { label: "...", ... }, ... ], ... }
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_annotations(fp: Path) -> Dict[str, List[dict]]:
    with fp.open("r", encoding="utf-8") as f:
        return json.load(f)

def compute_counts_by_time(data: Dict[str, List[dict]]) -> Tuple[List[str], List[Tuple[str, Dict[str,int]]]]:
    """
    è¿”å›:
      - æ‰€æœ‰å‡ºç°è¿‡çš„ label åˆ—è¡¨(æŒ‰å­—æ¯åº)
      - æŒ‰æ—¶é—´æ’åºçš„ [(display_time, {label: count, ...}), ...]
    """
    # æ”¶é›† label é›†
    all_labels = set()
    # ä¸´æ—¶è®°å½•: time_key â†’ Counter(label)
    time_counters: Dict[Tuple[int,int,int,int,str], Counter] = {}  # tuple: (MM, DD, HH, mm, display_str)

    # éå†æ•°æ®, ç»Ÿè®¡æ¯ä¸ªæ—¶é—´ç‚¹çš„ç±»åˆ«æ•°é‡
    for original_name, ann_list in data.items():
        key = parse_time_key(original_name)
        ctr = time_counters.setdefault(key, Counter())
        for ann in ann_list:
            lbl = ann.get("label")
            if not lbl:
                continue
            all_labels.add(lbl)
            ctr[lbl] += 1
    # æŒ‰å­—æ¯åºæ’åºæ‰€æœ‰ label
    labels_sorted = sorted(all_labels)
    # æŒ‰è§£æåçš„æ—¶é—´é”®æ’åº
    entries: List[Tuple[str, Dict[str,int]]] = []
    for k in sorted(time_counters.keys()):
        disp = k[4]                         # display time éƒ¨åˆ† -- display_str
        ctr  = time_counters[k] # Counter
        # print(f"[debug] {disp} è®¡æ•°: {ctr}")
        row_counts = {lbl: ctr.get(lbl, 0) for lbl in labels_sorted} # ä¿æŒåˆ—é¡ºåº
        entries.append((disp, row_counts)) # (display, {label: count, ...})
    return labels_sorted, entries

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3) å†™å‡º CSV: åŸå§‹è®¡æ•° & ç´¯ç§¯è®¡æ•°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def write_counts_and_cumu_onefile(
    labels: List[str],
    entries: List[Tuple[str, Dict[str, int]]],
    out_path: Path,
) -> None:
    """
    è¾“å‡ºä¸€ä¸ª CSVï¼Œæ¯ä¸ªç±»åˆ«æœ‰ä¸¤åˆ—ï¼š<label>_count, <label>_cumu
    """
    # ç´¯ç§¯è®¡æ•°å™¨
    cumu = {lbl: 0 for lbl in labels}

    # æ„é€ è¡¨å¤´
    headers = ["time"]
    for lbl in labels:
        headers.append(f"{lbl}_count")
        headers.append(f"{lbl}_cumu")

    with out_path.open("w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(headers)

        for disp, row_counts in entries:
            row = [disp]
            for lbl in labels:
                count = row_counts.get(lbl, 0)
                cumu[lbl] += count
                row.extend([count, cumu[lbl]])
            writer.writerow(row)



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4) ä¸»æµç¨‹
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_analysis(
    in_path: Path,
    out_path: Path,
):
    if not in_path.exists():
        raise FileNotFoundError(f"æœªæ‰¾åˆ°è¾“å…¥æ–‡ä»¶: {in_path.resolve()}")

    data = load_annotations(in_path)
    labels, entries = compute_counts_by_time(data)

    # è¾“å‡ºä¸€ä¸ª CSV
    write_counts_and_cumu_onefile(labels, entries, out_path)

    # å‹å¥½æ‰“å°é¢„è§ˆ
    print(f"âœ… ç»Ÿè®¡å®Œæˆï¼Œå…±å‘ç° {len(labels)} ä¸ªç±»åˆ«: {labels}")
    print(f"âœ… æ—¶é—´ç‚¹æ•°é‡: {len(entries)}")
    print(f"ğŸ’¾ å·²å†™å‡º: {out_path}")



# %%
for chosen_dir in chosen_dirs:
    print(f"é€‰æ‹©çš„ç›®å½•: {chosen_dir}")
    run_analysis(
        in_path = chosen_dir.parent / "raw_data_sliced_merge" / f"{run_type}_{version}" / "09_filtered_annotations.json",
        out_path = chosen_dir.parent / "raw_data_sliced_merge" / f"{run_type}_{version}" / "11_statistics_filtered_annotations_counts.csv",
    )

# %% [markdown]
# # å»é‡ç»Ÿè®¡ -- ç»Ÿè®¡å„ç±»åˆ«é™¤å»å¤šå¼ ç…§ç‰‡ä¹‹é—´é‡å¤ä½ç½®çš„objectsæ•°é‡ï¼Œå¹¶å¯¼å‡ºä¸ºCSVæ–‡ä»¶

# %%